{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "import matplotlib.gridspec as gridspec    \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpoch = 12\n",
    "batch_size = 1 #replace it with 8 if you want to compare optimizers for minibatch size 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(image_dim))])\n",
    "train_set = torchvision.datasets.MNIST(root='./data/MNIST', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RJPMel7XBOZ"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 500)  #layer with 1000 nodes is removed\n",
    "        self.fc3 = nn.Linear(500, 250)\n",
    "        self.fc4 = nn.Linear(250, 30)\n",
    "             \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc3(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 250)\n",
    "        self.fc2 = nn.Linear(250, 500)   #layer with 1000 nodes is removed\n",
    "        self.fc4 = nn.Linear(500, 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc2(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        out = self.fc4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda:0\")\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "enc_dim = 30                      #given in paper\n",
    "image_dim = 784                   # 28*28 size of input image flattened to 784       \n",
    "enc = Encoder(image_dim).to(device)\n",
    "dec = Decoder(enc_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLVAzHlAVQPG"
   },
   "outputs": [],
   "source": [
    "class AccSGD(Optimizer):\n",
    "    r\"\"\"Implements the algorithm proposed in https://arxiv.org/pdf/1704.08227.pdf, which is a provably accelerated method \n",
    "    for stochastic optimization. This has been employed in https://openreview.net/forum?id=rJTutzbA- for training several \n",
    "    deep learning models of practical interest. This code has been implemented by building on the construction of the SGD \n",
    "    optimization module found in pytorch codebase.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (required)\n",
    "        kappa (float, optional): ratio of long to short step (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from AccSGD import *\n",
    "        >>> optimizer = AccSGD(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, kappa = 1000.0, xi = 10.0, smallConst = 0.7, weight_decay=0):\n",
    "        defaults = dict(lr=lr, kappa=kappa, xi=xi, smallConst=smallConst,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AccSGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AccSGD, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            large_lr = (group['lr']*group['kappa'])/(group['smallConst'])\n",
    "            Alpha = 1.0 - ((group['smallConst']*group['smallConst']*group['xi'])/group['kappa'])\n",
    "            Beta = 1.0 - Alpha\n",
    "            zeta = group['smallConst']/(group['smallConst']+Beta)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    param_state['momentum_buffer'] = copy.deepcopy(p.data)\n",
    "                buf = param_state['momentum_buffer']\n",
    "                buf.mul_((1.0/Beta)-1.0)\n",
    "                buf.add_(-large_lr,d_p)\n",
    "                buf.add_(p.data)\n",
    "                buf.mul_(Beta)\n",
    "\n",
    "                p.data.add_(-group['lr'],d_p)\n",
    "                p.data.mul_(zeta)\n",
    "                p.data.add_(1.0-zeta,buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn(optimizer):    \n",
    "    loss_all2 = []\n",
    "    for epoch in range(nEpoch):\n",
    "        losses = []\n",
    "        trainloader = tqdm(train_loader)\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            if i * batch_size >= 10000:\n",
    "                break\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                z = enc(inputs)\n",
    "                outputs = dec(z)\n",
    "\n",
    "                loss = F.mse_loss(outputs, inputs, size_average=False) / inputs.shape[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # keep track of the loss and update the stats\n",
    "                losses.append(loss.item())\n",
    "                trainloader.set_postfix(loss=np.mean(losses), epoch=epoch)\n",
    "        loss_all2.append(np.sqrt(np.mean(losses)))\n",
    "    return loss_all2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Gradient descent functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1UYDhZjdcf7W"
   },
   "outputs": [],
   "source": [
    "def SGD(lr):\n",
    "    optimizer = optim.SGD(chain(enc.parameters(), dec.parameters()), lr)\n",
    "    return lossfn(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAM( lr):\n",
    "    optimizer = optim.Adam(chain(enc.parameters(), dec.parameters()), lr)\n",
    "    return lossfn(optimizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAnsHkXKCRv9"
   },
   "outputs": [],
   "source": [
    "def ASGD(lr, kappa, xi):\n",
    "    optimizer = AccSGD(chain(enc.parameters(), dec.parameters()), lr, kappa, xi)\n",
    "    return lossfn(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_ob64TnCRv-"
   },
   "outputs": [],
   "source": [
    "def NAG( lr, momentum):\n",
    "    optimizer = optim.SGD(chain(enc.parameters(), dec.parameters()), lr, momentum, nesterov = True)\n",
    "    return lossfn(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rY816u3MCRwB"
   },
   "outputs": [],
   "source": [
    "def HB(lr, momentum):\n",
    "    optimizer = optim.SGD(chain(enc.parameters(), dec.parameters()), lr, momentum)\n",
    "    return lossfn(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss, name, length):\n",
    "    loss_init = max(loss[0,:])\n",
    "    dif = np.zeros(np.size(loss[0, :]))\n",
    "    dif = loss_init * np.ones(np.size(loss[0, :])) - loss[0, :]\n",
    "    for i in range(np.size(loss[:, 0])):\n",
    "        for j in range(np.size(loss[0, :])):\n",
    "            loss[i, j] = loss[i, j] + dif[j]\n",
    "    loss[0, :] = loss_init * np.ones(np.size(loss[0, :]))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Choosing learning rate of '+name)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE lose')\n",
    "    x = np.linspace(0, nEpoch, nEpoch)\n",
    "    for i in range(length):\n",
    "        plt.plot(x, loss[:, i], label = str(i))\n",
    "        plt.legend()\n",
    "    plt.savefig(name+'.png')\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]   \n",
    "\n",
    "i = 0\n",
    "loss = np.zeros((nEpoch, 10))\n",
    "for lr in lr_grid:\n",
    "    loss_SGD = SGD( lr)          \n",
    "    loss[:, i] = loss_SGD\n",
    "    i += 1\n",
    "plot(loss, 'SGD', int(np.size(lr_grid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "\n",
    "i = 0\n",
    "loss = np.zeros((nEpoch, int(np.size(lr_grid))))\n",
    "for lr in lr_grid:\n",
    "    loss_ADAM = ADAM(lr)          \n",
    "    loss[:, i] = loss_ADAM\n",
    "    i += 1\n",
    "    \n",
    "plot(loss, 'Adam', int(np.size(lr_grid)*np.size(momentum_grid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "momentum_grid = [0, 0.5, 0.75, 0.9, 0.95, 0.9]\n",
    "\n",
    "i = 0\n",
    "loss = np.zeros((nEpoch,int(np.size(lr_grid)*np.size(momentum_grid))) )\n",
    "for lr in lr_grid:\n",
    "    for momentum in momentum_grid:\n",
    "        loss_NAG = NAG(lr, momentum)          \n",
    "        loss[:, i] = loss_NAG\n",
    "        i += 1\n",
    "        \n",
    "plot(loss, 'NAG', int(np.size(lr_grid)*np.size(momentum_grid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "momentum_grid = [0, 0.5, 0.75, 0.9, 0.95, 0.9]\n",
    "\n",
    "i = 0\n",
    "loss = np.zeros((nEpoch, int(np.size(lr_grid)*np.size(momentum_grid))))\n",
    "for lr in lr_grid:\n",
    "    for momentum in momentum_grid:\n",
    "        loss_HB = HB(lr, momentum)          \n",
    "        loss[:, i] = loss_HB\n",
    "        i += 1\n",
    "plot(loss, 'HB', int(np.size(lr_grid)*np.size(momentum_grid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "ls_grid = [100, 1000]\n",
    "ap_grid = [ 2.5, 5.0, 10.0, 20]\n",
    "\n",
    "i = 0\n",
    "loss = np.zeros((nEpoch, int(np.size(lr_grid)*np.size(ls_grid)*np.size(ap_grid))))\n",
    "for lr in lr_grid:\n",
    "    for ls in ls_grid:\n",
    "        for ap in ap_grid:\n",
    "            loss_ASGD = ASGD(lr, ls, ap)\n",
    "            loss[:, i] = loss_ASGD\n",
    "            i += 1\n",
    "            \n",
    "plot(loss, 'ASGD', int(np.size(lr_grid)*np.size(momentum_grid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: rerun the grid search box many times to obatain again best parameters where the algorithms perform optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you rerun then do the following:\n",
    "First run the below cell.\n",
    "After that obtain two adjecent values where the algo perform best.\n",
    "Then devide those parameters into minor subsets to obtain better graphs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "compare_MNIST.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
