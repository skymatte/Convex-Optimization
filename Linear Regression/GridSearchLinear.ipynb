{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnNA98rPdNOa"
   },
   "source": [
    "## Import Libraries and Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7z2BBlDhi1X"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmJp8A4IhpVz"
   },
   "outputs": [],
   "source": [
    "#Code for ASGD ported from https://github.com/rahulkidambi/AccSGD\n",
    "\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import copy\n",
    "\n",
    "class AccSGD(Optimizer):\n",
    "    r\"\"\"Implements the algorithm proposed in https://arxiv.org/pdf/1704.08227.pdf, which is a provably accelerated method \n",
    "    for stochastic optimization. This has been employed in https://openreview.net/forum?id=rJTutzbA- for training several \n",
    "    deep learning models of practical interest. This code has been implemented by building on the construction of the SGD \n",
    "    optimization module found in pytorch codebase.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate (required)\n",
    "        kappa (float, optional): ratio of long to short step (default: 1000)\n",
    "        xi (float, optional): statistical advantage parameter (default: 10)\n",
    "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
    "    Example:\n",
    "        >>> from AccSGD import *\n",
    "        >>> optimizer = AccSGD(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.001, kappa = 1000.0, xi = 10.0, smallConst = 0.7, weight_decay=0):\n",
    "        defaults = dict(lr=lr, kappa=kappa, xi=xi, smallConst=smallConst,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AccSGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AccSGD, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            large_lr = (group['lr']*group['kappa'])/(group['smallConst'])\n",
    "            Alpha = 1.0 - ((group['smallConst']*group['smallConst']*group['xi'])/group['kappa'])\n",
    "            Beta = 1.0 - Alpha\n",
    "            zeta = group['smallConst']/(group['smallConst']+Beta)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    param_state['momentum_buffer'] = copy.deepcopy(p.data)\n",
    "                buf = param_state['momentum_buffer']\n",
    "                buf.mul_((1.0/Beta)-1.0)\n",
    "                buf.add_(-large_lr,d_p)\n",
    "                buf.add_(p.data)\n",
    "                buf.mul_(Beta)\n",
    "\n",
    "                p.data.add_(-group['lr'],d_p)\n",
    "                p.data.mul_(zeta)\n",
    "                p.data.add_(1.0-zeta,buf)\n",
    "\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HvVAl1thrfQ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Randomly select a weight matrix such as\n",
    "# w = torch.rand(2,1)\n",
    "# Ensure that you're using the same 'w' used in LinearRegression.ipynb for grid search to make sense\n",
    "w = torch.tensor([[0.5],[2]])\n",
    "\n",
    "#Define Gaussian Distribution as per Original Paper\n",
    "def gaussian(N,k):    \n",
    "    a = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "        loc=torch.Tensor([0,0]), covariance_matrix=torch.Tensor([[1, 0],[0, 1/k]])).sample((N,))\n",
    "    b = a@w\n",
    "    if torch.cuda.is_available(): \n",
    "        return Variable(a).cuda(),Variable(b).cuda()\n",
    "    else:\n",
    "        return Variable(a),Variable(b)\n",
    "\n",
    "#Define Discrete Distribution as per Original Paper\n",
    "def discrete(N,k):\n",
    "  a = torch.zeros(N,2)\n",
    "  b = torch.zeros(N,1)\n",
    "\n",
    "  for i in range(N):\n",
    "    dist = torch.multinomial(input= torch.tensor([0.5, 0.5]),num_samples=2).float()\n",
    "    a[i] = dist\n",
    "\n",
    "  a[:,1]=a[:,1].float()*(2.0/k)\n",
    "  b = a @ w  \n",
    "\n",
    "  if torch.cuda.is_available(): \n",
    "        return Variable(a).cuda(),Variable(b).cuda()\n",
    "  else:\n",
    "        return Variable(a),Variable(b)\n",
    "\n",
    "#Define a simplistic Neural Network for Linear Regression\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(2,1)\n",
    "\n",
    "    def forward(self,input):\n",
    "        output = self.linear(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mwL93wN3jBV"
   },
   "source": [
    "## 1. SGD Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2N9dvAroxYgF"
   },
   "outputs": [],
   "source": [
    "def error_SGD(lr, n = 9, Gaussian=False):\n",
    "\n",
    "  seed = 10\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  \n",
    "  Num_samples = 1000\n",
    "  k_set = torch.zeros(n)\n",
    "\n",
    "  #Maintain loss list having loss values for each Condition Number k, i.e., a global loss list\n",
    "  error_list = []\n",
    "    \n",
    "  #Iterate over each possible value of Condition Number k\n",
    "  for i in range (n):\n",
    "    k = 2**(i+4)\n",
    "    k_set[i] = k\n",
    "\n",
    "    #Maintain a per Condition Number Loss List, i.e., a local loss list\n",
    "    per_k_error_list = []\n",
    "\n",
    "    iterations = 5*k\n",
    "\n",
    "    loss_list = torch.zeros(9, iterations).cuda()\n",
    "\n",
    "    #Generate Training Data\n",
    "    if Gaussian:\n",
    "      X, y = gaussian(Num_samples, k_set[i])\n",
    "    else:\n",
    "      X, y = discrete(Num_samples, k_set[i])\n",
    "\n",
    "    model = LinearRegression().cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    initial_loss = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for t in range(iterations):\n",
    "\n",
    "        y_hat = model(X)\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss_list[i][t] = loss.item()\n",
    "        \n",
    "        if t == 0:\n",
    "          initial_loss = loss.item()\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        #Verify Geometric Convergence Criteria\n",
    "        elif t > int(iterations/2):\n",
    "            \n",
    "            #If convergence criteria has not been violated, append the loss value to the local loss list\n",
    "            if loss.item() < initial_loss:\n",
    "              r =  loss.item()\n",
    "              per_k_error_list.append(r)\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "    \n",
    "            #If convergence criteria fails at any one loss value (and therefore t),\n",
    "            #straight away assign infinite loss to the particular hyperparameter setting\n",
    "            else:\n",
    "              print(loss.item(), initial_loss)\n",
    "              print(\"Violation of Convergence criteria at learning rate: \",lr,\" k: \",k, \" epoch: \",t, \"iter: \",iterations)\n",
    "              print(\"Assigning average infinite loss to learning rate: \", lr)\n",
    "              per_k_error_list = []\n",
    "              return float('inf')\n",
    "\n",
    "        else:\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    \n",
    "    print(\"------At k = {}, Size of Sub-Converge List of Loss Values: {}\".format(k, len(per_k_error_list))) \n",
    "\n",
    "    #Randomly sample at max 100 loss values from the local loss list and append them to the global loss list\n",
    "    if len(per_k_error_list) > 100:\n",
    "      per_k_error_list = random.sample(per_k_error_list, k=100)\n",
    "\n",
    "    for error in per_k_error_list:\n",
    "      error_list.append(error)\n",
    "\n",
    "  print(\"--Size of Converge List across all values of k on Random Sampling from Each Sub-Converge List: \",len(error_list))\n",
    "\n",
    "  #Randomly sample at max 100 loss values from the global loss list\n",
    "  if len(error_list) > 100:\n",
    "      error_list = random.sample(error_list, k=100)\n",
    "\n",
    "  print(\"--Size of Converge List on re-random sampling (at max 100 values): \",len(error_list))\n",
    "\n",
    "  #If loss list is empty, return infinite loss. This corresponds to violation of Convergence Criteria somewhere before\n",
    "  if len(error_list) == 0:\n",
    "    return float('inf')\n",
    "  \n",
    "  #Return the average of all the sampled loss values as final loss at a particular hyperparameter setting\n",
    "  return sum(error_list)/len(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iUjWnOwxYXO"
   },
   "outputs": [],
   "source": [
    "#Create Learning Rate Set as {0.01, 0.02, ...., 0.99, 1} for Grid Search\n",
    "learning_rate_set = np.linspace(0.01,1,99)\n",
    "#Create Loss Set to store the loss values for each learning rate above\n",
    "loss_set_SGD = np.zeros(99)\n",
    "\n",
    "min_error = 0\n",
    "x = 0\n",
    "\n",
    "#Iterate over Learning Rate Set\n",
    "for lr in learning_rate_set:\n",
    "\n",
    "    #Compute the loss for a particular hyperparameter setting\n",
    "    error = error_SGD(lr, Gaussian=True)\n",
    "    loss_set_SGD[x] = error\n",
    "\n",
    "    #Update the minimum loss value and the corresponding optmal hyperparameter value found so far\n",
    "    if (min_error == 0 or error<min_error) and error!=0:\n",
    "      min_lr = lr\n",
    "      min_x = x\n",
    "      min_error = error\n",
    "    print(\"Learning Rate: {}  has Average Loss Value: {}\\n\".format(lr, error))\n",
    "\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lZhsQNmocAW"
   },
   "outputs": [],
   "source": [
    "#Print the entire loss set as well the optimal settings found using Grid Search\n",
    "print(\"Loss Set for SGD :\")\n",
    "print(loss_set_SGD)\n",
    "print(\"Minimum Loss found is: {} @ Learning Rate {}\".format(loss_set_SGD[min_x], min_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFQvIOJnxU2v"
   },
   "source": [
    "## 2. Heavy Ball Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0h9p5i2mZqxU"
   },
   "outputs": [],
   "source": [
    "def error_HB(lr, momentum, n = 9, Gaussian=False):\n",
    "\n",
    "  seed = 7\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  Num_samples = 1000\n",
    "  k_set = torch.zeros(n)\n",
    "\n",
    "  error_list = []\n",
    "\n",
    "  #Iterate over each possible value of Condition Number k\n",
    "  for i in range (n):\n",
    "    k = 2**(i+4)\n",
    "    k_set[i] = k\n",
    "\n",
    "    #Maintain a per Condition Number Loss List, i.e., a local loss list\n",
    "    per_k_error_list = []\n",
    "\n",
    "    iterations = 5*k\n",
    "\n",
    "    loss_list = torch.zeros(9, iterations).cuda()\n",
    "\n",
    "    #Generate Training Data\n",
    "    if Gaussian:\n",
    "      X, y = gaussian(Num_samples, k_set[i])\n",
    "    else:\n",
    "      X, y = discrete(Num_samples, k_set[i])\n",
    "\n",
    "    model = LinearRegression().cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum = momentum)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    initial_loss = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for t in range(iterations):\n",
    "\n",
    "        y_hat = model(X)\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss_list[i][t] = loss.item()\n",
    "\n",
    "        if t == 0:\n",
    "          initial_loss = loss.item()\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        #Verify Geometric Convergence Criteria\n",
    "        elif t > int(iterations/2):\n",
    "\n",
    "            #If convergence criteria has not been violated, append the loss value to the local loss list\n",
    "            if loss.data < initial_loss:\n",
    "              r =  loss.item()\n",
    "              per_k_error_list.append(r)\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "            #If convergence criteria fails at any one loss value (and therefore t),\n",
    "            #straight away assign infinite loss to the particular hyperparameter setting \n",
    "            else:\n",
    "              print(loss.item(), initial_loss)\n",
    "              print(\"Violation of Convergence criteria at learning rate: \",lr,\" momentum: \",momentum,\" k: \",k, \" epoch: \",t, \"iter: \",iterations)\n",
    "              print(\"Assigning average infinite loss to learning rate: \", lr)\n",
    "              per_k_error_list = []\n",
    "              return float('inf')\n",
    "\n",
    "        else:\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    print(\"------At k = {}, Size of Sub-Converge List of Loss Values: {}\".format(k, len(per_k_error_list))) \n",
    "\n",
    "    #Randomly sample at max 100 loss values from the local loss list and append them to the global loss list\n",
    "    if len(per_k_error_list) > 100:\n",
    "      per_k_error_list = random.sample(per_k_error_list, k=100)\n",
    "\n",
    "    for error in per_k_error_list:\n",
    "      error_list.append(error)\n",
    "\n",
    "  print(\"--Size of Converge List across all values of k on Random Sampling from Each Sub-Converge List: \",len(error_list))\n",
    "\n",
    "  #Randomly sample at max 100 loss values from the global loss list\n",
    "  if len(error_list) > 100:\n",
    "      error_list = random.sample(error_list, k=100)\n",
    "\n",
    "  print(\"--Size of Converge List on re-random sampling (at max 100 values): \",len(error_list))\n",
    "\n",
    "  #If loss list is empty, return infinite loss. This corresponds to violation of Convergence Criteria somewhere before\n",
    "  if len(error_list) == 0:\n",
    "    return float('inf')\n",
    "\n",
    "  #Return the average of all the sampled loss values as final loss at a particular hyperparameter setting\n",
    "  return sum(error_list)/len(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMrYqadxhu8-"
   },
   "outputs": [],
   "source": [
    "#Create Learning Rate Set as {0.1, 0.2, ...., 0.9, 1} for Grid Search\n",
    "learning_rate_set = np.linspace(0.1,1,10)\n",
    "#Create Momentum Set as {0.1, 0.2, ...., 0.9, 1} for Grid Search\n",
    "momentum_set = np.linspace(0.1,1,10)\n",
    "loss_set_HB = np.zeros([10,10])\n",
    "\n",
    "min_error = 0\n",
    "x = 0\n",
    "\n",
    "#Iterate over learning rate set\n",
    "for lr in learning_rate_set:\n",
    "    y = 0\n",
    "\n",
    "    #Iterate over Momentum set\n",
    "    for m in momentum_set:\n",
    "\n",
    "        #Compute the loss for a particular hyperparameter setting\n",
    "        error = error_HB(lr, m, Gaussian=True)\n",
    "        loss_set_HB[x,y] = error\n",
    "    \n",
    "        #Update the minimum loss value and the corresponding optmal hyperparameter value found so far\n",
    "        if (min_error == 0 or error<min_error) and error!=0:\n",
    "          min_lr = lr\n",
    "          min_m = m\n",
    "          min_x = x\n",
    "          min_y = y\n",
    "          min_error = error\n",
    "\n",
    "        print(\"Learning Rate: {}, Momentum: {}  has Average Loss Value: {}\\n\".format(lr, m, error))\n",
    "        y += 1\n",
    "\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOYEO6IyhaS4"
   },
   "outputs": [],
   "source": [
    "#Print the entire loss set as well the optimal settings found using Grid Search\n",
    "print(\"Loss Set for Heavy Ball :\")\n",
    "print(loss_set_HB)\n",
    "print(\"Minimum Loss found is: {} @ Learning Rate {}, Momentum: {}\".format(loss_set_HB[min_x], min_lr, min_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ee6hIluQ3uvc"
   },
   "source": [
    "## 3. NAG Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9cvk4T63ZHK"
   },
   "outputs": [],
   "source": [
    "def error_NAG(lr, momentum, n = 9):\n",
    "\n",
    "  seed = 7\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  Num_samples = 1000\n",
    "  k_set = torch.zeros(n)\n",
    "\n",
    "  error_list = []\n",
    "\n",
    "  #Iterate over each possible value of Condition Number k \n",
    "  for i in range (n):\n",
    "    k = 2**(i+4)\n",
    "    k_set[i] = k\n",
    "\n",
    "    #Maintain a per Condition Number Loss List, i.e., a local loss list\n",
    "    per_k_error_list = []\n",
    "\n",
    "    iterations = 5*k\n",
    "\n",
    "    loss_list = torch.zeros(9, iterations).cuda()\n",
    "\n",
    "    #Generate Training Data\n",
    "    if Gaussian:\n",
    "      X, y = gaussian(Num_samples, k_set[i])\n",
    "    else:\n",
    "      X, y = discrete(Num_samples, k_set[i])\n",
    "\n",
    "    model = LinearRegression().cuda()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum = momentum, nesterov = True)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    initial_loss = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for t in range(iterations):\n",
    "\n",
    "        y_hat = model(X)\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss_list[i][t] = loss.item()\n",
    "\n",
    "        if loss.item() == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        if t == 0:\n",
    "          initial_loss = loss.item()\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        #Verify Geometric Convergence Criteria\n",
    "        elif t > int(iterations/2):\n",
    "\n",
    "            #If convergence criteria has not been violated, append the loss value to the local loss list\n",
    "            if loss.item() < initial_loss:\n",
    "              r =  loss.item()\n",
    "              per_k_error_list.append(r)\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "            #If convergence criteria fails at any one loss value (and therefore t),\n",
    "            #straight away assign infinite loss to the particular hyperparameter setting    \n",
    "            else:\n",
    "              print(loss.item(), initial_loss)\n",
    "              print(\"Violation of Convergence criteria at learning rate: \",lr,\" momentum: \",momentum,\" k: \",k, \" epoch: \",t, \"iter: \",iterations)\n",
    "              print(\"Assigning average infinite loss to learning rate: \", lr)\n",
    "              per_k_error_list = []\n",
    "              return float('inf')\n",
    "\n",
    "        else:\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    print(\"------At k = {}, Size of Sub-Converge List of Loss Values: {}\".format(k, len(per_k_error_list))) \n",
    "\n",
    "    #Randomly sample at max 100 loss values from the local loss list and append them to the global loss list\n",
    "    if len(per_k_error_list) > 100:\n",
    "      per_k_error_list = random.sample(per_k_error_list, k=100)\n",
    "\n",
    "    for error in per_k_error_list:\n",
    "      error_list.append(error)\n",
    "\n",
    "  print(\"--Size of Converge List across all values of k on Random Sampling from Each Sub-Converge List: \",len(error_list))\n",
    "\n",
    "  #Randomly sample at max 100 loss values from the global loss list\n",
    "  if len(error_list) > 100:\n",
    "      error_list = random.sample(error_list, k=100)\n",
    "\n",
    "  print(\"--Size of Converge List on re-random sampling (at max 100 values): \",len(error_list))\n",
    "\n",
    "  #If loss list is empty, return infinite loss. This corresponds to violation of Convergence Criteria somewhere before\n",
    "  if len(error_list) == 0:\n",
    "    return float('inf')\n",
    "\n",
    "  #Return the average of all the sampled loss values as final loss at a particular hyperparameter setting\n",
    "  return sum(error_list)/len(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJhsQTBY3tWV"
   },
   "outputs": [],
   "source": [
    "learning_rate_set = np.linspace(0.1,1,10)\n",
    "momentum_set = np.linspace(0.1,1,10)\n",
    "loss_set_NAG = np.zeros([10,10])\n",
    "\n",
    "min_error = 1e8\n",
    "x = 0\n",
    "for lr in learning_rate_set:\n",
    "    y = 0\n",
    "    for m in momentum_set:\n",
    "\n",
    "        #Compute the loss for a particular hyperparameter setting\n",
    "        error = error_NAG(lr, m, Gaussian=True)\n",
    "        loss_set_NAG[x,y] = error\n",
    "\n",
    "        #Update the minimum loss value and the corresponding optmal hyperparameter value found so far\n",
    "        if (min_error == 1e8 or error<min_error) and error!=0:\n",
    "          min_lr = lr\n",
    "          min_m = m\n",
    "          min_x = x\n",
    "          min_y = y\n",
    "          min_error = error\n",
    "\n",
    "        print(\"Learning Rate: {}, Momentum: {}  has Average Loss Value: {}\\n\".format(lr, m, error))\n",
    "        y += 1\n",
    "\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBrPRd-5nJ2d"
   },
   "outputs": [],
   "source": [
    "#Print the entire loss set as well the optimal settings found using Grid Search\n",
    "print(\"Loss Set for Heavy Ball :\")\n",
    "print(loss_set_NAG)\n",
    "print(\"Minimum Loss found is: {} @ Learning Rate {}, Momentum: {}\".format(loss_set_NAG[min_x], min_lr, min_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOkefeJAO4wg"
   },
   "source": [
    "## ASGD Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8p7u2ggfHlTP"
   },
   "outputs": [],
   "source": [
    "def error_ASGD(lr, n = 9, Gaussian=False):\n",
    "\n",
    "  seed = 10\n",
    "  torch.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  Num_samples = 1000\n",
    "  k_set = torch.zeros(n)\n",
    "\n",
    "  error_list = []\n",
    "\n",
    "  #Iterate over each possible value of Condition Number k\n",
    "  for i in range (n):\n",
    "    k = 2**(i+4)\n",
    "    k_set[i] = k\n",
    "\n",
    "    per_k_error_list = []\n",
    "\n",
    "    iterations = 5*k\n",
    "\n",
    "    llr = 2*k\n",
    "    sap = np.sqrt(2/3 * k)\n",
    "\n",
    "    loss_list = torch.zeros(9, iterations).cuda()\n",
    "\n",
    "    #Generate Training Data\n",
    "    if Gaussian:\n",
    "      X, y = gaussian(Num_samples, k_set[i])\n",
    "    else:\n",
    "      X, y = discrete(Num_samples, k_set[i])\n",
    "\n",
    "    model = LinearRegression().cuda()\n",
    "    optimizer = AccSGD(model.parameters(), lr=lr,  kappa = llr, xi = sap)    \n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    initial_loss = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    for t in range(iterations):\n",
    "\n",
    "        y_hat = model(X)\n",
    "        loss = loss_func(y_hat,y)\n",
    "        loss_list[i][t] = loss.item()\n",
    "\n",
    "        if t == 0:\n",
    "          initial_loss = loss.item()\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        #Verify Geometric Convergence Criteria\n",
    "        elif t > int(iterations/2):\n",
    "\n",
    "            #If convergence criteria has not been violated, append the loss value to the local loss list\n",
    "            if loss.item() < initial_loss:\n",
    "              r =  loss.item()\n",
    "              per_k_error_list.append(r)\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "\n",
    "            #If convergence criteria fails at any one loss value (and therefore t),\n",
    "            #straight away assign infinite loss to the particular hyperparameter setting  \n",
    "            else:\n",
    "              print(loss.item(), initial_loss)\n",
    "              print(\"Violation of Convergence criteria at learning rate: \",lr,\" k: \",k, \" epoch: \",t, \"iter: \",iterations)\n",
    "              print(\"Assigning average infinite loss to learning rate: \", lr)\n",
    "              per_k_error_list = []\n",
    "              return float('inf')\n",
    "\n",
    "        else:\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    print(\"------At k = {}, Size of Sub-Converge List of Loss Values: {}\".format(k, len(per_k_error_list))) \n",
    "\n",
    "    #Randomly sample at max 100 loss values from the local loss list and append them to the global loss list\n",
    "    if len(per_k_error_list) > 100:\n",
    "      per_k_error_list = random.sample(per_k_error_list, k=100)\n",
    "\n",
    "    for error in per_k_error_list:\n",
    "      error_list.append(error)\n",
    "\n",
    "  print(\"--Size of Converge List across all values of k on Random Sampling from Each Sub-Converge List: \",len(error_list))\n",
    "\n",
    "  #Randomly sample at max 100 loss values from the global loss list\n",
    "  if len(error_list) > 100:\n",
    "      error_list = random.sample(error_list, k=100)\n",
    "\n",
    "  print(\"--Size of Converge List on re-random sampling (at max 100 values): \",len(error_list))\n",
    "\n",
    "  #If loss list is empty, return infinite loss. This corresponds to violation of Convergence Criteria somewhere before\n",
    "  if len(error_list) == 0:\n",
    "    return float('inf')\n",
    "\n",
    "  #Return the average of all the sampled loss values as final loss at a particular hyperparameter setting\n",
    "  return sum(error_list)/len(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEUjAteB3jTZ"
   },
   "outputs": [],
   "source": [
    "#Create Learning Rate Set as {0.01, 0.02, ...., 0.99, 1} for Grid Search\n",
    "learning_rate_set = np.linspace(0.01,1,99)\n",
    "#Create Loss Set to store the loss values for each learning rate above\n",
    "loss_set_ASGD = np.zeros([99])\n",
    "\n",
    "min_error = 0\n",
    "x = 0\n",
    "\n",
    "#Iterate over Learning Rate Set\n",
    "for lr in learning_rate_set:\n",
    "\n",
    "    #Compute the loss for a particular hyperparameter setting\n",
    "    error = error_ASGD(lr, Gaussian=True)\n",
    "    loss_set_ASGD[x] = error\n",
    "\n",
    "    #Update the minimum loss value and the corresponding optmal hyperparameter value found so far\n",
    "    if (min_error == 0 or error<min_error) and error!=0:\n",
    "      min_lr = lr\n",
    "      min_x = x\n",
    "      min_error = error\n",
    "    print(\"Learning Rate: {}  has Average Loss Value: {}\\n\".format(lr, error))\n",
    "\n",
    "    x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjXsIzvZoMD2"
   },
   "outputs": [],
   "source": [
    "#Print the entire loss set as well the optimal settings found using Grid Search\n",
    "print(\"Loss Set for ASGD :\")\n",
    "print(loss_set_ASGD)\n",
    "print(\"Minimum Loss found is: {} @ Learning Rate {}\".format(loss_set_ASGD[min_x], min_lr))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GridSearchLinear_submission.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
