{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Resnet128_fin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7f74a815aba4039b835b28ade76826c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_520677eaac554529b50ed141ab4ea6de",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14b4b0c95e674f55974f639bc30a1080",
              "IPY_MODEL_69142f7c72e24c54ad8a0495a47ed2b4"
            ]
          }
        },
        "520677eaac554529b50ed141ab4ea6de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "14b4b0c95e674f55974f639bc30a1080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_32fe4953753b461294805de2d5a09bae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b3e1ea5886614d1e8bf068379861dde6"
          }
        },
        "69142f7c72e24c54ad8a0495a47ed2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a07e11616b804625af4fc97e339a35c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 29100588.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1be5e249cb8346279a7437cdaf312481"
          }
        },
        "32fe4953753b461294805de2d5a09bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b3e1ea5886614d1e8bf068379861dde6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a07e11616b804625af4fc97e339a35c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1be5e249cb8346279a7437cdaf312481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_0ffhKiDCm-J"
      },
      "source": [
        "#Import Libraries and Stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNapRRR_pGnO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4a1dc9b9-55f0-4c66-b8b7-37d11f23613d"
      },
      "source": [
        "# Uncomment the following line to install torchbearer\n",
        "!pip install torchbearer\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchbearer\n",
        "from torch import optim\n",
        "from torchbearer import Trial"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a11487f5658d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchbearer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchbearer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchbearer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zH36lyVppLkZ",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d1h-dLPApPfB",
        "colab": {}
      },
      "source": [
        "#Code for ASGD Optimizer has been ported from https://github.com/rahulkidambi/AccSGD/blob/master/AccSGD.py\n",
        "\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import copy\n",
        "\n",
        "class AccSGD(Optimizer):\n",
        "    r\"\"\"Implements the algorithm proposed in https://arxiv.org/pdf/1704.08227.pdf, which is a provably accelerated method \n",
        "    for stochastic optimization. This has been employed in https://openreview.net/forum?id=rJTutzbA- for training several \n",
        "    deep learning models of practical interest. This code has been implemented by building on the construction of the SGD \n",
        "    optimization module found in pytorch codebase.\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate (required)\n",
        "        kappa (float, optional): ratio of long to short step (default: 1000)\n",
        "        xi (float, optional): statistical advantage parameter (default: 10)\n",
        "        smallConst (float, optional): any value <=1 (default: 0.7)\n",
        "    Example:\n",
        "        >>> from AccSGD import *\n",
        "        >>> optimizer = AccSGD(model.parameters(), lr=0.1, kappa = 1000.0, xi = 10.0)\n",
        "        >>> optimizer.zero_grad()\n",
        "        >>> loss_fn(model(input), target).backward()\n",
        "        >>> optimizer.step()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=0.001, kappa = 1000.0, xi = 10.0, smallConst = 0.7, weight_decay=0):\n",
        "        defaults = dict(lr=lr, kappa=kappa, xi=xi, smallConst=smallConst,\n",
        "                        weight_decay=weight_decay)\n",
        "        super(AccSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(AccSGD, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\" Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            large_lr = (group['lr']*group['kappa'])/(group['smallConst'])\n",
        "            Alpha = 1.0 - ((group['smallConst']*group['smallConst']*group['xi'])/group['kappa'])\n",
        "            Beta = 1.0 - Alpha\n",
        "            zeta = group['smallConst']/(group['smallConst']+Beta)\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                param_state = self.state[p]\n",
        "                if 'momentum_buffer' not in param_state:\n",
        "                    param_state['momentum_buffer'] = copy.deepcopy(p.data)\n",
        "                buf = param_state['momentum_buffer']\n",
        "                buf.mul_((1.0/Beta)-1.0)\n",
        "                buf.add_(-large_lr,d_p)\n",
        "                buf.add_(p.data)\n",
        "                buf.mul_(Beta)\n",
        "\n",
        "                p.data.add_(-group['lr'],d_p)\n",
        "                p.data.mul_(zeta)\n",
        "                p.data.add_(1.0-zeta,buf)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MxtMUzme6GLN"
      },
      "source": [
        "#Prepare Data and Model Architecture for Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "21YHyTJ46IoD"
      },
      "source": [
        "#Download CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IXmB5H_7pSS8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "a7f74a815aba4039b835b28ade76826c",
            "520677eaac554529b50ed141ab4ea6de",
            "14b4b0c95e674f55974f639bc30a1080",
            "69142f7c72e24c54ad8a0495a47ed2b4",
            "32fe4953753b461294805de2d5a09bae",
            "b3e1ea5886614d1e8bf068379861dde6",
            "a07e11616b804625af4fc97e339a35c0",
            "1be5e249cb8346279a7437cdaf312481"
          ]
        },
        "outputId": "5a43fc53-aee4-4f3b-e867-2a97b0f05245"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
        "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
        "\n",
        "#Perform Data augmentation to make robust models without requiring larger dataset, using Randomly Flipping the Image Horizontally and Randomly Cropping the Image\n",
        "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),transforms.Normalize(mean, std)])\n",
        "test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "#Download and Transform the datasets\n",
        "train_data = datasets.CIFAR10(root=\".\", train=True, download=True, transform=train_transform)\n",
        "test_data = datasets.CIFAR10(root=\".\", train=False, download=True, transform=test_transform)\n",
        "valid_data = datasets.CIFAR10(root=\".\", train=True, download=True, transform=train_transform,)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7f74a815aba4039b835b28ade76826c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tRAxousC7dAV"
      },
      "source": [
        "##Split Data into Train and Validation Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iNTQQejarTEa",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "#Split Train Data into Train and Validation Data\n",
        "#Desired ratio of split into Validation and Train Set\n",
        "ratio = 0.2\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(ratio * num_train))\n",
        "\n",
        "np.random.seed(7)\n",
        "np.random.shuffle(indices)\n",
        "batch_size = 128\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "#Create Mini Batches for Train, Validation and Test Datasets\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, sampler=valid_sampler,pin_memory=True,)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lsvVcCddEOW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "ce2208d7-b177-4acf-f3a0-712fd3d455e2"
      },
      "source": [
        "print(\"Count of Train MiniBatches: \", len(train_loader))\n",
        "print(\"Count of Validation MiniBatches: \", len(valid_loader))\n",
        "print(\"Ratio of Count of Validation to Train Minibatches: \",len(valid_loader)/(len(valid_loader) + len(train_loader)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of Train MiniBatches:  313\n",
            "Count of Validation MiniBatches:  79\n",
            "Ratio of Count of Validation to Train Minibatches:  0.20153061224489796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NBZLHBMx7gVy"
      },
      "source": [
        "##Create Resnet44 Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRAQW9kTusJR",
        "colab": {}
      },
      "source": [
        "#Resnet Architecture ported from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py and along the lines of https://arxiv.org/abs/1512.03385\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class DownsampleCIFAR(nn.Module):  \n",
        "\n",
        "  def __init__(self, nIn, nOut, stride):\n",
        "    super(DownsampleCIFAR, self).__init__() \n",
        "    assert stride == 2    \n",
        "    self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)   \n",
        "\n",
        "  def forward(self, x):   \n",
        "    x = self.avg(x)  \n",
        "    return torch.cat((x, x.mul(0)), 1)  \n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "\n",
        "  expansion = 1\n",
        "  def __init__(self, inplanes, planes, stride, downsample, Type):\n",
        "    super(ResnetBlock, self).__init__()\n",
        "\n",
        "    self.Type = Type\n",
        "\n",
        "    self.norm1 = nn.BatchNorm2d(inplanes)\n",
        "    self.conv_a = nn.Conv2d(inplanes, planes, 3, stride, padding = 1, bias = False)\n",
        "\n",
        "    self.norm2 = nn.BatchNorm2d(planes)\n",
        "    self.conv_b = nn.Conv2d(planes, planes, 3, stride = 1, padding = 1, bias = False)\n",
        "\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.downsample = downsample\n",
        "\n",
        "  \n",
        "  def forward(self, x):\n",
        "    initial = x\n",
        "\n",
        "    block = self.norm1(x)\n",
        "    block = self.relu(x)\n",
        "\n",
        "    if self.Type == 'both_preact':\n",
        "      initial = block\n",
        "\n",
        "    block = self.conv_a(block)\n",
        "    block = self.norm2(block)\n",
        "    block = self.relu(block)\n",
        "    block = self.conv_b(block)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "     initial = self.downsample(initial)\n",
        "\n",
        "    return initial + block\n",
        "\n",
        "\n",
        "class Resnet(nn.Module):\n",
        "\n",
        "  def __init__(self, block, depth, num_classes):\n",
        "\n",
        "    super(Resnet, self).__init__()\n",
        "\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "    layer_blocks = (depth-2)//6\n",
        "\n",
        "    self.conv = nn.Conv2d(3, 16, 3, 1, padding=1, bias = False)\n",
        "\n",
        "    self.inplanes = 16\n",
        "    self.conv1 = self.make_layer(block, 16, layer_blocks, 1)\n",
        "    self.conv2 = self.make_layer(block, 32, layer_blocks, 2)\n",
        "    self.conv3 = self.make_layer(block, 64, layer_blocks, 2)\n",
        "    self.final_activation = nn.Sequential(nn.BatchNorm2d(64*block.expansion), nn.ReLU(inplace=True))\n",
        "    self.avgpool = nn.AvgPool2d(8)\n",
        "    self.classifier = nn.Linear(64*block.expansion, num_classes)\n",
        "  \n",
        "  def make_layer(self, block, planes, blocks, stride = 1):\n",
        "    downsample = None\n",
        "\n",
        "    if (stride != 1 or self.inplanes != planes*block.expansion):\n",
        "      downsample = DownsampleCIFAR(self.inplanes, planes*block.expansion, stride)\n",
        "\n",
        "    layers = []\n",
        "    layers.append(block(self.inplanes, planes, stride, downsample, 'both_preact'))\n",
        "    self.inplanes = planes*block.expansion\n",
        "    for i in range(1, blocks):\n",
        "      layers.append(block(self.inplanes, planes, 1, None, 'normal'))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.final_activation(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    return self.classifier(x)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3JgXCL_pFCrw"
      },
      "source": [
        "#Decayed Hyperparameter Schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UgZZrYKH5zk7"
      },
      "source": [
        "##SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bdTME63rZbO",
        "colab": {}
      },
      "source": [
        "def decayed_stats_SGD(lr, decay, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_SGD = np.zeros(epochs)\n",
        "  train_accuracy_SGD = np.zeros(epochs)\n",
        "  valid_loss_SGD = np.zeros(epochs)\n",
        "  valid_accuracy_SGD = np.zeros(epochs)\n",
        "  test_accuracy_SGD = np.zeros(epochs)\n",
        "  test_loss_SGD = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  i = 0\n",
        "\n",
        "  #Initilise validation set error as criteria at change point\n",
        "  error = 100\n",
        "  prev_error = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #Creates a deep copy of the parameter and gradient tensors and makes them shareable to enable re-use of Resnet modules multiple times\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = optim.SGD(model.parameters(), lr = lr, weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation error reduced by more than 0.2% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_error-error)/prev_error < 0.002) and epoch!=0:\n",
        "        \n",
        "        if lr/decay < 0.001:\n",
        "         lr = 0.001\n",
        "        else:\n",
        "            lr=lr/decay\n",
        "\n",
        "      else:\n",
        "          lr=lr\n",
        "\n",
        "      prev_error = error\n",
        "\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    test_metric = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_SGD[epoch] = result[0]['loss']\n",
        "    train_accuracy_SGD[epoch] = result[0]['acc']\n",
        "    valid_loss_SGD[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_SGD[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_SGD[epoch] = result[0]['test_acc']\n",
        "    test_loss_SGD[epoch] = result[0]['test_loss']\n",
        "\n",
        "    error = 1 - result[0]['val_acc']\n",
        "\n",
        "    print(epoch, lr, result)\n",
        "\n",
        "  return lr, train_loss_SGD, train_accuracy_SGD, valid_loss_SGD, valid_accuracy_SGD, test_loss_SGD, test_accuracy_SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEhX3ePUpvVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b368b353-ec9a-4991-f5b0-dc7aee651237"
      },
      "source": [
        "final_lr_SGD, train_loss_SGD, train_accuracy_SGD, valid_loss_SGD, valid_accuracy_SGD, test_loss_SGD, test_accuracy_SGD = decayed_stats_SGD(0.27, 2, 120, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.27 [{'running_loss': 1.4959304332733154, 'running_acc': 0.4521874785423279, 'loss': 1.7118340730667114, 'acc': 0.3577750027179718, 'val_loss': 1.7312296628952026, 'val_acc': 0.353300005197525, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6418858766555786, 'test_acc': 0.384799987077713}]\n",
            "1 0.27 [{'running_loss': 1.154645323753357, 'running_acc': 0.5834375023841858, 'loss': 1.2852612733840942, 'acc': 0.5321999788284302, 'val_loss': 2.3042330741882324, 'val_acc': 0.3391999900341034, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.385242462158203, 'test_acc': 0.3564999997615814}]\n",
            "2 0.27 [{'running_loss': 0.9531030654907227, 'running_acc': 0.6574999690055847, 'loss': 1.0266669988632202, 'acc': 0.6320750117301941, 'val_loss': 1.222274661064148, 'val_acc': 0.562999963760376, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.2196778059005737, 'test_acc': 0.5755000114440918}]\n",
            "3 0.27 [{'running_loss': 0.7783443927764893, 'running_acc': 0.727343738079071, 'loss': 0.8564606308937073, 'acc': 0.6997999548912048, 'val_loss': 2.4777331352233887, 'val_acc': 0.3375000059604645, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.73059344291687, 'test_acc': 0.3422999978065491}]\n",
            "4 0.27 [{'running_loss': 0.7346825003623962, 'running_acc': 0.7475000023841858, 'loss': 0.7580462694168091, 'acc': 0.7358999848365784, 'val_loss': 1.5208379030227661, 'val_acc': 0.5597999691963196, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.5510029792785645, 'test_acc': 0.5789999961853027}]\n",
            "5 0.27 [{'running_loss': 0.6373152136802673, 'running_acc': 0.780468761920929, 'loss': 0.678417444229126, 'acc': 0.7634750008583069, 'val_loss': 0.8060048222541809, 'val_acc': 0.7299999594688416, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8141038417816162, 'test_acc': 0.734499990940094}]\n",
            "6 0.27 [{'running_loss': 0.6152932047843933, 'running_acc': 0.7914062142372131, 'loss': 0.6223698258399963, 'acc': 0.7877500057220459, 'val_loss': 0.8413739800453186, 'val_acc': 0.7013999819755554, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8585695028305054, 'test_acc': 0.7116999626159668}]\n",
            "7 0.27 [{'running_loss': 0.5890426635742188, 'running_acc': 0.8009374737739563, 'loss': 0.5782725811004639, 'acc': 0.800125002861023, 'val_loss': 0.9980326890945435, 'val_acc': 0.6697999835014343, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.0548100471496582, 'test_acc': 0.6622999906539917}]\n",
            "8 0.27 [{'running_loss': 0.5502176284790039, 'running_acc': 0.8090624809265137, 'loss': 0.5488448739051819, 'acc': 0.8104249835014343, 'val_loss': 1.274584412574768, 'val_acc': 0.6352999806404114, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.3281232118606567, 'test_acc': 0.6376999616622925}]\n",
            "9 0.27 [{'running_loss': 0.5114517211914062, 'running_acc': 0.8243749737739563, 'loss': 0.5220829248428345, 'acc': 0.8210499882698059, 'val_loss': 0.8826119303703308, 'val_acc': 0.7107999920845032, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8795095682144165, 'test_acc': 0.724399983882904}]\n",
            "10 0.27 [{'running_loss': 0.5068170428276062, 'running_acc': 0.8207812309265137, 'loss': 0.4963209629058838, 'acc': 0.8279249668121338, 'val_loss': 1.069698691368103, 'val_acc': 0.6628999710083008, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.184849500656128, 'test_acc': 0.6502999663352966}]\n",
            "11 0.27 [{'running_loss': 0.4851891100406647, 'running_acc': 0.8290624618530273, 'loss': 0.4744921326637268, 'acc': 0.8349499702453613, 'val_loss': 0.9136472344398499, 'val_acc': 0.6801999807357788, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.9355252385139465, 'test_acc': 0.6841999888420105}]\n",
            "12 0.27 [{'running_loss': 0.4304039776325226, 'running_acc': 0.8501562476158142, 'loss': 0.4576840102672577, 'acc': 0.8413249850273132, 'val_loss': 0.9698054194450378, 'val_acc': 0.6811999678611755, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.101080298423767, 'test_acc': 0.670199990272522}]\n",
            "13 0.27 [{'running_loss': 0.4419451355934143, 'running_acc': 0.84765625, 'loss': 0.4377424120903015, 'acc': 0.8490749597549438, 'val_loss': 0.896999716758728, 'val_acc': 0.7135999798774719, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.9821354746818542, 'test_acc': 0.7005999684333801}]\n",
            "14 0.27 [{'running_loss': 0.4093317687511444, 'running_acc': 0.8623437285423279, 'loss': 0.42715123295783997, 'acc': 0.8521999716758728, 'val_loss': 0.8385357856750488, 'val_acc': 0.7292999625205994, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8309718370437622, 'test_acc': 0.7432999610900879}]\n",
            "15 0.27 [{'running_loss': 0.4132431745529175, 'running_acc': 0.8581249713897705, 'loss': 0.4137677550315857, 'acc': 0.8570500016212463, 'val_loss': 0.8337621688842773, 'val_acc': 0.7414999604225159, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.9567456841468811, 'test_acc': 0.7270999550819397}]\n",
            "16 0.27 [{'running_loss': 0.39551302790641785, 'running_acc': 0.864062488079071, 'loss': 0.3978967070579529, 'acc': 0.8642999529838562, 'val_loss': 0.7126004099845886, 'val_acc': 0.7685999870300293, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7373366951942444, 'test_acc': 0.7705999612808228}]\n",
            "17 0.27 [{'running_loss': 0.3992932438850403, 'running_acc': 0.8624999523162842, 'loss': 0.3882775902748108, 'acc': 0.8658249974250793, 'val_loss': 1.8102253675460815, 'val_acc': 0.57669997215271, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.1174564361572266, 'test_acc': 0.5597999691963196}]\n",
            "18 0.27 [{'running_loss': 0.3802153766155243, 'running_acc': 0.8671875, 'loss': 0.3779233992099762, 'acc': 0.8689999580383301, 'val_loss': 1.4725549221038818, 'val_acc': 0.6175000071525574, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.5960545539855957, 'test_acc': 0.6265000104904175}]\n",
            "19 0.27 [{'running_loss': 0.35858508944511414, 'running_acc': 0.8760937452316284, 'loss': 0.3706298768520355, 'acc': 0.8727749586105347, 'val_loss': 1.191626787185669, 'val_acc': 0.6281999945640564, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.2966511249542236, 'test_acc': 0.6169999837875366}]\n",
            "20 0.135 [{'running_loss': 0.37185409665107727, 'running_acc': 0.8684374690055847, 'loss': 0.36042043566703796, 'acc': 0.8759499788284302, 'val_loss': 0.8981397747993469, 'val_acc': 0.7251999974250793, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.9424252510070801, 'test_acc': 0.7256999611854553}]\n",
            "21 0.135 [{'running_loss': 0.28975990414619446, 'running_acc': 0.8979687094688416, 'loss': 0.2757547199726105, 'acc': 0.9065249562263489, 'val_loss': 0.5425958633422852, 'val_acc': 0.8274999856948853, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5394436717033386, 'test_acc': 0.8303999900817871}]\n",
            "22 0.135 [{'running_loss': 0.2586025595664978, 'running_acc': 0.9117187261581421, 'loss': 0.25966477394104004, 'acc': 0.9096249938011169, 'val_loss': 0.5369559526443481, 'val_acc': 0.8251999616622925, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5609966516494751, 'test_acc': 0.8253999948501587}]\n",
            "23 0.135 [{'running_loss': 0.24966807663440704, 'running_acc': 0.9099999666213989, 'loss': 0.25247639417648315, 'acc': 0.9124999642372131, 'val_loss': 0.46139103174209595, 'val_acc': 0.8481999635696411, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4497593343257904, 'test_acc': 0.8531000018119812}]\n",
            "24 0.135 [{'running_loss': 0.27372419834136963, 'running_acc': 0.9087499976158142, 'loss': 0.25004151463508606, 'acc': 0.9140249490737915, 'val_loss': 0.5037732124328613, 'val_acc': 0.8351999521255493, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.49403879046440125, 'test_acc': 0.8450999855995178}]\n",
            "25 0.135 [{'running_loss': 0.2586402893066406, 'running_acc': 0.9092187285423279, 'loss': 0.24434970319271088, 'acc': 0.9154499769210815, 'val_loss': 0.5919074416160583, 'val_acc': 0.8114999532699585, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.6364316344261169, 'test_acc': 0.8055999875068665}]\n",
            "26 0.135 [{'running_loss': 0.2562571167945862, 'running_acc': 0.9128124713897705, 'loss': 0.23836718499660492, 'acc': 0.9165250062942505, 'val_loss': 0.8214184641838074, 'val_acc': 0.7566999793052673, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.9455583095550537, 'test_acc': 0.7418000102043152}]\n",
            "27 0.135 [{'running_loss': 0.2352028787136078, 'running_acc': 0.9214062094688416, 'loss': 0.23635980486869812, 'acc': 0.9196499586105347, 'val_loss': 0.48486411571502686, 'val_acc': 0.8417999744415283, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4859647750854492, 'test_acc': 0.8464999794960022}]\n",
            "28 0.0675 [{'running_loss': 0.24201014637947083, 'running_acc': 0.9192187190055847, 'loss': 0.23331961035728455, 'acc': 0.9206500053405762, 'val_loss': 0.4967873990535736, 'val_acc': 0.8343999981880188, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4905923306941986, 'test_acc': 0.8418999910354614}]\n",
            "29 0.0675 [{'running_loss': 0.17704148590564728, 'running_acc': 0.9399999976158142, 'loss': 0.17285875976085663, 'acc': 0.9404249787330627, 'val_loss': 0.4084066152572632, 'val_acc': 0.8707999587059021, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.405475914478302, 'test_acc': 0.8745999932289124}]\n",
            "30 0.0675 [{'running_loss': 0.16327619552612305, 'running_acc': 0.9434374570846558, 'loss': 0.1647181659936905, 'acc': 0.9443749785423279, 'val_loss': 0.44597575068473816, 'val_acc': 0.8600999712944031, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.46340155601501465, 'test_acc': 0.861799955368042}]\n",
            "31 0.0675 [{'running_loss': 0.16596969962120056, 'running_acc': 0.9426562190055847, 'loss': 0.15806175768375397, 'acc': 0.945449948310852, 'val_loss': 0.3869701325893402, 'val_acc': 0.8774999976158142, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3880605399608612, 'test_acc': 0.8811999559402466}]\n",
            "32 0.0675 [{'running_loss': 0.1596171259880066, 'running_acc': 0.9478124976158142, 'loss': 0.15096639096736908, 'acc': 0.9483499526977539, 'val_loss': 0.40809643268585205, 'val_acc': 0.8716999888420105, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41738346219062805, 'test_acc': 0.8761999607086182}]\n",
            "33 0.0675 [{'running_loss': 0.14887286722660065, 'running_acc': 0.9479687213897705, 'loss': 0.14549852907657623, 'acc': 0.9509499669075012, 'val_loss': 0.437568724155426, 'val_acc': 0.8644999861717224, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.45480436086654663, 'test_acc': 0.861299991607666}]\n",
            "34 0.0675 [{'running_loss': 0.16120201349258423, 'running_acc': 0.9443749785423279, 'loss': 0.14816734194755554, 'acc': 0.9485499858856201, 'val_loss': 0.4247763454914093, 'val_acc': 0.8680999875068665, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.44443270564079285, 'test_acc': 0.8694999814033508}]\n",
            "35 0.0675 [{'running_loss': 0.15428504347801208, 'running_acc': 0.9448437094688416, 'loss': 0.1410396546125412, 'acc': 0.9508000016212463, 'val_loss': 0.5177059769630432, 'val_acc': 0.8430999517440796, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5209147334098816, 'test_acc': 0.8486999869346619}]\n",
            "36 0.03375 [{'running_loss': 0.15170788764953613, 'running_acc': 0.9481250047683716, 'loss': 0.14679040014743805, 'acc': 0.9493499994277954, 'val_loss': 0.4527275562286377, 'val_acc': 0.8592000007629395, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.48285749554634094, 'test_acc': 0.8578000068664551}]\n",
            "37 0.03375 [{'running_loss': 0.11084270477294922, 'running_acc': 0.9610937237739563, 'loss': 0.11151567846536636, 'acc': 0.9612749814987183, 'val_loss': 0.3758288025856018, 'val_acc': 0.8852999806404114, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36450743675231934, 'test_acc': 0.8910999894142151}]\n",
            "38 0.03375 [{'running_loss': 0.10370717942714691, 'running_acc': 0.9639062285423279, 'loss': 0.09702438861131668, 'acc': 0.9677000045776367, 'val_loss': 0.35866838693618774, 'val_acc': 0.890999972820282, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3475135266780853, 'test_acc': 0.8953999876976013}]\n",
            "39 0.03375 [{'running_loss': 0.0970076322555542, 'running_acc': 0.96484375, 'loss': 0.09195668250322342, 'acc': 0.9693499803543091, 'val_loss': 0.3736027777194977, 'val_acc': 0.8860999941825867, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39073076844215393, 'test_acc': 0.8888999819755554}]\n",
            "40 0.03375 [{'running_loss': 0.08955813944339752, 'running_acc': 0.96937495470047, 'loss': 0.08726014941930771, 'acc': 0.9714750051498413, 'val_loss': 0.363338828086853, 'val_acc': 0.8902999758720398, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37310904264450073, 'test_acc': 0.8930999636650085}]\n",
            "41 0.03375 [{'running_loss': 0.09008822590112686, 'running_acc': 0.9679687023162842, 'loss': 0.08903810381889343, 'acc': 0.9704749584197998, 'val_loss': 0.40663060545921326, 'val_acc': 0.8809999823570251, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.43214040994644165, 'test_acc': 0.8757999539375305}]\n",
            "42 0.03375 [{'running_loss': 0.09058632701635361, 'running_acc': 0.9682812094688416, 'loss': 0.08671601861715317, 'acc': 0.9706249833106995, 'val_loss': 0.44482943415641785, 'val_acc': 0.8712999820709229, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.45991483330726624, 'test_acc': 0.8776999711990356}]\n",
            "43 0.03375 [{'running_loss': 0.08123619109392166, 'running_acc': 0.9742187261581421, 'loss': 0.08106351643800735, 'acc': 0.9736250042915344, 'val_loss': 0.4436888098716736, 'val_acc': 0.8709999918937683, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.46153655648231506, 'test_acc': 0.8736000061035156}]\n",
            "44 0.016875 [{'running_loss': 0.08282873779535294, 'running_acc': 0.9720312356948853, 'loss': 0.07830023765563965, 'acc': 0.9742749929428101, 'val_loss': 0.39615052938461304, 'val_acc': 0.8847000002861023, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4012214243412018, 'test_acc': 0.8894999623298645}]\n",
            "45 0.016875 [{'running_loss': 0.0641336441040039, 'running_acc': 0.9781249761581421, 'loss': 0.0641600638628006, 'acc': 0.9789999723434448, 'val_loss': 0.36777859926223755, 'val_acc': 0.8937000036239624, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3614161014556885, 'test_acc': 0.8984999656677246}]\n",
            "46 0.016875 [{'running_loss': 0.06674906611442566, 'running_acc': 0.9782811999320984, 'loss': 0.0603601410984993, 'acc': 0.9813999533653259, 'val_loss': 0.3693201541900635, 'val_acc': 0.8950999975204468, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3650670349597931, 'test_acc': 0.8974999785423279}]\n",
            "47 0.016875 [{'running_loss': 0.05421948432922363, 'running_acc': 0.9829687476158142, 'loss': 0.05559513345360756, 'acc': 0.9823249578475952, 'val_loss': 0.3763662874698639, 'val_acc': 0.8915999531745911, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37520042061805725, 'test_acc': 0.896399974822998}]\n",
            "48 0.016875 [{'running_loss': 0.04996345564723015, 'running_acc': 0.9837499856948853, 'loss': 0.05471288412809372, 'acc': 0.9823499917984009, 'val_loss': 0.3728126585483551, 'val_acc': 0.8939999938011169, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3775964379310608, 'test_acc': 0.8996999859809875}]\n",
            "49 0.016875 [{'running_loss': 0.052306316792964935, 'running_acc': 0.9824999570846558, 'loss': 0.05302291363477707, 'acc': 0.9831499457359314, 'val_loss': 0.3877514898777008, 'val_acc': 0.8970999717712402, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3741492033004761, 'test_acc': 0.8986999988555908}]\n",
            "50 0.016875 [{'running_loss': 0.05314328148961067, 'running_acc': 0.9835937023162842, 'loss': 0.050679199397563934, 'acc': 0.9842249751091003, 'val_loss': 0.3744986653327942, 'val_acc': 0.8935999870300293, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3826470673084259, 'test_acc': 0.8981999754905701}]\n",
            "51 0.016875 [{'running_loss': 0.053485702723264694, 'running_acc': 0.981249988079071, 'loss': 0.05034264549612999, 'acc': 0.9841249585151672, 'val_loss': 0.37452366948127747, 'val_acc': 0.8944999575614929, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3827078938484192, 'test_acc': 0.8977999687194824}]\n",
            "52 0.016875 [{'running_loss': 0.04742894694209099, 'running_acc': 0.9854687452316284, 'loss': 0.04890452325344086, 'acc': 0.984499990940094, 'val_loss': 0.3954727053642273, 'val_acc': 0.8912000060081482, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3931564390659332, 'test_acc': 0.8958999514579773}]\n",
            "53 0.016875 [{'running_loss': 0.0476103238761425, 'running_acc': 0.9859374761581421, 'loss': 0.04670164734125137, 'acc': 0.9853499531745911, 'val_loss': 0.41018614172935486, 'val_acc': 0.8872999548912048, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4121958613395691, 'test_acc': 0.8937999606132507}]\n",
            "54 0.016875 [{'running_loss': 0.046073831617832184, 'running_acc': 0.9854687452316284, 'loss': 0.046071551740169525, 'acc': 0.9860249757766724, 'val_loss': 0.3893187940120697, 'val_acc': 0.8914999961853027, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40006938576698303, 'test_acc': 0.8944000005722046}]\n",
            "55 0.016875 [{'running_loss': 0.05339711159467697, 'running_acc': 0.9820312261581421, 'loss': 0.04658716544508934, 'acc': 0.9854249954223633, 'val_loss': 0.3767303228378296, 'val_acc': 0.894599974155426, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3836786448955536, 'test_acc': 0.8986999988555908}]\n",
            "56 0.0084375 [{'running_loss': 0.052828073501586914, 'running_acc': 0.9832812547683716, 'loss': 0.04401183873414993, 'acc': 0.986674964427948, 'val_loss': 0.391785591840744, 'val_acc': 0.894599974155426, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40689435601234436, 'test_acc': 0.8932999968528748}]\n",
            "57 0.0084375 [{'running_loss': 0.03787963464856148, 'running_acc': 0.9878124594688416, 'loss': 0.039463821798563004, 'acc': 0.9878999590873718, 'val_loss': 0.37869396805763245, 'val_acc': 0.8962000012397766, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38289958238601685, 'test_acc': 0.8988999724388123}]\n",
            "58 0.0084375 [{'running_loss': 0.03543044626712799, 'running_acc': 0.9899999499320984, 'loss': 0.03559090197086334, 'acc': 0.9895749688148499, 'val_loss': 0.390531063079834, 'val_acc': 0.8955000042915344, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3843304216861725, 'test_acc': 0.9005999565124512}]\n",
            "59 0.0084375 [{'running_loss': 0.0315447635948658, 'running_acc': 0.9907812476158142, 'loss': 0.033953890204429626, 'acc': 0.9905499815940857, 'val_loss': 0.38991183042526245, 'val_acc': 0.8968999981880188, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38938456773757935, 'test_acc': 0.899899959564209}]\n",
            "60 0.0084375 [{'running_loss': 0.03153792396187782, 'running_acc': 0.9906249642372131, 'loss': 0.03294867277145386, 'acc': 0.9904999732971191, 'val_loss': 0.3913465142250061, 'val_acc': 0.8946999907493591, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39129018783569336, 'test_acc': 0.9006999731063843}]\n",
            "61 0.0084375 [{'running_loss': 0.029882561415433884, 'running_acc': 0.9915624856948853, 'loss': 0.03182639181613922, 'acc': 0.9909499883651733, 'val_loss': 0.38680964708328247, 'val_acc': 0.8995999693870544, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39563846588134766, 'test_acc': 0.899899959564209}]\n",
            "62 0.0084375 [{'running_loss': 0.0325712189078331, 'running_acc': 0.9910937547683716, 'loss': 0.031068462878465652, 'acc': 0.9909499883651733, 'val_loss': 0.39379802346229553, 'val_acc': 0.8983999490737915, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3860642611980438, 'test_acc': 0.901699960231781}]\n",
            "63 0.0084375 [{'running_loss': 0.034516166895627975, 'running_acc': 0.9895312190055847, 'loss': 0.031103840097784996, 'acc': 0.9910249710083008, 'val_loss': 0.39160993695259094, 'val_acc': 0.9013999700546265, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39490118622779846, 'test_acc': 0.9023999571800232}]\n",
            "64 0.0084375 [{'running_loss': 0.030689267441630363, 'running_acc': 0.9909374713897705, 'loss': 0.031414832919836044, 'acc': 0.9903499484062195, 'val_loss': 0.40651801228523254, 'val_acc': 0.8944999575614929, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39954742789268494, 'test_acc': 0.9016000032424927}]\n",
            "65 0.0084375 [{'running_loss': 0.030395101755857468, 'running_acc': 0.9918749928474426, 'loss': 0.029834076762199402, 'acc': 0.9917749762535095, 'val_loss': 0.3945601284503937, 'val_acc': 0.8980000019073486, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3949333727359772, 'test_acc': 0.9002999663352966}]\n",
            "66 0.0084375 [{'running_loss': 0.02898837998509407, 'running_acc': 0.9917187094688416, 'loss': 0.03047763556241989, 'acc': 0.991349995136261, 'val_loss': 0.3943933844566345, 'val_acc': 0.8991999626159668, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39529088139533997, 'test_acc': 0.9018999934196472}]\n",
            "67 0.0084375 [{'running_loss': 0.028787991032004356, 'running_acc': 0.9932812452316284, 'loss': 0.0314108245074749, 'acc': 0.9910249710083008, 'val_loss': 0.39015308022499084, 'val_acc': 0.8958999514579773, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39838090538978577, 'test_acc': 0.8991000056266785}]\n",
            "68 0.00421875 [{'running_loss': 0.03028407320380211, 'running_acc': 0.9904687404632568, 'loss': 0.02924451045691967, 'acc': 0.9916999936103821, 'val_loss': 0.40463292598724365, 'val_acc': 0.8970999717712402, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40936946868896484, 'test_acc': 0.8991999626159668}]\n",
            "69 0.00421875 [{'running_loss': 0.025603128597140312, 'running_acc': 0.9926562309265137, 'loss': 0.027730567380785942, 'acc': 0.9921000003814697, 'val_loss': 0.38241422176361084, 'val_acc': 0.8992999792098999, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3892439007759094, 'test_acc': 0.9034000039100647}]\n",
            "70 0.00421875 [{'running_loss': 0.024598974734544754, 'running_acc': 0.9935937523841858, 'loss': 0.02435952052474022, 'acc': 0.9932999610900879, 'val_loss': 0.39269840717315674, 'val_acc': 0.9010999798774719, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39534977078437805, 'test_acc': 0.9023999571800232}]\n",
            "71 0.00421875 [{'running_loss': 0.02476210705935955, 'running_acc': 0.9931249618530273, 'loss': 0.026466740295290947, 'acc': 0.9925499558448792, 'val_loss': 0.38796907663345337, 'val_acc': 0.9001999497413635, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3956732749938965, 'test_acc': 0.9023999571800232}]\n",
            "72 0.00421875 [{'running_loss': 0.024768711999058723, 'running_acc': 0.992968738079071, 'loss': 0.02466619201004505, 'acc': 0.9930749535560608, 'val_loss': 0.39778029918670654, 'val_acc': 0.9013999700546265, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3903333246707916, 'test_acc': 0.9032999873161316}]\n",
            "73 0.00421875 [{'running_loss': 0.027507657185196877, 'running_acc': 0.9912499785423279, 'loss': 0.025015950202941895, 'acc': 0.9928249716758728, 'val_loss': 0.4044598639011383, 'val_acc': 0.8974999785423279, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39459291100502014, 'test_acc': 0.9034000039100647}]\n",
            "74 0.00421875 [{'running_loss': 0.023785611614584923, 'running_acc': 0.9937499761581421, 'loss': 0.023703329265117645, 'acc': 0.993274986743927, 'val_loss': 0.4075154662132263, 'val_acc': 0.8969999551773071, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3994973599910736, 'test_acc': 0.9030999541282654}]\n",
            "75 0.00421875 [{'running_loss': 0.022187966853380203, 'running_acc': 0.9948437213897705, 'loss': 0.023336905986070633, 'acc': 0.9938499927520752, 'val_loss': 0.4063307046890259, 'val_acc': 0.8960999846458435, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39513731002807617, 'test_acc': 0.9017999768257141}]\n",
            "76 0.002109375 [{'running_loss': 0.020724207162857056, 'running_acc': 0.9950000047683716, 'loss': 0.023594040423631668, 'acc': 0.9936249852180481, 'val_loss': 0.4035824239253998, 'val_acc': 0.899399995803833, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3970060646533966, 'test_acc': 0.9023000001907349}]\n",
            "77 0.002109375 [{'running_loss': 0.023530084639787674, 'running_acc': 0.9935937523841858, 'loss': 0.023311199620366096, 'acc': 0.9936749935150146, 'val_loss': 0.40028175711631775, 'val_acc': 0.8967999815940857, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39661410450935364, 'test_acc': 0.9031999707221985}]\n",
            "78 0.002109375 [{'running_loss': 0.021956566721200943, 'running_acc': 0.9932812452316284, 'loss': 0.022372256964445114, 'acc': 0.9936249852180481, 'val_loss': 0.40996238589286804, 'val_acc': 0.8982999920845032, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39962494373321533, 'test_acc': 0.9036999940872192}]\n",
            "79 0.002109375 [{'running_loss': 0.02341347374022007, 'running_acc': 0.9945312142372131, 'loss': 0.022501477971673012, 'acc': 0.9943749904632568, 'val_loss': 0.3894959092140198, 'val_acc': 0.8984999656677246, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39359790086746216, 'test_acc': 0.9037999510765076}]\n",
            "80 0.002109375 [{'running_loss': 0.02427636831998825, 'running_acc': 0.992968738079071, 'loss': 0.02328263223171234, 'acc': 0.9935500025749207, 'val_loss': 0.40242451429367065, 'val_acc': 0.8986999988555908, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3930492401123047, 'test_acc': 0.9048999547958374}]\n",
            "81 0.002109375 [{'running_loss': 0.02053414285182953, 'running_acc': 0.9953124523162842, 'loss': 0.020883522927761078, 'acc': 0.994949996471405, 'val_loss': 0.40613821148872375, 'val_acc': 0.8941999673843384, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.397971510887146, 'test_acc': 0.9032999873161316}]\n",
            "82 0.002109375 [{'running_loss': 0.022212151437997818, 'running_acc': 0.9935937523841858, 'loss': 0.021692682057619095, 'acc': 0.9943249821662903, 'val_loss': 0.40774309635162354, 'val_acc': 0.8982999920845032, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3973313570022583, 'test_acc': 0.9023000001907349}]\n",
            "83 0.002109375 [{'running_loss': 0.020779689773917198, 'running_acc': 0.9951562285423279, 'loss': 0.02040395513176918, 'acc': 0.994949996471405, 'val_loss': 0.4049774408340454, 'val_acc': 0.9007999897003174, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39686423540115356, 'test_acc': 0.9024999737739563}]\n",
            "84 0.002109375 [{'running_loss': 0.024631645530462265, 'running_acc': 0.9926562309265137, 'loss': 0.023762613534927368, 'acc': 0.9931749701499939, 'val_loss': 0.41013288497924805, 'val_acc': 0.9005999565124512, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3990734815597534, 'test_acc': 0.9019999504089355}]\n",
            "85 0.002109375 [{'running_loss': 0.017202185466885567, 'running_acc': 0.9965624809265137, 'loss': 0.020413871854543686, 'acc': 0.9947499632835388, 'val_loss': 0.41823309659957886, 'val_acc': 0.8996999859809875, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39787086844444275, 'test_acc': 0.9012999534606934}]\n",
            "86 0.002109375 [{'running_loss': 0.022453609853982925, 'running_acc': 0.992968738079071, 'loss': 0.02031361311674118, 'acc': 0.9946749806404114, 'val_loss': 0.4060930907726288, 'val_acc': 0.8989999890327454, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40117523074150085, 'test_acc': 0.9012999534606934}]\n",
            "87 0.002109375 [{'running_loss': 0.018846342340111732, 'running_acc': 0.9962499737739563, 'loss': 0.019323786720633507, 'acc': 0.9952999949455261, 'val_loss': 0.39134469628334045, 'val_acc': 0.9006999731063843, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39703893661499023, 'test_acc': 0.9027000069618225}]\n",
            "88 0.0010546875 [{'running_loss': 0.017818301916122437, 'running_acc': 0.99609375, 'loss': 0.01970447227358818, 'acc': 0.9948249459266663, 'val_loss': 0.3955272436141968, 'val_acc': 0.9016000032424927, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39788931608200073, 'test_acc': 0.901199996471405}]\n",
            "89 0.0010546875 [{'running_loss': 0.02076796442270279, 'running_acc': 0.9935937523841858, 'loss': 0.020450418815016747, 'acc': 0.9946249723434448, 'val_loss': 0.4041229784488678, 'val_acc': 0.8973999619483948, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4022042155265808, 'test_acc': 0.9023999571800232}]\n",
            "90 0.0010546875 [{'running_loss': 0.018553445115685463, 'running_acc': 0.9956249594688416, 'loss': 0.019454069435596466, 'acc': 0.9952499866485596, 'val_loss': 0.39923185110092163, 'val_acc': 0.8996999859809875, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40013745427131653, 'test_acc': 0.9021999835968018}]\n",
            "91 0.0010546875 [{'running_loss': 0.02238459698855877, 'running_acc': 0.9943749904632568, 'loss': 0.01968991383910179, 'acc': 0.9950249791145325, 'val_loss': 0.3989776372909546, 'val_acc': 0.8984999656677246, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4003255367279053, 'test_acc': 0.9025999903678894}]\n",
            "92 0.001 [{'running_loss': 0.01981569081544876, 'running_acc': 0.9946874976158142, 'loss': 0.020809145644307137, 'acc': 0.9947499632835388, 'val_loss': 0.41130805015563965, 'val_acc': 0.897599995136261, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3957767188549042, 'test_acc': 0.9023999571800232}]\n",
            "93 0.001 [{'running_loss': 0.018398044630885124, 'running_acc': 0.9953124523162842, 'loss': 0.01945067010819912, 'acc': 0.995449960231781, 'val_loss': 0.4071047306060791, 'val_acc': 0.9002999663352966, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4027765691280365, 'test_acc': 0.9014999866485596}]\n",
            "94 0.001 [{'running_loss': 0.020928505808115005, 'running_acc': 0.9953124523162842, 'loss': 0.020953647792339325, 'acc': 0.9946749806404114, 'val_loss': 0.4127279818058014, 'val_acc': 0.9021999835968018, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3995044231414795, 'test_acc': 0.9030999541282654}]\n",
            "95 0.001 [{'running_loss': 0.02417447790503502, 'running_acc': 0.9923437237739563, 'loss': 0.020864635705947876, 'acc': 0.9946999549865723, 'val_loss': 0.410983145236969, 'val_acc': 0.8991999626159668, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.399420827627182, 'test_acc': 0.9007999897003174}]\n",
            "96 0.001 [{'running_loss': 0.017784487456083298, 'running_acc': 0.9959374666213989, 'loss': 0.019656891003251076, 'acc': 0.9953999519348145, 'val_loss': 0.4058859646320343, 'val_acc': 0.8991999626159668, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3982693552970886, 'test_acc': 0.9027999639511108}]\n",
            "97 0.001 [{'running_loss': 0.01881910301744938, 'running_acc': 0.9957812428474426, 'loss': 0.019166313111782074, 'acc': 0.9952999949455261, 'val_loss': 0.3999320864677429, 'val_acc': 0.8999999761581421, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3992089033126831, 'test_acc': 0.9030999541282654}]\n",
            "98 0.001 [{'running_loss': 0.022432144731283188, 'running_acc': 0.9948437213897705, 'loss': 0.020172802731394768, 'acc': 0.9943999648094177, 'val_loss': 0.42206013202667236, 'val_acc': 0.894599974155426, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3984413146972656, 'test_acc': 0.901699960231781}]\n",
            "99 0.001 [{'running_loss': 0.02009904384613037, 'running_acc': 0.9946874976158142, 'loss': 0.018813829869031906, 'acc': 0.9952749609947205, 'val_loss': 0.40449732542037964, 'val_acc': 0.8974999785423279, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3974301517009735, 'test_acc': 0.9010999798774719}]\n",
            "100 0.001 [{'running_loss': 0.019875934347510338, 'running_acc': 0.9948437213897705, 'loss': 0.01901531219482422, 'acc': 0.9956749677658081, 'val_loss': 0.4055318236351013, 'val_acc': 0.8998000025749207, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4003683924674988, 'test_acc': 0.901699960231781}]\n",
            "101 0.001 [{'running_loss': 0.01809794269502163, 'running_acc': 0.9962499737739563, 'loss': 0.01883718930184841, 'acc': 0.9951249957084656, 'val_loss': 0.4130003750324249, 'val_acc': 0.9000999927520752, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4020257294178009, 'test_acc': 0.9019999504089355}]\n",
            "102 0.001 [{'running_loss': 0.01948407106101513, 'running_acc': 0.9950000047683716, 'loss': 0.019051644951105118, 'acc': 0.9956749677658081, 'val_loss': 0.40128013491630554, 'val_acc': 0.8999999761581421, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40194299817085266, 'test_acc': 0.9020999670028687}]\n",
            "103 0.001 [{'running_loss': 0.01800566352903843, 'running_acc': 0.9953124523162842, 'loss': 0.0192815400660038, 'acc': 0.9952249526977539, 'val_loss': 0.4264396131038666, 'val_acc': 0.8956999778747559, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4025880694389343, 'test_acc': 0.901699960231781}]\n",
            "104 0.001 [{'running_loss': 0.021421197801828384, 'running_acc': 0.9939061999320984, 'loss': 0.019989799708127975, 'acc': 0.9948749542236328, 'val_loss': 0.40939635038375854, 'val_acc': 0.898099958896637, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4044739603996277, 'test_acc': 0.9009000062942505}]\n",
            "105 0.001 [{'running_loss': 0.01839591935276985, 'running_acc': 0.9959374666213989, 'loss': 0.018616091459989548, 'acc': 0.9952749609947205, 'val_loss': 0.404870867729187, 'val_acc': 0.9001999497413635, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4027310013771057, 'test_acc': 0.9019999504089355}]\n",
            "106 0.001 [{'running_loss': 0.015620927326381207, 'running_acc': 0.9965624809265137, 'loss': 0.01717681810259819, 'acc': 0.9963499903678894, 'val_loss': 0.40963998436927795, 'val_acc': 0.8991000056266785, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40506336092948914, 'test_acc': 0.901699960231781}]\n",
            "107 0.001 [{'running_loss': 0.01484738290309906, 'running_acc': 0.9975000023841858, 'loss': 0.017129434272646904, 'acc': 0.9960249662399292, 'val_loss': 0.40821877121925354, 'val_acc': 0.8962000012397766, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4085140526294708, 'test_acc': 0.9014999866485596}]\n",
            "108 0.001 [{'running_loss': 0.01728704571723938, 'running_acc': 0.9957812428474426, 'loss': 0.01878097839653492, 'acc': 0.9956749677658081, 'val_loss': 0.41959571838378906, 'val_acc': 0.8981999754905701, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4016825258731842, 'test_acc': 0.9018999934196472}]\n",
            "109 0.001 [{'running_loss': 0.0189047884196043, 'running_acc': 0.9943749904632568, 'loss': 0.01925724744796753, 'acc': 0.9949749708175659, 'val_loss': 0.42644381523132324, 'val_acc': 0.8964999914169312, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4030677080154419, 'test_acc': 0.9021999835968018}]\n",
            "110 0.001 [{'running_loss': 0.018042583018541336, 'running_acc': 0.9956249594688416, 'loss': 0.01833346113562584, 'acc': 0.9951750040054321, 'val_loss': 0.4162188172340393, 'val_acc': 0.8974999785423279, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4025169909000397, 'test_acc': 0.9030999541282654}]\n",
            "111 0.001 [{'running_loss': 0.019539009779691696, 'running_acc': 0.9951562285423279, 'loss': 0.019409751519560814, 'acc': 0.9952749609947205, 'val_loss': 0.4048067033290863, 'val_acc': 0.902999997138977, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3955356180667877, 'test_acc': 0.9025999903678894}]\n",
            "112 0.001 [{'running_loss': 0.017584409564733505, 'running_acc': 0.9970312118530273, 'loss': 0.018301481381058693, 'acc': 0.9952749609947205, 'val_loss': 0.3969547152519226, 'val_acc': 0.898099958896637, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4032934606075287, 'test_acc': 0.9025999903678894}]\n",
            "113 0.001 [{'running_loss': 0.017675135284662247, 'running_acc': 0.99609375, 'loss': 0.018282175064086914, 'acc': 0.9956249594688416, 'val_loss': 0.4015781581401825, 'val_acc': 0.9005999565124512, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4063606858253479, 'test_acc': 0.9019999504089355}]\n",
            "114 0.001 [{'running_loss': 0.017811715602874756, 'running_acc': 0.9950000047683716, 'loss': 0.018217409029603004, 'acc': 0.9956749677658081, 'val_loss': 0.41658779978752136, 'val_acc': 0.8965999484062195, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40456292033195496, 'test_acc': 0.9021999835968018}]\n",
            "115 0.001 [{'running_loss': 0.018454281613230705, 'running_acc': 0.9953124523162842, 'loss': 0.018129270523786545, 'acc': 0.9955999851226807, 'val_loss': 0.4104820191860199, 'val_acc': 0.8962000012397766, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4033866226673126, 'test_acc': 0.9023000001907349}]\n",
            "116 0.001 [{'running_loss': 0.018637077882885933, 'running_acc': 0.9957812428474426, 'loss': 0.01903502270579338, 'acc': 0.9953500032424927, 'val_loss': 0.4148285388946533, 'val_acc': 0.8982999920845032, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4035438001155853, 'test_acc': 0.9027000069618225}]\n",
            "117 0.001 [{'running_loss': 0.018834466114640236, 'running_acc': 0.9943749904632568, 'loss': 0.017832230776548386, 'acc': 0.9954249858856201, 'val_loss': 0.4056597352027893, 'val_acc': 0.8981999754905701, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4028356969356537, 'test_acc': 0.9027999639511108}]\n",
            "118 0.001 [{'running_loss': 0.01755094900727272, 'running_acc': 0.9959374666213989, 'loss': 0.017872119322419167, 'acc': 0.9955250024795532, 'val_loss': 0.41004881262779236, 'val_acc': 0.9027999639511108, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40391820669174194, 'test_acc': 0.9019999504089355}]\n",
            "119 0.001 [{'running_loss': 0.020574795082211494, 'running_acc': 0.9951562285423279, 'loss': 0.01850452832877636, 'acc': 0.9954999685287476, 'val_loss': 0.40084564685821533, 'val_acc': 0.8992999792098999, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4029596745967865, 'test_acc': 0.9030999541282654}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-UR8pBmQGGeZ"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CbtnlyVu2IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for SGD on Batch Size 128 and Decayed Hyperparameter Schedule\n",
        "\n",
        "\n",
        "# final_lr_SGD = 0.001\n",
        "# train_loss_SGD = np.array([1.71183407, 1.28526127, 1.026667  , 0.85646063, 0.75804627,\n",
        "#        0.67841744, 0.62236983, 0.57827258, 0.54884487, 0.52208292,\n",
        "#        0.49632096, 0.47449213, 0.45768401, 0.43774241, 0.42715123,\n",
        "#        0.41376776, 0.39789671, 0.38827759, 0.3779234 , 0.37062988,\n",
        "#        0.36042044, 0.27575472, 0.25966477, 0.25247639, 0.25004151,\n",
        "#        0.2443497 , 0.23836718, 0.2363598 , 0.23331961, 0.17285876,\n",
        "#        0.16471817, 0.15806176, 0.15096639, 0.14549853, 0.14816734,\n",
        "#        0.14103965, 0.1467904 , 0.11151568, 0.09702439, 0.09195668,\n",
        "#        0.08726015, 0.0890381 , 0.08671602, 0.08106352, 0.07830024,\n",
        "#        0.06416006, 0.06036014, 0.05559513, 0.05471288, 0.05302291,\n",
        "#        0.0506792 , 0.05034265, 0.04890452, 0.04670165, 0.04607155,\n",
        "#        0.04658717, 0.04401184, 0.03946382, 0.0355909 , 0.03395389,\n",
        "#        0.03294867, 0.03182639, 0.03106846, 0.03110384, 0.03141483,\n",
        "#        0.02983408, 0.03047764, 0.03141082, 0.02924451, 0.02773057,\n",
        "#        0.02435952, 0.02646674, 0.02466619, 0.02501595, 0.02370333,\n",
        "#        0.02333691, 0.02359404, 0.0233112 , 0.02237226, 0.02250148,\n",
        "#        0.02328263, 0.02088352, 0.02169268, 0.02040396, 0.02376261,\n",
        "#        0.02041387, 0.02031361, 0.01932379, 0.01970447, 0.02045042,\n",
        "#        0.01945407, 0.01968991, 0.02080915, 0.01945067, 0.02095365,\n",
        "#        0.02086464, 0.01965689, 0.01916631, 0.0201728 , 0.01881383,\n",
        "#        0.01901531, 0.01883719, 0.01905164, 0.01928154, 0.0199898 ,\n",
        "#        0.01861609, 0.01717682, 0.01712943, 0.01878098, 0.01925725,\n",
        "#        0.01833346, 0.01940975, 0.01830148, 0.01828218, 0.01821741,\n",
        "#        0.01812927, 0.01903502, 0.01783223, 0.01787212, 0.01850453])\n",
        "# train_accuracy_SGD = np.array([0.357775  , 0.53219998, 0.63207501, 0.69979995, 0.73589998,\n",
        "#        0.763475  , 0.78775001, 0.800125  , 0.81042498, 0.82104999,\n",
        "#        0.82792497, 0.83494997, 0.84132499, 0.84907496, 0.85219997,\n",
        "#        0.85705   , 0.86429995, 0.865825  , 0.86899996, 0.87277496,\n",
        "#        0.87594998, 0.90652496, 0.90962499, 0.91249996, 0.91402495,\n",
        "#        0.91544998, 0.91652501, 0.91964996, 0.92065001, 0.94042498,\n",
        "#        0.94437498, 0.94544995, 0.94834995, 0.95094997, 0.94854999,\n",
        "#        0.9508    , 0.94935   , 0.96127498, 0.9677    , 0.96934998,\n",
        "#        0.97147501, 0.97047496, 0.97062498, 0.973625  , 0.97427499,\n",
        "#        0.97899997, 0.98139995, 0.98232496, 0.98234999, 0.98314995,\n",
        "#        0.98422498, 0.98412496, 0.98449999, 0.98534995, 0.98602498,\n",
        "#        0.985425  , 0.98667496, 0.98789996, 0.98957497, 0.99054998,\n",
        "#        0.99049997, 0.99094999, 0.99094999, 0.99102497, 0.99034995,\n",
        "#        0.99177498, 0.99135   , 0.99102497, 0.99169999, 0.9921    ,\n",
        "#        0.99329996, 0.99254996, 0.99307495, 0.99282497, 0.99327499,\n",
        "#        0.99384999, 0.99362499, 0.99367499, 0.99362499, 0.99437499,\n",
        "#        0.99355   , 0.99495   , 0.99432498, 0.99495   , 0.99317497,\n",
        "#        0.99474996, 0.99467498, 0.99529999, 0.99482495, 0.99462497,\n",
        "#        0.99524999, 0.99502498, 0.99474996, 0.99544996, 0.99467498,\n",
        "#        0.99469995, 0.99539995, 0.99529999, 0.99439996, 0.99527496,\n",
        "#        0.99567497, 0.995125  , 0.99567497, 0.99522495, 0.99487495,\n",
        "#        0.99527496, 0.99634999, 0.99602497, 0.99567497, 0.99497497,\n",
        "#        0.995175  , 0.99527496, 0.99527496, 0.99562496, 0.99567497,\n",
        "#        0.99559999, 0.99535   , 0.99542499, 0.995525  , 0.99549997])\n",
        "# valid_loss_SGD = np.array([1.73122966, 2.30423307, 1.22227466, 2.47773314, 1.5208379 ,\n",
        "#        0.80600482, 0.84137398, 0.99803269, 1.27458441, 0.88261193,\n",
        "#        1.06969869, 0.91364723, 0.96980542, 0.89699972, 0.83853579,\n",
        "#        0.83376217, 0.71260041, 1.81022537, 1.47255492, 1.19162679,\n",
        "#        0.89813977, 0.54259586, 0.53695595, 0.46139103, 0.50377321,\n",
        "#        0.59190744, 0.82141846, 0.48486412, 0.4967874 , 0.40840662,\n",
        "#        0.44597575, 0.38697013, 0.40809643, 0.43756872, 0.42477635,\n",
        "#        0.51770598, 0.45272756, 0.3758288 , 0.35866839, 0.37360278,\n",
        "#        0.36333883, 0.40663061, 0.44482943, 0.44368881, 0.39615053,\n",
        "#        0.3677786 , 0.36932015, 0.37636629, 0.37281266, 0.38775149,\n",
        "#        0.37449867, 0.37452367, 0.39547271, 0.41018614, 0.38931879,\n",
        "#        0.37673032, 0.39178559, 0.37869397, 0.39053106, 0.38991183,\n",
        "#        0.39134651, 0.38680965, 0.39379802, 0.39160994, 0.40651801,\n",
        "#        0.39456013, 0.39439338, 0.39015308, 0.40463293, 0.38241422,\n",
        "#        0.39269841, 0.38796908, 0.3977803 , 0.40445986, 0.40751547,\n",
        "#        0.4063307 , 0.40358242, 0.40028176, 0.40996239, 0.38949591,\n",
        "#        0.40242451, 0.40613821, 0.4077431 , 0.40497744, 0.41013288,\n",
        "#        0.4182331 , 0.40609309, 0.3913447 , 0.39552724, 0.40412298,\n",
        "#        0.39923185, 0.39897764, 0.41130805, 0.40710473, 0.41272798,\n",
        "#        0.41098315, 0.40588596, 0.39993209, 0.42206013, 0.40449733,\n",
        "#        0.40553182, 0.41300038, 0.40128013, 0.42643961, 0.40939635,\n",
        "#        0.40487087, 0.40963998, 0.40821877, 0.41959572, 0.42644382,\n",
        "#        0.41621882, 0.4048067 , 0.39695472, 0.40157816, 0.4165878 ,\n",
        "#        0.41048202, 0.41482854, 0.40565974, 0.41004881, 0.40084565])\n",
        "# valid_accuracy_SGD = np.array([0.35330001, 0.33919999, 0.56299996, 0.33750001, 0.55979997,\n",
        "#        0.72999996, 0.70139998, 0.66979998, 0.63529998, 0.71079999,\n",
        "#        0.66289997, 0.68019998, 0.68119997, 0.71359998, 0.72929996,\n",
        "#        0.74149996, 0.76859999, 0.57669997, 0.61750001, 0.62819999,\n",
        "#        0.7252    , 0.82749999, 0.82519996, 0.84819996, 0.83519995,\n",
        "#        0.81149995, 0.75669998, 0.84179997, 0.8344    , 0.87079996,\n",
        "#        0.86009997, 0.8775    , 0.87169999, 0.86449999, 0.86809999,\n",
        "#        0.84309995, 0.8592    , 0.88529998, 0.89099997, 0.88609999,\n",
        "#        0.89029998, 0.88099998, 0.87129998, 0.87099999, 0.8847    ,\n",
        "#        0.8937    , 0.8951    , 0.89159995, 0.89399999, 0.89709997,\n",
        "#        0.89359999, 0.89449996, 0.89120001, 0.88729995, 0.8915    ,\n",
        "#        0.89459997, 0.89459997, 0.8962    , 0.8955    , 0.8969    ,\n",
        "#        0.89469999, 0.89959997, 0.89839995, 0.90139997, 0.89449996,\n",
        "#        0.898     , 0.89919996, 0.89589995, 0.89709997, 0.89929998,\n",
        "#        0.90109998, 0.90019995, 0.90139997, 0.89749998, 0.89699996,\n",
        "#        0.89609998, 0.8994    , 0.89679998, 0.89829999, 0.89849997,\n",
        "#        0.8987    , 0.89419997, 0.89829999, 0.90079999, 0.90059996,\n",
        "#        0.89969999, 0.89899999, 0.90069997, 0.9016    , 0.89739996,\n",
        "#        0.89969999, 0.89849997, 0.8976    , 0.90029997, 0.90219998,\n",
        "#        0.89919996, 0.89919996, 0.89999998, 0.89459997, 0.89749998,\n",
        "#        0.8998    , 0.90009999, 0.89999998, 0.89569998, 0.89809996,\n",
        "#        0.90019995, 0.89910001, 0.8962    , 0.89819998, 0.89649999,\n",
        "#        0.89749998, 0.903     , 0.89809996, 0.90059996, 0.89659995,\n",
        "#        0.8962    , 0.89829999, 0.89819998, 0.90279996, 0.89929998])\n",
        "# test_loss_SGD = np.array([1.64188588, 2.38524246, 1.21967781, 2.73059344, 1.55100298,\n",
        "#        0.81410384, 0.8585695 , 1.05481005, 1.32812321, 0.87950957,\n",
        "#        1.1848495 , 0.93552524, 1.1010803 , 0.98213547, 0.83097184,\n",
        "#        0.95674568, 0.7373367 , 2.11745644, 1.59605455, 1.29665112,\n",
        "#        0.94242525, 0.53944367, 0.56099665, 0.44975933, 0.49403879,\n",
        "#        0.63643163, 0.94555831, 0.48596478, 0.49059233, 0.40547591,\n",
        "#        0.46340156, 0.38806054, 0.41738346, 0.45480436, 0.44443271,\n",
        "#        0.52091473, 0.4828575 , 0.36450744, 0.34751353, 0.39073077,\n",
        "#        0.37310904, 0.43214041, 0.45991483, 0.46153656, 0.40122142,\n",
        "#        0.3614161 , 0.36506703, 0.37520042, 0.37759644, 0.3741492 ,\n",
        "#        0.38264707, 0.38270789, 0.39315644, 0.41219586, 0.40006939,\n",
        "#        0.38367864, 0.40689436, 0.38289958, 0.38433042, 0.38938457,\n",
        "#        0.39129019, 0.39563847, 0.38606426, 0.39490119, 0.39954743,\n",
        "#        0.39493337, 0.39529088, 0.39838091, 0.40936947, 0.3892439 ,\n",
        "#        0.39534977, 0.39567327, 0.39033332, 0.39459291, 0.39949736,\n",
        "#        0.39513731, 0.39700606, 0.3966141 , 0.39962494, 0.3935979 ,\n",
        "#        0.39304924, 0.39797151, 0.39733136, 0.39686424, 0.39907348,\n",
        "#        0.39787087, 0.40117523, 0.39703894, 0.39788932, 0.40220422,\n",
        "#        0.40013745, 0.40032554, 0.39577672, 0.40277657, 0.39950442,\n",
        "#        0.39942083, 0.39826936, 0.3992089 , 0.39844131, 0.39743015,\n",
        "#        0.40036839, 0.40202573, 0.401943  , 0.40258807, 0.40447396,\n",
        "#        0.402731  , 0.40506336, 0.40851405, 0.40168253, 0.40306771,\n",
        "#        0.40251699, 0.39553562, 0.40329346, 0.40636069, 0.40456292,\n",
        "#        0.40338662, 0.4035438 , 0.4028357 , 0.40391821, 0.40295967])\n",
        "# test_accuracy_SGD = np.array([0.38479999, 0.3565    , 0.57550001, 0.3423    , 0.579     ,\n",
        "#        0.73449999, 0.71169996, 0.66229999, 0.63769996, 0.72439998,\n",
        "#        0.65029997, 0.68419999, 0.67019999, 0.70059997, 0.74329996,\n",
        "#        0.72709996, 0.77059996, 0.55979997, 0.62650001, 0.61699998,\n",
        "#        0.72569996, 0.83039999, 0.82539999, 0.8531    , 0.84509999,\n",
        "#        0.80559999, 0.74180001, 0.84649998, 0.84189999, 0.87459999,\n",
        "#        0.86179996, 0.88119996, 0.87619996, 0.86129999, 0.86949998,\n",
        "#        0.84869999, 0.85780001, 0.89109999, 0.89539999, 0.88889998,\n",
        "#        0.89309996, 0.87579995, 0.87769997, 0.87360001, 0.88949996,\n",
        "#        0.89849997, 0.89749998, 0.89639997, 0.89969999, 0.8987    ,\n",
        "#        0.89819998, 0.89779997, 0.89589995, 0.89379996, 0.8944    ,\n",
        "#        0.8987    , 0.8933    , 0.89889997, 0.90059996, 0.89989996,\n",
        "#        0.90069997, 0.89989996, 0.90169996, 0.90239996, 0.9016    ,\n",
        "#        0.90029997, 0.90189999, 0.89910001, 0.89919996, 0.9034    ,\n",
        "#        0.90239996, 0.90239996, 0.90329999, 0.9034    , 0.90309995,\n",
        "#        0.90179998, 0.9023    , 0.90319997, 0.90369999, 0.90379995,\n",
        "#        0.90489995, 0.90329999, 0.9023    , 0.90249997, 0.90199995,\n",
        "#        0.90129995, 0.90129995, 0.90270001, 0.9012    , 0.90239996,\n",
        "#        0.90219998, 0.90259999, 0.90239996, 0.90149999, 0.90309995,\n",
        "#        0.90079999, 0.90279996, 0.90309995, 0.90169996, 0.90109998,\n",
        "#        0.90169996, 0.90199995, 0.90209997, 0.90169996, 0.90090001,\n",
        "#        0.90199995, 0.90169996, 0.90149999, 0.90189999, 0.90219998,\n",
        "#        0.90309995, 0.90259999, 0.90259999, 0.90199995, 0.90219998,\n",
        "#        0.9023    , 0.90270001, 0.90279996, 0.90199995, 0.90309995])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FO9f4813byh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e879e2f-a06b-42ed-af6b-8d40bbad2ae2"
      },
      "source": [
        "print(\"Final Learning Rate reached at the end of training: \", final_lr_SGD)\n",
        "print(\"Training Loss Set: \", repr(train_loss_SGD))\n",
        "print(\"Training Accuracy Set: \", repr(train_accuracy_SGD))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_SGD))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_SGD))\n",
        "print(\"Test Loss Set: \", repr(test_loss_SGD))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_SGD))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Learning Rate reached at the end of training:  0.001\n",
            "Training Loss Set:  array([1.71183407, 1.28526127, 1.026667  , 0.85646063, 0.75804627,\n",
            "       0.67841744, 0.62236983, 0.57827258, 0.54884487, 0.52208292,\n",
            "       0.49632096, 0.47449213, 0.45768401, 0.43774241, 0.42715123,\n",
            "       0.41376776, 0.39789671, 0.38827759, 0.3779234 , 0.37062988,\n",
            "       0.36042044, 0.27575472, 0.25966477, 0.25247639, 0.25004151,\n",
            "       0.2443497 , 0.23836718, 0.2363598 , 0.23331961, 0.17285876,\n",
            "       0.16471817, 0.15806176, 0.15096639, 0.14549853, 0.14816734,\n",
            "       0.14103965, 0.1467904 , 0.11151568, 0.09702439, 0.09195668,\n",
            "       0.08726015, 0.0890381 , 0.08671602, 0.08106352, 0.07830024,\n",
            "       0.06416006, 0.06036014, 0.05559513, 0.05471288, 0.05302291,\n",
            "       0.0506792 , 0.05034265, 0.04890452, 0.04670165, 0.04607155,\n",
            "       0.04658717, 0.04401184, 0.03946382, 0.0355909 , 0.03395389,\n",
            "       0.03294867, 0.03182639, 0.03106846, 0.03110384, 0.03141483,\n",
            "       0.02983408, 0.03047764, 0.03141082, 0.02924451, 0.02773057,\n",
            "       0.02435952, 0.02646674, 0.02466619, 0.02501595, 0.02370333,\n",
            "       0.02333691, 0.02359404, 0.0233112 , 0.02237226, 0.02250148,\n",
            "       0.02328263, 0.02088352, 0.02169268, 0.02040396, 0.02376261,\n",
            "       0.02041387, 0.02031361, 0.01932379, 0.01970447, 0.02045042,\n",
            "       0.01945407, 0.01968991, 0.02080915, 0.01945067, 0.02095365,\n",
            "       0.02086464, 0.01965689, 0.01916631, 0.0201728 , 0.01881383,\n",
            "       0.01901531, 0.01883719, 0.01905164, 0.01928154, 0.0199898 ,\n",
            "       0.01861609, 0.01717682, 0.01712943, 0.01878098, 0.01925725,\n",
            "       0.01833346, 0.01940975, 0.01830148, 0.01828218, 0.01821741,\n",
            "       0.01812927, 0.01903502, 0.01783223, 0.01787212, 0.01850453])\n",
            "Training Accuracy Set:  array([0.357775  , 0.53219998, 0.63207501, 0.69979995, 0.73589998,\n",
            "       0.763475  , 0.78775001, 0.800125  , 0.81042498, 0.82104999,\n",
            "       0.82792497, 0.83494997, 0.84132499, 0.84907496, 0.85219997,\n",
            "       0.85705   , 0.86429995, 0.865825  , 0.86899996, 0.87277496,\n",
            "       0.87594998, 0.90652496, 0.90962499, 0.91249996, 0.91402495,\n",
            "       0.91544998, 0.91652501, 0.91964996, 0.92065001, 0.94042498,\n",
            "       0.94437498, 0.94544995, 0.94834995, 0.95094997, 0.94854999,\n",
            "       0.9508    , 0.94935   , 0.96127498, 0.9677    , 0.96934998,\n",
            "       0.97147501, 0.97047496, 0.97062498, 0.973625  , 0.97427499,\n",
            "       0.97899997, 0.98139995, 0.98232496, 0.98234999, 0.98314995,\n",
            "       0.98422498, 0.98412496, 0.98449999, 0.98534995, 0.98602498,\n",
            "       0.985425  , 0.98667496, 0.98789996, 0.98957497, 0.99054998,\n",
            "       0.99049997, 0.99094999, 0.99094999, 0.99102497, 0.99034995,\n",
            "       0.99177498, 0.99135   , 0.99102497, 0.99169999, 0.9921    ,\n",
            "       0.99329996, 0.99254996, 0.99307495, 0.99282497, 0.99327499,\n",
            "       0.99384999, 0.99362499, 0.99367499, 0.99362499, 0.99437499,\n",
            "       0.99355   , 0.99495   , 0.99432498, 0.99495   , 0.99317497,\n",
            "       0.99474996, 0.99467498, 0.99529999, 0.99482495, 0.99462497,\n",
            "       0.99524999, 0.99502498, 0.99474996, 0.99544996, 0.99467498,\n",
            "       0.99469995, 0.99539995, 0.99529999, 0.99439996, 0.99527496,\n",
            "       0.99567497, 0.995125  , 0.99567497, 0.99522495, 0.99487495,\n",
            "       0.99527496, 0.99634999, 0.99602497, 0.99567497, 0.99497497,\n",
            "       0.995175  , 0.99527496, 0.99527496, 0.99562496, 0.99567497,\n",
            "       0.99559999, 0.99535   , 0.99542499, 0.995525  , 0.99549997])\n",
            "Validation Loss Set:  array([1.73122966, 2.30423307, 1.22227466, 2.47773314, 1.5208379 ,\n",
            "       0.80600482, 0.84137398, 0.99803269, 1.27458441, 0.88261193,\n",
            "       1.06969869, 0.91364723, 0.96980542, 0.89699972, 0.83853579,\n",
            "       0.83376217, 0.71260041, 1.81022537, 1.47255492, 1.19162679,\n",
            "       0.89813977, 0.54259586, 0.53695595, 0.46139103, 0.50377321,\n",
            "       0.59190744, 0.82141846, 0.48486412, 0.4967874 , 0.40840662,\n",
            "       0.44597575, 0.38697013, 0.40809643, 0.43756872, 0.42477635,\n",
            "       0.51770598, 0.45272756, 0.3758288 , 0.35866839, 0.37360278,\n",
            "       0.36333883, 0.40663061, 0.44482943, 0.44368881, 0.39615053,\n",
            "       0.3677786 , 0.36932015, 0.37636629, 0.37281266, 0.38775149,\n",
            "       0.37449867, 0.37452367, 0.39547271, 0.41018614, 0.38931879,\n",
            "       0.37673032, 0.39178559, 0.37869397, 0.39053106, 0.38991183,\n",
            "       0.39134651, 0.38680965, 0.39379802, 0.39160994, 0.40651801,\n",
            "       0.39456013, 0.39439338, 0.39015308, 0.40463293, 0.38241422,\n",
            "       0.39269841, 0.38796908, 0.3977803 , 0.40445986, 0.40751547,\n",
            "       0.4063307 , 0.40358242, 0.40028176, 0.40996239, 0.38949591,\n",
            "       0.40242451, 0.40613821, 0.4077431 , 0.40497744, 0.41013288,\n",
            "       0.4182331 , 0.40609309, 0.3913447 , 0.39552724, 0.40412298,\n",
            "       0.39923185, 0.39897764, 0.41130805, 0.40710473, 0.41272798,\n",
            "       0.41098315, 0.40588596, 0.39993209, 0.42206013, 0.40449733,\n",
            "       0.40553182, 0.41300038, 0.40128013, 0.42643961, 0.40939635,\n",
            "       0.40487087, 0.40963998, 0.40821877, 0.41959572, 0.42644382,\n",
            "       0.41621882, 0.4048067 , 0.39695472, 0.40157816, 0.4165878 ,\n",
            "       0.41048202, 0.41482854, 0.40565974, 0.41004881, 0.40084565])\n",
            "Validation Accuracy Set:  array([0.35330001, 0.33919999, 0.56299996, 0.33750001, 0.55979997,\n",
            "       0.72999996, 0.70139998, 0.66979998, 0.63529998, 0.71079999,\n",
            "       0.66289997, 0.68019998, 0.68119997, 0.71359998, 0.72929996,\n",
            "       0.74149996, 0.76859999, 0.57669997, 0.61750001, 0.62819999,\n",
            "       0.7252    , 0.82749999, 0.82519996, 0.84819996, 0.83519995,\n",
            "       0.81149995, 0.75669998, 0.84179997, 0.8344    , 0.87079996,\n",
            "       0.86009997, 0.8775    , 0.87169999, 0.86449999, 0.86809999,\n",
            "       0.84309995, 0.8592    , 0.88529998, 0.89099997, 0.88609999,\n",
            "       0.89029998, 0.88099998, 0.87129998, 0.87099999, 0.8847    ,\n",
            "       0.8937    , 0.8951    , 0.89159995, 0.89399999, 0.89709997,\n",
            "       0.89359999, 0.89449996, 0.89120001, 0.88729995, 0.8915    ,\n",
            "       0.89459997, 0.89459997, 0.8962    , 0.8955    , 0.8969    ,\n",
            "       0.89469999, 0.89959997, 0.89839995, 0.90139997, 0.89449996,\n",
            "       0.898     , 0.89919996, 0.89589995, 0.89709997, 0.89929998,\n",
            "       0.90109998, 0.90019995, 0.90139997, 0.89749998, 0.89699996,\n",
            "       0.89609998, 0.8994    , 0.89679998, 0.89829999, 0.89849997,\n",
            "       0.8987    , 0.89419997, 0.89829999, 0.90079999, 0.90059996,\n",
            "       0.89969999, 0.89899999, 0.90069997, 0.9016    , 0.89739996,\n",
            "       0.89969999, 0.89849997, 0.8976    , 0.90029997, 0.90219998,\n",
            "       0.89919996, 0.89919996, 0.89999998, 0.89459997, 0.89749998,\n",
            "       0.8998    , 0.90009999, 0.89999998, 0.89569998, 0.89809996,\n",
            "       0.90019995, 0.89910001, 0.8962    , 0.89819998, 0.89649999,\n",
            "       0.89749998, 0.903     , 0.89809996, 0.90059996, 0.89659995,\n",
            "       0.8962    , 0.89829999, 0.89819998, 0.90279996, 0.89929998])\n",
            "Test Loss Set:  array([1.64188588, 2.38524246, 1.21967781, 2.73059344, 1.55100298,\n",
            "       0.81410384, 0.8585695 , 1.05481005, 1.32812321, 0.87950957,\n",
            "       1.1848495 , 0.93552524, 1.1010803 , 0.98213547, 0.83097184,\n",
            "       0.95674568, 0.7373367 , 2.11745644, 1.59605455, 1.29665112,\n",
            "       0.94242525, 0.53944367, 0.56099665, 0.44975933, 0.49403879,\n",
            "       0.63643163, 0.94555831, 0.48596478, 0.49059233, 0.40547591,\n",
            "       0.46340156, 0.38806054, 0.41738346, 0.45480436, 0.44443271,\n",
            "       0.52091473, 0.4828575 , 0.36450744, 0.34751353, 0.39073077,\n",
            "       0.37310904, 0.43214041, 0.45991483, 0.46153656, 0.40122142,\n",
            "       0.3614161 , 0.36506703, 0.37520042, 0.37759644, 0.3741492 ,\n",
            "       0.38264707, 0.38270789, 0.39315644, 0.41219586, 0.40006939,\n",
            "       0.38367864, 0.40689436, 0.38289958, 0.38433042, 0.38938457,\n",
            "       0.39129019, 0.39563847, 0.38606426, 0.39490119, 0.39954743,\n",
            "       0.39493337, 0.39529088, 0.39838091, 0.40936947, 0.3892439 ,\n",
            "       0.39534977, 0.39567327, 0.39033332, 0.39459291, 0.39949736,\n",
            "       0.39513731, 0.39700606, 0.3966141 , 0.39962494, 0.3935979 ,\n",
            "       0.39304924, 0.39797151, 0.39733136, 0.39686424, 0.39907348,\n",
            "       0.39787087, 0.40117523, 0.39703894, 0.39788932, 0.40220422,\n",
            "       0.40013745, 0.40032554, 0.39577672, 0.40277657, 0.39950442,\n",
            "       0.39942083, 0.39826936, 0.3992089 , 0.39844131, 0.39743015,\n",
            "       0.40036839, 0.40202573, 0.401943  , 0.40258807, 0.40447396,\n",
            "       0.402731  , 0.40506336, 0.40851405, 0.40168253, 0.40306771,\n",
            "       0.40251699, 0.39553562, 0.40329346, 0.40636069, 0.40456292,\n",
            "       0.40338662, 0.4035438 , 0.4028357 , 0.40391821, 0.40295967])\n",
            "Test Accuracy Set:  array([0.38479999, 0.3565    , 0.57550001, 0.3423    , 0.579     ,\n",
            "       0.73449999, 0.71169996, 0.66229999, 0.63769996, 0.72439998,\n",
            "       0.65029997, 0.68419999, 0.67019999, 0.70059997, 0.74329996,\n",
            "       0.72709996, 0.77059996, 0.55979997, 0.62650001, 0.61699998,\n",
            "       0.72569996, 0.83039999, 0.82539999, 0.8531    , 0.84509999,\n",
            "       0.80559999, 0.74180001, 0.84649998, 0.84189999, 0.87459999,\n",
            "       0.86179996, 0.88119996, 0.87619996, 0.86129999, 0.86949998,\n",
            "       0.84869999, 0.85780001, 0.89109999, 0.89539999, 0.88889998,\n",
            "       0.89309996, 0.87579995, 0.87769997, 0.87360001, 0.88949996,\n",
            "       0.89849997, 0.89749998, 0.89639997, 0.89969999, 0.8987    ,\n",
            "       0.89819998, 0.89779997, 0.89589995, 0.89379996, 0.8944    ,\n",
            "       0.8987    , 0.8933    , 0.89889997, 0.90059996, 0.89989996,\n",
            "       0.90069997, 0.89989996, 0.90169996, 0.90239996, 0.9016    ,\n",
            "       0.90029997, 0.90189999, 0.89910001, 0.89919996, 0.9034    ,\n",
            "       0.90239996, 0.90239996, 0.90329999, 0.9034    , 0.90309995,\n",
            "       0.90179998, 0.9023    , 0.90319997, 0.90369999, 0.90379995,\n",
            "       0.90489995, 0.90329999, 0.9023    , 0.90249997, 0.90199995,\n",
            "       0.90129995, 0.90129995, 0.90270001, 0.9012    , 0.90239996,\n",
            "       0.90219998, 0.90259999, 0.90239996, 0.90149999, 0.90309995,\n",
            "       0.90079999, 0.90279996, 0.90309995, 0.90169996, 0.90109998,\n",
            "       0.90169996, 0.90199995, 0.90209997, 0.90169996, 0.90090001,\n",
            "       0.90199995, 0.90169996, 0.90149999, 0.90189999, 0.90219998,\n",
            "       0.90309995, 0.90259999, 0.90259999, 0.90199995, 0.90219998,\n",
            "       0.9023    , 0.90270001, 0.90279996, 0.90199995, 0.90309995])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PAbXWpCgHHtS"
      },
      "source": [
        "##Heavy Ball"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QkBjIoIqBbEG",
        "colab": {}
      },
      "source": [
        "def decayed_stats_HeavyBall(lr, momentum, decay, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_HeavyBall = np.zeros(epochs)\n",
        "  train_accuracy_HeavyBall = np.zeros(epochs)\n",
        "  valid_loss_HeavyBall = np.zeros(epochs)\n",
        "  valid_accuracy_HeavyBall = np.zeros(epochs)\n",
        "  test_accuracy_HeavyBall = np.zeros(epochs)\n",
        "  test_loss_HeavyBall = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  #Initilise validation set error as criteria at change point\n",
        "  error = 100\n",
        "  prev_error = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #Creates a deep copy of the parameter and gradient tensors and makes them shareable to enable re-use of Resnet modules multiple times\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = optim.SGD(model.parameters(), lr = lr, momentum = momentum, weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'],  verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation error reduced by more than 0.2% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_error-error)/prev_error < 0.002) and epoch!=0:\n",
        "        \n",
        "        if lr/decay < 0.001:\n",
        "         lr = 0.001\n",
        "        else:\n",
        "            lr=lr/decay\n",
        "\n",
        "      else:\n",
        "          lr=lr\n",
        "\n",
        "      prev_error = error\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_HeavyBall[epoch] = result[0]['loss']\n",
        "    train_accuracy_HeavyBall[epoch] = result[0]['acc']\n",
        "    valid_loss_HeavyBall[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_HeavyBall[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_HeavyBall[epoch] = result[0]['test_acc']\n",
        "    test_loss_HeavyBall[epoch] = result[0]['test_loss']\n",
        "\n",
        "    error = 1 - result[0]['val_acc']\n",
        "\n",
        "    print(epoch, lr, momentum, result)\n",
        "\n",
        "  return lr, train_loss_HeavyBall, train_accuracy_HeavyBall, valid_loss_HeavyBall, valid_accuracy_HeavyBall, test_loss_HeavyBall, test_accuracy_HeavyBall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ki_J60nmeWam",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ed68fb5-77f5-4fda-f26f-124d16b8a1dc"
      },
      "source": [
        "final_lr_HeavyBall, train_loss_HeavyBall, train_accuracy_HeavyBall, valid_loss_HeavyBall, valid_accuracy_HeavyBall, test_loss_HeavyBall, test_accuracy_HeavyBall = decayed_stats_HeavyBall(0.27, 0.5, 2, 120, 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.27 0.5 [{'running_loss': 1.4122717380523682, 'running_acc': 0.47968748211860657, 'loss': 1.6455152034759521, 'acc': 0.3839249908924103, 'val_loss': 1.7173004150390625, 'val_acc': 0.407399982213974, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.7037882804870605, 'test_acc': 0.42089998722076416}]\n",
            "1 0.27 0.5 [{'running_loss': 1.0947754383087158, 'running_acc': 0.6037499904632568, 'loss': 1.2118505239486694, 'acc': 0.5630999803543091, 'val_loss': 1.3329198360443115, 'val_acc': 0.5327000021934509, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.305202603340149, 'test_acc': 0.5383999943733215}]\n",
            "2 0.27 0.5 [{'running_loss': 0.8957086205482483, 'running_acc': 0.6773437261581421, 'loss': 0.9726648926734924, 'acc': 0.6537249684333801, 'val_loss': 1.4468469619750977, 'val_acc': 0.5309000015258789, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4749679565429688, 'test_acc': 0.5345999598503113}]\n",
            "3 0.27 0.5 [{'running_loss': 0.7700963616371155, 'running_acc': 0.7345312237739563, 'loss': 0.8114712834358215, 'acc': 0.7165499925613403, 'val_loss': 1.4037754535675049, 'val_acc': 0.5564999580383301, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4283274412155151, 'test_acc': 0.5634999871253967}]\n",
            "4 0.135 0.5 [{'running_loss': 0.6951158046722412, 'running_acc': 0.7593749761581421, 'loss': 0.7118450999259949, 'acc': 0.7539499998092651, 'val_loss': 1.2576736211776733, 'val_acc': 0.5651000142097473, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.2589811086654663, 'test_acc': 0.5848000049591064}]\n",
            "5 0.135 0.5 [{'running_loss': 0.5604655146598816, 'running_acc': 0.8056249618530273, 'loss': 0.5729542970657349, 'acc': 0.8022249937057495, 'val_loss': 0.7308769822120667, 'val_acc': 0.7487999796867371, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.6994919180870056, 'test_acc': 0.762499988079071}]\n",
            "6 0.135 0.5 [{'running_loss': 0.5315866470336914, 'running_acc': 0.8167187571525574, 'loss': 0.5349392294883728, 'acc': 0.8145749568939209, 'val_loss': 0.8020491003990173, 'val_acc': 0.7346999645233154, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8235801458358765, 'test_acc': 0.739799976348877}]\n",
            "7 0.135 0.5 [{'running_loss': 0.506317675113678, 'running_acc': 0.8190624713897705, 'loss': 0.5142866969108582, 'acc': 0.8227499723434448, 'val_loss': 0.8374754786491394, 'val_acc': 0.7210999727249146, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8636606931686401, 'test_acc': 0.7286999821662903}]\n",
            "8 0.0675 0.5 [{'running_loss': 0.4973919987678528, 'running_acc': 0.8274999856948853, 'loss': 0.4889587163925171, 'acc': 0.8303999900817871, 'val_loss': 0.8091529607772827, 'val_acc': 0.7276999950408936, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7725645303726196, 'test_acc': 0.7407999634742737}]\n",
            "9 0.0675 0.5 [{'running_loss': 0.39067062735557556, 'running_acc': 0.8642187118530273, 'loss': 0.40700972080230713, 'acc': 0.8603249788284302, 'val_loss': 0.5613657832145691, 'val_acc': 0.8115999698638916, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5557823181152344, 'test_acc': 0.8130999803543091}]\n",
            "10 0.0675 0.5 [{'running_loss': 0.3649492561817169, 'running_acc': 0.87953120470047, 'loss': 0.3894985616207123, 'acc': 0.8665499687194824, 'val_loss': 0.5271844863891602, 'val_acc': 0.8270999789237976, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5155701637268066, 'test_acc': 0.8351999521255493}]\n",
            "11 0.0675 0.5 [{'running_loss': 0.3927721381187439, 'running_acc': 0.8646875023841858, 'loss': 0.38079366087913513, 'acc': 0.8682249784469604, 'val_loss': 0.5582062602043152, 'val_acc': 0.8145999908447266, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5583797693252563, 'test_acc': 0.8136000037193298}]\n",
            "12 0.03375 0.5 [{'running_loss': 0.36132970452308655, 'running_acc': 0.8720312118530273, 'loss': 0.36235013604164124, 'acc': 0.8748749494552612, 'val_loss': 0.5422441363334656, 'val_acc': 0.8172999620437622, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.515658438205719, 'test_acc': 0.8335999846458435}]\n",
            "13 0.03375 0.5 [{'running_loss': 0.30444586277008057, 'running_acc': 0.8940624594688416, 'loss': 0.3099520802497864, 'acc': 0.8919999599456787, 'val_loss': 0.4722887873649597, 'val_acc': 0.8442999720573425, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.43822720646858215, 'test_acc': 0.8554999828338623}]\n",
            "14 0.03375 0.5 [{'running_loss': 0.30535751581192017, 'running_acc': 0.8932812213897705, 'loss': 0.2950010597705841, 'acc': 0.8974999785423279, 'val_loss': 0.4703874886035919, 'val_acc': 0.839199960231781, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.475532591342926, 'test_acc': 0.8412999510765076}]\n",
            "15 0.03375 0.5 [{'running_loss': 0.2912279963493347, 'running_acc': 0.8981249928474426, 'loss': 0.2864686846733093, 'acc': 0.9017249941825867, 'val_loss': 0.4629707932472229, 'val_acc': 0.8499000072479248, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.43354329466819763, 'test_acc': 0.8569999933242798}]\n",
            "16 0.016875 0.5 [{'running_loss': 0.2780500054359436, 'running_acc': 0.90234375, 'loss': 0.2773711383342743, 'acc': 0.9035999774932861, 'val_loss': 0.46482911705970764, 'val_acc': 0.8442999720573425, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4390997588634491, 'test_acc': 0.8551999926567078}]\n",
            "17 0.016875 0.5 [{'running_loss': 0.23534949123859406, 'running_acc': 0.9192187190055847, 'loss': 0.2408268302679062, 'acc': 0.9180749654769897, 'val_loss': 0.39108628034591675, 'val_acc': 0.8671999573707581, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3880329728126526, 'test_acc': 0.8736000061035156}]\n",
            "18 0.016875 0.5 [{'running_loss': 0.2298838049173355, 'running_acc': 0.9221875071525574, 'loss': 0.2305144965648651, 'acc': 0.920574963092804, 'val_loss': 0.4166821539402008, 'val_acc': 0.8611999750137329, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38762784004211426, 'test_acc': 0.871399998664856}]\n",
            "19 0.016875 0.5 [{'running_loss': 0.22490988671779633, 'running_acc': 0.9189062118530273, 'loss': 0.22335100173950195, 'acc': 0.9243499636650085, 'val_loss': 0.41298291087150574, 'val_acc': 0.8633999824523926, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40002521872520447, 'test_acc': 0.8718000054359436}]\n",
            "20 0.0084375 0.5 [{'running_loss': 0.24076800048351288, 'running_acc': 0.9154687523841858, 'loss': 0.22170871496200562, 'acc': 0.9231749773025513, 'val_loss': 0.4185639023780823, 'val_acc': 0.8628000020980835, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40757471323013306, 'test_acc': 0.8702999949455261}]\n",
            "21 0.0084375 0.5 [{'running_loss': 0.20029619336128235, 'running_acc': 0.9300000071525574, 'loss': 0.19469182193279266, 'acc': 0.934249997138977, 'val_loss': 0.39157599210739136, 'val_acc': 0.8700000047683716, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36895906925201416, 'test_acc': 0.8804000020027161}]\n",
            "22 0.0084375 0.5 [{'running_loss': 0.18651051819324493, 'running_acc': 0.9385937452316284, 'loss': 0.18790669739246368, 'acc': 0.9359749555587769, 'val_loss': 0.3942612409591675, 'val_acc': 0.8695999979972839, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36920398473739624, 'test_acc': 0.8797000050544739}]\n",
            "23 0.0084375 0.5 [{'running_loss': 0.18782638013362885, 'running_acc': 0.9367187023162842, 'loss': 0.18315334618091583, 'acc': 0.9367499947547913, 'val_loss': 0.3888739347457886, 'val_acc': 0.8727999925613403, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36728277802467346, 'test_acc': 0.8830999732017517}]\n",
            "24 0.00421875 0.5 [{'running_loss': 0.1784772127866745, 'running_acc': 0.9376562237739563, 'loss': 0.18013527989387512, 'acc': 0.9385749697685242, 'val_loss': 0.3852737247943878, 'val_acc': 0.8754000067710876, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3800438940525055, 'test_acc': 0.8807999491691589}]\n",
            "25 0.00421875 0.5 [{'running_loss': 0.15970508754253387, 'running_acc': 0.9470311999320984, 'loss': 0.16958345472812653, 'acc': 0.9432249665260315, 'val_loss': 0.37447530031204224, 'val_acc': 0.8786999583244324, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.354348748922348, 'test_acc': 0.8862999677658081}]\n",
            "26 0.00421875 0.5 [{'running_loss': 0.17014332115650177, 'running_acc': 0.9407812356948853, 'loss': 0.1624867171049118, 'acc': 0.9455999732017517, 'val_loss': 0.38446298241615295, 'val_acc': 0.8793999552726746, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3634125292301178, 'test_acc': 0.8865000009536743}]\n",
            "27 0.00421875 0.5 [{'running_loss': 0.17139758169651031, 'running_acc': 0.94203120470047, 'loss': 0.16045953333377838, 'acc': 0.9453999996185303, 'val_loss': 0.3861665427684784, 'val_acc': 0.8780999779701233, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35798197984695435, 'test_acc': 0.8873999714851379}]\n",
            "28 0.002109375 0.5 [{'running_loss': 0.17559197545051575, 'running_acc': 0.9445312023162842, 'loss': 0.15692484378814697, 'acc': 0.9467249512672424, 'val_loss': 0.39236703515052795, 'val_acc': 0.8782999515533447, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36476778984069824, 'test_acc': 0.887499988079071}]\n",
            "29 0.002109375 0.5 [{'running_loss': 0.14623139798641205, 'running_acc': 0.9506250023841858, 'loss': 0.15187658369541168, 'acc': 0.9489749670028687, 'val_loss': 0.3781094253063202, 'val_acc': 0.8822000026702881, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35906484723091125, 'test_acc': 0.8869999647140503}]\n",
            "30 0.002109375 0.5 [{'running_loss': 0.1509656459093094, 'running_acc': 0.9489062428474426, 'loss': 0.1507551074028015, 'acc': 0.9487749934196472, 'val_loss': 0.3864610493183136, 'val_acc': 0.8781999945640564, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3579241931438446, 'test_acc': 0.8866999745368958}]\n",
            "31 0.002109375 0.5 [{'running_loss': 0.14530843496322632, 'running_acc': 0.9504687190055847, 'loss': 0.15203286707401276, 'acc': 0.9486249685287476, 'val_loss': 0.38309407234191895, 'val_acc': 0.8795999884605408, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3594174385070801, 'test_acc': 0.8870999813079834}]\n",
            "32 0.0010546875 0.5 [{'running_loss': 0.14178629219532013, 'running_acc': 0.9523437023162842, 'loss': 0.1485380381345749, 'acc': 0.9498999714851379, 'val_loss': 0.38216692209243774, 'val_acc': 0.8776999711990356, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3590007722377777, 'test_acc': 0.8869999647140503}]\n",
            "33 0.0010546875 0.5 [{'running_loss': 0.14107513427734375, 'running_acc': 0.9549999833106995, 'loss': 0.1410498321056366, 'acc': 0.9526000022888184, 'val_loss': 0.3815729320049286, 'val_acc': 0.8770999908447266, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35830363631248474, 'test_acc': 0.887499988079071}]\n",
            "34 0.0010546875 0.5 [{'running_loss': 0.1572553813457489, 'running_acc': 0.9464062452316284, 'loss': 0.1438133865594864, 'acc': 0.9517999887466431, 'val_loss': 0.3741234242916107, 'val_acc': 0.880299985408783, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3626454472541809, 'test_acc': 0.8863999843597412}]\n",
            "35 0.0010546875 0.5 [{'running_loss': 0.13624843955039978, 'running_acc': 0.95765620470047, 'loss': 0.1419002115726471, 'acc': 0.9520750045776367, 'val_loss': 0.3756435215473175, 'val_acc': 0.8815000057220459, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36228877305984497, 'test_acc': 0.8866999745368958}]\n",
            "36 0.00052734375 0.5 [{'running_loss': 0.14403393864631653, 'running_acc': 0.9509374499320984, 'loss': 0.14049966633319855, 'acc': 0.951949954032898, 'val_loss': 0.3771842122077942, 'val_acc': 0.8800999522209167, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3607761561870575, 'test_acc': 0.8859999775886536}]\n",
            "37 0.00052734375 0.5 [{'running_loss': 0.13746820390224457, 'running_acc': 0.9554687142372131, 'loss': 0.14238443970680237, 'acc': 0.9519000053405762, 'val_loss': 0.38437727093696594, 'val_acc': 0.8780999779701233, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3601820468902588, 'test_acc': 0.8870999813079834}]\n",
            "38 0.00052734375 0.5 [{'running_loss': 0.13892698287963867, 'running_acc': 0.9556249976158142, 'loss': 0.13868339359760284, 'acc': 0.9538499712944031, 'val_loss': 0.3880431354045868, 'val_acc': 0.8766999840736389, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36307844519615173, 'test_acc': 0.8858999609947205}]\n",
            "39 0.00052734375 0.5 [{'running_loss': 0.136550635099411, 'running_acc': 0.9548436999320984, 'loss': 0.13841594755649567, 'acc': 0.9541249871253967, 'val_loss': 0.36401069164276123, 'val_acc': 0.8792999982833862, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3590472340583801, 'test_acc': 0.8876000046730042}]\n",
            "40 0.000263671875 0.5 [{'running_loss': 0.13797450065612793, 'running_acc': 0.9542187452316284, 'loss': 0.13819724321365356, 'acc': 0.9538999795913696, 'val_loss': 0.3861963152885437, 'val_acc': 0.8752999901771545, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3627299964427948, 'test_acc': 0.8870999813079834}]\n",
            "41 0.000263671875 0.5 [{'running_loss': 0.13293373584747314, 'running_acc': 0.9557812213897705, 'loss': 0.1354474127292633, 'acc': 0.9543499946594238, 'val_loss': 0.37409552931785583, 'val_acc': 0.8797000050544739, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3613583743572235, 'test_acc': 0.8860999941825867}]\n",
            "42 0.000263671875 0.5 [{'running_loss': 0.12894724309444427, 'running_acc': 0.9584375023841858, 'loss': 0.13516943156719208, 'acc': 0.9558500051498413, 'val_loss': 0.3681642413139343, 'val_acc': 0.8822999596595764, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36103081703186035, 'test_acc': 0.887499988079071}]\n",
            "43 0.000263671875 0.5 [{'running_loss': 0.12928803265094757, 'running_acc': 0.9596874713897705, 'loss': 0.13564825057983398, 'acc': 0.9545999765396118, 'val_loss': 0.38237065076828003, 'val_acc': 0.8789999485015869, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36028486490249634, 'test_acc': 0.8854999542236328}]\n",
            "44 0.0001318359375 0.5 [{'running_loss': 0.13770927488803864, 'running_acc': 0.9546874761581421, 'loss': 0.13726629316806793, 'acc': 0.9540249705314636, 'val_loss': 0.37677139043807983, 'val_acc': 0.8795999884605408, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3605166971683502, 'test_acc': 0.8873999714851379}]\n",
            "45 0.0001318359375 0.5 [{'running_loss': 0.1424964815378189, 'running_acc': 0.9517187476158142, 'loss': 0.1361996978521347, 'acc': 0.9548499584197998, 'val_loss': 0.37607529759407043, 'val_acc': 0.87909996509552, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3614148795604706, 'test_acc': 0.887499988079071}]\n",
            "46 0.0001318359375 0.5 [{'running_loss': 0.13134555518627167, 'running_acc': 0.95703125, 'loss': 0.13600510358810425, 'acc': 0.9548249840736389, 'val_loss': 0.3769721984863281, 'val_acc': 0.8811999559402466, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3628964126110077, 'test_acc': 0.8876000046730042}]\n",
            "47 0.0001318359375 0.5 [{'running_loss': 0.13070054352283478, 'running_acc': 0.9565624594688416, 'loss': 0.13504935801029205, 'acc': 0.9544999599456787, 'val_loss': 0.3756524324417114, 'val_acc': 0.8806999921798706, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36319267749786377, 'test_acc': 0.8877999782562256}]\n",
            "48 6.591796875e-05 0.5 [{'running_loss': 0.13240109384059906, 'running_acc': 0.9557812213897705, 'loss': 0.13443374633789062, 'acc': 0.9542749524116516, 'val_loss': 0.37975582480430603, 'val_acc': 0.8768999576568604, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36171671748161316, 'test_acc': 0.8867999911308289}]\n",
            "49 6.591796875e-05 0.5 [{'running_loss': 0.13866953551769257, 'running_acc': 0.9532812237739563, 'loss': 0.1346011608839035, 'acc': 0.9552499651908875, 'val_loss': 0.3821457624435425, 'val_acc': 0.8804000020027161, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3614019751548767, 'test_acc': 0.8858000040054321}]\n",
            "50 6.591796875e-05 0.5 [{'running_loss': 0.14201000332832336, 'running_acc': 0.9542187452316284, 'loss': 0.13456282019615173, 'acc': 0.9553499817848206, 'val_loss': 0.38417428731918335, 'val_acc': 0.8805999755859375, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36094793677330017, 'test_acc': 0.8867999911308289}]\n",
            "51 6.591796875e-05 0.5 [{'running_loss': 0.1310591846704483, 'running_acc': 0.9560937285423279, 'loss': 0.13440360128879547, 'acc': 0.9558749794960022, 'val_loss': 0.3823276460170746, 'val_acc': 0.8794999718666077, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3629305958747864, 'test_acc': 0.8872999548912048}]\n",
            "52 6.25e-05 0.5 [{'running_loss': 0.13757099211215973, 'running_acc': 0.9528124928474426, 'loss': 0.13758663833141327, 'acc': 0.9529249668121338, 'val_loss': 0.38340845704078674, 'val_acc': 0.8786999583244324, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35974743962287903, 'test_acc': 0.8871999979019165}]\n",
            "53 6.25e-05 0.5 [{'running_loss': 0.12683439254760742, 'running_acc': 0.9598437547683716, 'loss': 0.13539443910121918, 'acc': 0.9548499584197998, 'val_loss': 0.3846679627895355, 'val_acc': 0.8799999952316284, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3645082414150238, 'test_acc': 0.8871999979019165}]\n",
            "54 6.25e-05 0.5 [{'running_loss': 0.1386406123638153, 'running_acc': 0.9560937285423279, 'loss': 0.13549037277698517, 'acc': 0.9549499750137329, 'val_loss': 0.38714665174484253, 'val_acc': 0.8755999803543091, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3625026345252991, 'test_acc': 0.8863999843597412}]\n",
            "55 6.25e-05 0.5 [{'running_loss': 0.1408911496400833, 'running_acc': 0.949999988079071, 'loss': 0.13677114248275757, 'acc': 0.9538999795913696, 'val_loss': 0.38165751099586487, 'val_acc': 0.8768999576568604, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35952287912368774, 'test_acc': 0.8876000046730042}]\n",
            "56 6.25e-05 0.5 [{'running_loss': 0.13144895434379578, 'running_acc': 0.9574999809265137, 'loss': 0.13375034928321838, 'acc': 0.9555249810218811, 'val_loss': 0.3788299560546875, 'val_acc': 0.877299964427948, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3614635467529297, 'test_acc': 0.8865000009536743}]\n",
            "57 6.25e-05 0.5 [{'running_loss': 0.1277889758348465, 'running_acc': 0.9587499499320984, 'loss': 0.1333584189414978, 'acc': 0.9562749862670898, 'val_loss': 0.3846753239631653, 'val_acc': 0.8789999485015869, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.362575501203537, 'test_acc': 0.8876999616622925}]\n",
            "58 6.25e-05 0.5 [{'running_loss': 0.13877171277999878, 'running_acc': 0.9548436999320984, 'loss': 0.1362205296754837, 'acc': 0.9546749591827393, 'val_loss': 0.3773218095302582, 'val_acc': 0.8822999596595764, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36314427852630615, 'test_acc': 0.8872999548912048}]\n",
            "59 6.25e-05 0.5 [{'running_loss': 0.134796604514122, 'running_acc': 0.9559375047683716, 'loss': 0.13412036001682281, 'acc': 0.9554749727249146, 'val_loss': 0.391845166683197, 'val_acc': 0.8763999938964844, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36305975914001465, 'test_acc': 0.8868999481201172}]\n",
            "60 6.25e-05 0.5 [{'running_loss': 0.14614811539649963, 'running_acc': 0.949999988079071, 'loss': 0.13512590527534485, 'acc': 0.9542999863624573, 'val_loss': 0.3783007264137268, 'val_acc': 0.8813999891281128, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3613108694553375, 'test_acc': 0.8863999843597412}]\n",
            "61 6.25e-05 0.5 [{'running_loss': 0.1382656693458557, 'running_acc': 0.9524999856948853, 'loss': 0.1342611014842987, 'acc': 0.9544000029563904, 'val_loss': 0.3816854953765869, 'val_acc': 0.8804000020027161, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36287763714790344, 'test_acc': 0.8858999609947205}]\n",
            "62 6.25e-05 0.5 [{'running_loss': 0.13538193702697754, 'running_acc': 0.953906238079071, 'loss': 0.13574466109275818, 'acc': 0.9535499811172485, 'val_loss': 0.3756154179573059, 'val_acc': 0.8824999928474426, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36350783705711365, 'test_acc': 0.886199951171875}]\n",
            "63 6.25e-05 0.5 [{'running_loss': 0.1348734200000763, 'running_acc': 0.953125, 'loss': 0.1330580860376358, 'acc': 0.9556249976158142, 'val_loss': 0.39083853363990784, 'val_acc': 0.8794999718666077, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36346447467803955, 'test_acc': 0.8865000009536743}]\n",
            "64 6.25e-05 0.5 [{'running_loss': 0.13680189847946167, 'running_acc': 0.9520312547683716, 'loss': 0.135396808385849, 'acc': 0.9533249735832214, 'val_loss': 0.38485512137413025, 'val_acc': 0.8791999816894531, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36274537444114685, 'test_acc': 0.885699987411499}]\n",
            "65 6.25e-05 0.5 [{'running_loss': 0.138142392039299, 'running_acc': 0.9551562070846558, 'loss': 0.13797162473201752, 'acc': 0.9539249539375305, 'val_loss': 0.3816460967063904, 'val_acc': 0.8764999508857727, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36441758275032043, 'test_acc': 0.8867999911308289}]\n",
            "66 6.25e-05 0.5 [{'running_loss': 0.1282874345779419, 'running_acc': 0.9564062356948853, 'loss': 0.1323135644197464, 'acc': 0.955674946308136, 'val_loss': 0.3840029239654541, 'val_acc': 0.8777999877929688, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3618253767490387, 'test_acc': 0.8865000009536743}]\n",
            "67 6.25e-05 0.5 [{'running_loss': 0.14072157442569733, 'running_acc': 0.9515624642372131, 'loss': 0.13499192893505096, 'acc': 0.9541749954223633, 'val_loss': 0.390683650970459, 'val_acc': 0.8779999613761902, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3634122908115387, 'test_acc': 0.8854999542236328}]\n",
            "68 6.25e-05 0.5 [{'running_loss': 0.14169803261756897, 'running_acc': 0.9528124928474426, 'loss': 0.1346363127231598, 'acc': 0.9550249576568604, 'val_loss': 0.38965314626693726, 'val_acc': 0.8788999915122986, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36329010128974915, 'test_acc': 0.8859999775886536}]\n",
            "69 6.25e-05 0.5 [{'running_loss': 0.133157417178154, 'running_acc': 0.95703125, 'loss': 0.1318344920873642, 'acc': 0.9573249816894531, 'val_loss': 0.38291826844215393, 'val_acc': 0.8781999945640564, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3612686097621918, 'test_acc': 0.8871999979019165}]\n",
            "70 6.25e-05 0.5 [{'running_loss': 0.13808369636535645, 'running_acc': 0.9546874761581421, 'loss': 0.13396279513835907, 'acc': 0.9549999833106995, 'val_loss': 0.3814397156238556, 'val_acc': 0.8759999871253967, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3636433780193329, 'test_acc': 0.8858999609947205}]\n",
            "71 6.25e-05 0.5 [{'running_loss': 0.1320847123861313, 'running_acc': 0.9553124904632568, 'loss': 0.1330510824918747, 'acc': 0.9553249478340149, 'val_loss': 0.38182535767555237, 'val_acc': 0.8797999620437622, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3625587224960327, 'test_acc': 0.8859999775886536}]\n",
            "72 6.25e-05 0.5 [{'running_loss': 0.13695128262043, 'running_acc': 0.9559375047683716, 'loss': 0.1317313313484192, 'acc': 0.9559749960899353, 'val_loss': 0.3872259557247162, 'val_acc': 0.8768999576568604, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36168190836906433, 'test_acc': 0.8866999745368958}]\n",
            "73 6.25e-05 0.5 [{'running_loss': 0.13010653853416443, 'running_acc': 0.9564062356948853, 'loss': 0.1351056545972824, 'acc': 0.9556499719619751, 'val_loss': 0.3750145733356476, 'val_acc': 0.8780999779701233, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3621210753917694, 'test_acc': 0.8876999616622925}]\n",
            "74 6.25e-05 0.5 [{'running_loss': 0.12831434607505798, 'running_acc': 0.95765620470047, 'loss': 0.13292233645915985, 'acc': 0.9556999802589417, 'val_loss': 0.38061803579330444, 'val_acc': 0.87909996509552, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3619140684604645, 'test_acc': 0.8865000009536743}]\n",
            "75 6.25e-05 0.5 [{'running_loss': 0.1258183866739273, 'running_acc': 0.9573437571525574, 'loss': 0.13290081918239594, 'acc': 0.9552499651908875, 'val_loss': 0.3900304436683655, 'val_acc': 0.8761000037193298, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3604656755924225, 'test_acc': 0.8883999586105347}]\n",
            "76 6.25e-05 0.5 [{'running_loss': 0.1343013197183609, 'running_acc': 0.9567187428474426, 'loss': 0.13433027267456055, 'acc': 0.9553999900817871, 'val_loss': 0.38280144333839417, 'val_acc': 0.8807999491691589, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36357977986335754, 'test_acc': 0.8867999911308289}]\n",
            "77 6.25e-05 0.5 [{'running_loss': 0.13239535689353943, 'running_acc': 0.9560937285423279, 'loss': 0.13568681478500366, 'acc': 0.9544249773025513, 'val_loss': 0.3805823028087616, 'val_acc': 0.8797999620437622, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36239659786224365, 'test_acc': 0.8853999972343445}]\n",
            "78 6.25e-05 0.5 [{'running_loss': 0.14302204549312592, 'running_acc': 0.9517187476158142, 'loss': 0.13585524260997772, 'acc': 0.9546249508857727, 'val_loss': 0.3787137567996979, 'val_acc': 0.8750999569892883, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3638205826282501, 'test_acc': 0.8867999911308289}]\n",
            "79 6.25e-05 0.5 [{'running_loss': 0.1369141787290573, 'running_acc': 0.957812488079071, 'loss': 0.13375528156757355, 'acc': 0.9558249711990356, 'val_loss': 0.38077569007873535, 'val_acc': 0.8800999522209167, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3639577031135559, 'test_acc': 0.8865000009536743}]\n",
            "80 6.25e-05 0.5 [{'running_loss': 0.14228381216526031, 'running_acc': 0.9509374499320984, 'loss': 0.13262839615345, 'acc': 0.9561249613761902, 'val_loss': 0.38646915555000305, 'val_acc': 0.8801999688148499, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3627680540084839, 'test_acc': 0.8852999806404114}]\n",
            "81 6.25e-05 0.5 [{'running_loss': 0.13839852809906006, 'running_acc': 0.9543749690055847, 'loss': 0.13231191039085388, 'acc': 0.9559749960899353, 'val_loss': 0.39144808053970337, 'val_acc': 0.8759999871253967, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3627978563308716, 'test_acc': 0.887499988079071}]\n",
            "82 6.25e-05 0.5 [{'running_loss': 0.13728158175945282, 'running_acc': 0.9540624618530273, 'loss': 0.13528765738010406, 'acc': 0.9545750021934509, 'val_loss': 0.38476628065109253, 'val_acc': 0.87909996509552, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3620373010635376, 'test_acc': 0.8862999677658081}]\n",
            "83 6.25e-05 0.5 [{'running_loss': 0.12879113852977753, 'running_acc': 0.957812488079071, 'loss': 0.13245835900306702, 'acc': 0.9556249976158142, 'val_loss': 0.3751088082790375, 'val_acc': 0.8829999566078186, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3600902259349823, 'test_acc': 0.887499988079071}]\n",
            "84 6.25e-05 0.5 [{'running_loss': 0.1297769844532013, 'running_acc': 0.9568749666213989, 'loss': 0.13268320262432098, 'acc': 0.9550749659538269, 'val_loss': 0.3829139769077301, 'val_acc': 0.8818999528884888, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3632678985595703, 'test_acc': 0.8865999579429626}]\n",
            "85 6.25e-05 0.5 [{'running_loss': 0.14264589548110962, 'running_acc': 0.9532812237739563, 'loss': 0.13207761943340302, 'acc': 0.956974983215332, 'val_loss': 0.37382471561431885, 'val_acc': 0.8800999522209167, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36392152309417725, 'test_acc': 0.8851999640464783}]\n",
            "86 6.25e-05 0.5 [{'running_loss': 0.13544480502605438, 'running_acc': 0.9545312523841858, 'loss': 0.1336791068315506, 'acc': 0.9557249546051025, 'val_loss': 0.3878242075443268, 'val_acc': 0.8831999897956848, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36555925011634827, 'test_acc': 0.8859999775886536}]\n",
            "87 6.25e-05 0.5 [{'running_loss': 0.13597439229488373, 'running_acc': 0.9515624642372131, 'loss': 0.13356192409992218, 'acc': 0.9543749690055847, 'val_loss': 0.39169833064079285, 'val_acc': 0.8757999539375305, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36256542801856995, 'test_acc': 0.885699987411499}]\n",
            "88 6.25e-05 0.5 [{'running_loss': 0.1286223828792572, 'running_acc': 0.953906238079071, 'loss': 0.1333501636981964, 'acc': 0.9544249773025513, 'val_loss': 0.37579917907714844, 'val_acc': 0.8792999982833862, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3614784777164459, 'test_acc': 0.8871999979019165}]\n",
            "89 6.25e-05 0.5 [{'running_loss': 0.1312171369791031, 'running_acc': 0.9562499523162842, 'loss': 0.12980878353118896, 'acc': 0.9570249915122986, 'val_loss': 0.3982374668121338, 'val_acc': 0.8792999982833862, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3647208511829376, 'test_acc': 0.8871999979019165}]\n",
            "90 6.25e-05 0.5 [{'running_loss': 0.14178046584129333, 'running_acc': 0.9507812261581421, 'loss': 0.1312863826751709, 'acc': 0.956725001335144, 'val_loss': 0.38401463627815247, 'val_acc': 0.8799999952316284, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36458921432495117, 'test_acc': 0.8858999609947205}]\n",
            "91 6.25e-05 0.5 [{'running_loss': 0.13198450207710266, 'running_acc': 0.9556249976158142, 'loss': 0.13295520842075348, 'acc': 0.9558749794960022, 'val_loss': 0.38699477910995483, 'val_acc': 0.880899965763092, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3627651333808899, 'test_acc': 0.8853999972343445}]\n",
            "92 6.25e-05 0.5 [{'running_loss': 0.1451408863067627, 'running_acc': 0.9507812261581421, 'loss': 0.13458134233951569, 'acc': 0.9545499682426453, 'val_loss': 0.38794460892677307, 'val_acc': 0.8788999915122986, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3639633059501648, 'test_acc': 0.8862999677658081}]\n",
            "93 6.25e-05 0.5 [{'running_loss': 0.12610937654972076, 'running_acc': 0.957812488079071, 'loss': 0.13392435014247894, 'acc': 0.9551249742507935, 'val_loss': 0.3799270987510681, 'val_acc': 0.8784999847412109, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3630443811416626, 'test_acc': 0.8868999481201172}]\n",
            "94 6.25e-05 0.5 [{'running_loss': 0.13752490282058716, 'running_acc': 0.9545312523841858, 'loss': 0.1325751692056656, 'acc': 0.9559749960899353, 'val_loss': 0.37778663635253906, 'val_acc': 0.8815999627113342, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3629610240459442, 'test_acc': 0.8872999548912048}]\n",
            "95 6.25e-05 0.5 [{'running_loss': 0.14732183516025543, 'running_acc': 0.9521874785423279, 'loss': 0.13245733082294464, 'acc': 0.956725001335144, 'val_loss': 0.3745569586753845, 'val_acc': 0.8815000057220459, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36554598808288574, 'test_acc': 0.8865999579429626}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FE5OVaHbOVzt"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MwrOzLhvRfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for Heavy Ball on Batch Size 128 and Decayed Hyperparameter Schedule\n",
        "\n",
        "# final_lr_HeavyBall = 0.001\n",
        "# train_loss_HeavyBall = np.array([1.63639855, 1.16524589, 0.93842459, 0.80115128, 0.71703941,\n",
        "#        0.65074068, 0.60588378, 0.56748879, 0.54011595, 0.52048105,\n",
        "#        0.49842277, 0.4781481 , 0.47079977, 0.45433897, 0.44086391,\n",
        "#        0.43515238, 0.42260206, 0.42274201, 0.40584901, 0.40511623,\n",
        "#        0.39582601, 0.39122081, 0.38812533, 0.37928233, 0.37536055,\n",
        "#        0.28057501, 0.25989857, 0.25889018, 0.2537483 , 0.2507388 ,\n",
        "#        0.24967523, 0.25530261, 0.2522625 , 0.17712839, 0.16458513,\n",
        "#        0.16427112, 0.15593426, 0.15731791, 0.15589088, 0.15641476,\n",
        "#        0.15255828, 0.10770587, 0.09357908, 0.08927524, 0.08619794,\n",
        "#        0.08248061, 0.08202739, 0.08139143, 0.0802817 , 0.07820573,\n",
        "#        0.08004044, 0.07801978, 0.0795709 , 0.07677002, 0.07564631,\n",
        "#        0.07951888, 0.08007317, 0.05003384, 0.04218028, 0.03755803,\n",
        "#        0.03431063, 0.03510899, 0.03455525, 0.03459555, 0.03180457,\n",
        "#        0.02613511, 0.02152506, 0.02148007, 0.01898943, 0.01803452,\n",
        "#        0.01691009, 0.01813209, 0.01760868, 0.01644369, 0.01767487,\n",
        "#        0.01569961, 0.01736432, 0.01515401, 0.01209177, 0.0123928 ,\n",
        "#        0.01241801, 0.01147574, 0.01128419, 0.012046  , 0.01230034,\n",
        "#        0.01005275, 0.00977574, 0.00988733, 0.00985803, 0.01017826,\n",
        "#        0.00998743, 0.00927842, 0.00939303, 0.00909515, 0.00918936,\n",
        "#        0.00895024, 0.00855465, 0.00840965, 0.00914173, 0.00894787,\n",
        "#        0.00892666, 0.00919945, 0.00835487, 0.00851858, 0.00866958,\n",
        "#        0.00821259, 0.00766291, 0.00825235, 0.00884607, 0.00868431,\n",
        "#        0.00896737, 0.00820503, 0.00912   , 0.00859701, 0.00849906,\n",
        "#        0.00773092, 0.00800199, 0.00839519, 0.00760326, 0.00846684])\n",
        "# train_accuracy_HeavyBall = np.array([0.39045   , 0.57997501, 0.66877496, 0.72149998, 0.74822497,\n",
        "#        0.77329999, 0.79004997, 0.80394995, 0.81355   , 0.82009995,\n",
        "#        0.82782495, 0.83524996, 0.83739996, 0.84265   , 0.84884995,\n",
        "#        0.847525  , 0.85502499, 0.85420001, 0.86104995, 0.85914999,\n",
        "#        0.86364996, 0.86495   , 0.86532497, 0.867975  , 0.87062496,\n",
        "#        0.9052    , 0.91029996, 0.91104996, 0.91249996, 0.914325  ,\n",
        "#        0.91525   , 0.91277498, 0.91177499, 0.93895   , 0.94312495,\n",
        "#        0.944125  , 0.94647497, 0.94509995, 0.94637495, 0.94529998,\n",
        "#        0.94734997, 0.963175  , 0.96899998, 0.96982497, 0.97147501,\n",
        "#        0.97364998, 0.97192496, 0.97292495, 0.97359997, 0.97382498,\n",
        "#        0.97382498, 0.97372496, 0.97277498, 0.97347498, 0.97507495,\n",
        "#        0.97332495, 0.97189999, 0.98499995, 0.98772496, 0.98857498,\n",
        "#        0.98984998, 0.98949999, 0.98995   , 0.98954999, 0.99072498,\n",
        "#        0.99239999, 0.994425  , 0.99404997, 0.99537498, 0.99497497,\n",
        "#        0.99582499, 0.99519998, 0.99567497, 0.99605   , 0.9957    ,\n",
        "#        0.99632496, 0.99549997, 0.99637496, 0.99759996, 0.99729997,\n",
        "#        0.99734998, 0.99754995, 0.99789995, 0.99724996, 0.99739999,\n",
        "#        0.99809998, 0.99822497, 0.99812496, 0.99812496, 0.997675  ,\n",
        "#        0.99794996, 0.99822497, 0.99812496, 0.99847496, 0.99817497,\n",
        "#        0.99839997, 0.99842495, 0.99855   , 0.9982    , 0.99859995,\n",
        "#        0.99832499, 0.99822497, 0.99862498, 0.99869996, 0.99842495,\n",
        "#        0.99864995, 0.99884999, 0.99844998, 0.99814999, 0.99859995,\n",
        "#        0.99839997, 0.99862498, 0.99827498, 0.99857497, 0.99839997,\n",
        "#        0.9989    , 0.99874997, 0.99847496, 0.99892497, 0.99834996])\n",
        "# valid_loss_HeavyBall = np.array([1.62342262, 1.23547566, 1.19479787, 1.53580368, 1.08412611,\n",
        "#        0.90649474, 0.75236231, 0.8768388 , 1.15808451, 0.75439602,\n",
        "#        0.90548778, 0.8853935 , 0.63843179, 0.63551033, 0.80657756,\n",
        "#        0.86939639, 0.66458052, 0.71015853, 0.98159319, 0.67434543,\n",
        "#        0.62285471, 0.69211686, 0.73795903, 0.77567118, 0.75192147,\n",
        "#        0.46415925, 0.45307934, 0.48942471, 0.57367247, 0.5518117 ,\n",
        "#        0.45561317, 0.64142799, 0.58449107, 0.36775607, 0.44709939,\n",
        "#        0.37292141, 0.47742018, 0.48038238, 0.41485056, 0.45572093,\n",
        "#        0.39309034, 0.34107766, 0.36909676, 0.36279622, 0.35023731,\n",
        "#        0.39409408, 0.39514256, 0.38495493, 0.40753016, 0.37448061,\n",
        "#        0.40596312, 0.3547467 , 0.43357411, 0.38619399, 0.38813961,\n",
        "#        0.39045057, 0.41463694, 0.3487103 , 0.35101467, 0.33390051,\n",
        "#        0.34542489, 0.36648101, 0.36530775, 0.35734597, 0.36367124,\n",
        "#        0.35189992, 0.34472728, 0.34275562, 0.35179687, 0.35628679,\n",
        "#        0.35194764, 0.33613381, 0.3522813 , 0.36797574, 0.36452708,\n",
        "#        0.37453303, 0.36632472, 0.35407275, 0.35510811, 0.37291777,\n",
        "#        0.35329685, 0.37258849, 0.3596327 , 0.37167972, 0.37243861,\n",
        "#        0.38023359, 0.35199553, 0.35344142, 0.36755151, 0.35836834,\n",
        "#        0.35599712, 0.35985643, 0.36201614, 0.35736108, 0.35841039,\n",
        "#        0.36518463, 0.36943892, 0.36624932, 0.3628391 , 0.36281943,\n",
        "#        0.36851233, 0.3740285 , 0.36143878, 0.38417286, 0.3762829 ,\n",
        "#        0.35935313, 0.36690712, 0.35444233, 0.36925125, 0.37213659,\n",
        "#        0.36541107, 0.37186453, 0.37104961, 0.35882875, 0.37652409,\n",
        "#        0.36537334, 0.37899083, 0.36350381, 0.37925407, 0.3580831 ])\n",
        "# valid_accuracy_HeavyBall = np.array([0.41429999, 0.54809999, 0.59920001, 0.53189999, 0.64749998,\n",
        "#        0.69089997, 0.74189997, 0.70309997, 0.64699996, 0.73640001,\n",
        "#        0.7087    , 0.70829999, 0.78179997, 0.78319997, 0.72490001,\n",
        "#        0.71700001, 0.77469999, 0.76459998, 0.6857    , 0.77459997,\n",
        "#        0.79719996, 0.76980001, 0.76279998, 0.76629996, 0.76339996,\n",
        "#        0.84779996, 0.8477    , 0.84129995, 0.81949997, 0.81979996,\n",
        "#        0.84779996, 0.79049999, 0.80769998, 0.88229996, 0.85829997,\n",
        "#        0.87979996, 0.85249996, 0.85420001, 0.87109995, 0.85969996,\n",
        "#        0.87219995, 0.89219999, 0.89019996, 0.88799995, 0.89559996,\n",
        "#        0.8926    , 0.88189995, 0.88849998, 0.88249999, 0.88709998,\n",
        "#        0.88369995, 0.89459997, 0.87709999, 0.88629997, 0.88699996,\n",
        "#        0.88379997, 0.87909997, 0.90499997, 0.9034    , 0.90919995,\n",
        "#        0.9048    , 0.90169996, 0.903     , 0.9059    , 0.90399998,\n",
        "#        0.90419996, 0.90899998, 0.9095    , 0.90789998, 0.90969998,\n",
        "#        0.90979999, 0.91229999, 0.90989995, 0.90709996, 0.91049999,\n",
        "#        0.90889996, 0.9084    , 0.90819997, 0.90829998, 0.90959996,\n",
        "#        0.91219997, 0.90869999, 0.90919995, 0.90919995, 0.91399997,\n",
        "#        0.91239995, 0.91069996, 0.91349995, 0.91049999, 0.90999997,\n",
        "#        0.91229999, 0.91339999, 0.91149998, 0.91339999, 0.91099995,\n",
        "#        0.9113    , 0.91079998, 0.91279995, 0.91249996, 0.90999997,\n",
        "#        0.9109    , 0.91179997, 0.91289997, 0.90779996, 0.90979999,\n",
        "#        0.91099995, 0.91039997, 0.91339999, 0.91169995, 0.91209996,\n",
        "#        0.91159999, 0.91109997, 0.90979999, 0.91169995, 0.9091    ,\n",
        "#        0.91149998, 0.91079998, 0.91319996, 0.91060001, 0.91599995])\n",
        "# test_loss_HeavyBall = np.array([1.60591662, 1.1914351 , 1.27395296, 1.61479247, 1.15463042,\n",
        "#        0.9112978 , 0.7439096 , 0.91708374, 1.1029551 , 0.74615741,\n",
        "#        0.95354801, 0.92271948, 0.61847419, 0.65734327, 0.83164448,\n",
        "#        0.88968611, 0.67372102, 0.73523706, 0.97652495, 0.68480676,\n",
        "#        0.5937553 , 0.69509816, 0.75287068, 0.8124947 , 0.78880024,\n",
        "#        0.46034014, 0.46721992, 0.47797444, 0.604581  , 0.5462262 ,\n",
        "#        0.41720402, 0.66754413, 0.55222332, 0.35296655, 0.45391065,\n",
        "#        0.36314747, 0.48926198, 0.4686349 , 0.4405002 , 0.46224856,\n",
        "#        0.39932123, 0.33536184, 0.37126353, 0.35161412, 0.35590288,\n",
        "#        0.41079843, 0.39812738, 0.39554238, 0.41176298, 0.37277153,\n",
        "#        0.41893753, 0.35542965, 0.4392381 , 0.37396297, 0.3828617 ,\n",
        "#        0.40712425, 0.42115387, 0.33073315, 0.35888204, 0.34095594,\n",
        "#        0.35057032, 0.36620033, 0.36187476, 0.3737314 , 0.35962859,\n",
        "#        0.36073065, 0.35075137, 0.35714161, 0.35152066, 0.36065724,\n",
        "#        0.35375023, 0.36053643, 0.35544458, 0.35669088, 0.3641853 ,\n",
        "#        0.36475533, 0.37937796, 0.36674395, 0.36566937, 0.36105937,\n",
        "#        0.36656183, 0.36568514, 0.36543134, 0.36367676, 0.37226641,\n",
        "#        0.36605671, 0.36457106, 0.36656725, 0.36508381, 0.36835787,\n",
        "#        0.36713141, 0.36508238, 0.36344811, 0.36629426, 0.36584523,\n",
        "#        0.36961344, 0.36795497, 0.36867648, 0.36686686, 0.36811981,\n",
        "#        0.36344513, 0.36942673, 0.36982876, 0.36895025, 0.37065941,\n",
        "#        0.36663896, 0.37236652, 0.37170222, 0.3675074 , 0.36788741,\n",
        "#        0.3720966 , 0.36677676, 0.36981419, 0.37307525, 0.37006655,\n",
        "#        0.36813325, 0.37113562, 0.37028101, 0.37077329, 0.36784762])\n",
        "# test_accuracy_HeavyBall = np.array([0.4276    , 0.56639999, 0.58829999, 0.53960001, 0.6505    ,\n",
        "#        0.6961    , 0.74869996, 0.70039999, 0.67109996, 0.74430001,\n",
        "#        0.70120001, 0.70910001, 0.78839999, 0.78119999, 0.73499995,\n",
        "#        0.71939999, 0.77389997, 0.7683    , 0.70660001, 0.7816    ,\n",
        "#        0.80239999, 0.77399999, 0.7622    , 0.76299995, 0.76099998,\n",
        "#        0.84630001, 0.85079998, 0.8484    , 0.8161    , 0.82309997,\n",
        "#        0.86299998, 0.79339999, 0.82409996, 0.88940001, 0.86059999,\n",
        "#        0.8836    , 0.85459995, 0.85509998, 0.87469995, 0.86139995,\n",
        "#        0.87619996, 0.89779997, 0.89219999, 0.89379996, 0.89679998,\n",
        "#        0.88789999, 0.88789999, 0.88909996, 0.88519996, 0.89219999,\n",
        "#        0.8829    , 0.89839995, 0.88189995, 0.89719999, 0.89279997,\n",
        "#        0.88549995, 0.87979996, 0.90999997, 0.903     , 0.90849996,\n",
        "#        0.91079998, 0.90789998, 0.9102    , 0.90469998, 0.91119999,\n",
        "#        0.91109997, 0.91319996, 0.91159999, 0.9127    , 0.9127    ,\n",
        "#        0.91339999, 0.91060001, 0.91399997, 0.91459996, 0.91249996,\n",
        "#        0.91349995, 0.90959996, 0.91259998, 0.91429996, 0.9152    ,\n",
        "#        0.91419995, 0.91419995, 0.91289997, 0.9152    , 0.91419995,\n",
        "#        0.9149    , 0.91499996, 0.91599995, 0.9149    , 0.91499996,\n",
        "#        0.91569996, 0.91659999, 0.91499996, 0.91569996, 0.9152    ,\n",
        "#        0.9145    , 0.91419995, 0.91509998, 0.91439998, 0.91459996,\n",
        "#        0.9152    , 0.91479999, 0.91479999, 0.91469997, 0.91479999,\n",
        "#        0.91589999, 0.91419995, 0.91539997, 0.91469997, 0.91549999,\n",
        "#        0.9149    , 0.91389996, 0.9149    , 0.91569996, 0.91529995,\n",
        "#        0.91539997, 0.91579998, 0.91479999, 0.91609997, 0.91529995])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7LOlzDiYJzgG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2e0c543-4d7d-4b01-d341-71549c08e2e4"
      },
      "source": [
        "print(\"Final Learning Rate reached at the end of training: \", final_lr_HeavyBall)\n",
        "print(\"Training Loss Set: \",repr(train_loss_HeavyBall))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_HeavyBall))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_HeavyBall))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_HeavyBall))\n",
        "print(\"Test Loss Set: \", repr(test_loss_HeavyBall))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_HeavyBall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Learning Rate reached at the end of training:  0.001\n",
            "Training Loss Set:  array([1.63639855, 1.16524589, 0.93842459, 0.80115128, 0.71703941,\n",
            "       0.65074068, 0.60588378, 0.56748879, 0.54011595, 0.52048105,\n",
            "       0.49842277, 0.4781481 , 0.47079977, 0.45433897, 0.44086391,\n",
            "       0.43515238, 0.42260206, 0.42274201, 0.40584901, 0.40511623,\n",
            "       0.39582601, 0.39122081, 0.38812533, 0.37928233, 0.37536055,\n",
            "       0.28057501, 0.25989857, 0.25889018, 0.2537483 , 0.2507388 ,\n",
            "       0.24967523, 0.25530261, 0.2522625 , 0.17712839, 0.16458513,\n",
            "       0.16427112, 0.15593426, 0.15731791, 0.15589088, 0.15641476,\n",
            "       0.15255828, 0.10770587, 0.09357908, 0.08927524, 0.08619794,\n",
            "       0.08248061, 0.08202739, 0.08139143, 0.0802817 , 0.07820573,\n",
            "       0.08004044, 0.07801978, 0.0795709 , 0.07677002, 0.07564631,\n",
            "       0.07951888, 0.08007317, 0.05003384, 0.04218028, 0.03755803,\n",
            "       0.03431063, 0.03510899, 0.03455525, 0.03459555, 0.03180457,\n",
            "       0.02613511, 0.02152506, 0.02148007, 0.01898943, 0.01803452,\n",
            "       0.01691009, 0.01813209, 0.01760868, 0.01644369, 0.01767487,\n",
            "       0.01569961, 0.01736432, 0.01515401, 0.01209177, 0.0123928 ,\n",
            "       0.01241801, 0.01147574, 0.01128419, 0.012046  , 0.01230034,\n",
            "       0.01005275, 0.00977574, 0.00988733, 0.00985803, 0.01017826,\n",
            "       0.00998743, 0.00927842, 0.00939303, 0.00909515, 0.00918936,\n",
            "       0.00895024, 0.00855465, 0.00840965, 0.00914173, 0.00894787,\n",
            "       0.00892666, 0.00919945, 0.00835487, 0.00851858, 0.00866958,\n",
            "       0.00821259, 0.00766291, 0.00825235, 0.00884607, 0.00868431,\n",
            "       0.00896737, 0.00820503, 0.00912   , 0.00859701, 0.00849906,\n",
            "       0.00773092, 0.00800199, 0.00839519, 0.00760326, 0.00846684])\n",
            "Training Accuracy Set:  array([0.39045   , 0.57997501, 0.66877496, 0.72149998, 0.74822497,\n",
            "       0.77329999, 0.79004997, 0.80394995, 0.81355   , 0.82009995,\n",
            "       0.82782495, 0.83524996, 0.83739996, 0.84265   , 0.84884995,\n",
            "       0.847525  , 0.85502499, 0.85420001, 0.86104995, 0.85914999,\n",
            "       0.86364996, 0.86495   , 0.86532497, 0.867975  , 0.87062496,\n",
            "       0.9052    , 0.91029996, 0.91104996, 0.91249996, 0.914325  ,\n",
            "       0.91525   , 0.91277498, 0.91177499, 0.93895   , 0.94312495,\n",
            "       0.944125  , 0.94647497, 0.94509995, 0.94637495, 0.94529998,\n",
            "       0.94734997, 0.963175  , 0.96899998, 0.96982497, 0.97147501,\n",
            "       0.97364998, 0.97192496, 0.97292495, 0.97359997, 0.97382498,\n",
            "       0.97382498, 0.97372496, 0.97277498, 0.97347498, 0.97507495,\n",
            "       0.97332495, 0.97189999, 0.98499995, 0.98772496, 0.98857498,\n",
            "       0.98984998, 0.98949999, 0.98995   , 0.98954999, 0.99072498,\n",
            "       0.99239999, 0.994425  , 0.99404997, 0.99537498, 0.99497497,\n",
            "       0.99582499, 0.99519998, 0.99567497, 0.99605   , 0.9957    ,\n",
            "       0.99632496, 0.99549997, 0.99637496, 0.99759996, 0.99729997,\n",
            "       0.99734998, 0.99754995, 0.99789995, 0.99724996, 0.99739999,\n",
            "       0.99809998, 0.99822497, 0.99812496, 0.99812496, 0.997675  ,\n",
            "       0.99794996, 0.99822497, 0.99812496, 0.99847496, 0.99817497,\n",
            "       0.99839997, 0.99842495, 0.99855   , 0.9982    , 0.99859995,\n",
            "       0.99832499, 0.99822497, 0.99862498, 0.99869996, 0.99842495,\n",
            "       0.99864995, 0.99884999, 0.99844998, 0.99814999, 0.99859995,\n",
            "       0.99839997, 0.99862498, 0.99827498, 0.99857497, 0.99839997,\n",
            "       0.9989    , 0.99874997, 0.99847496, 0.99892497, 0.99834996])\n",
            "Validation Loss Set:  array([1.62342262, 1.23547566, 1.19479787, 1.53580368, 1.08412611,\n",
            "       0.90649474, 0.75236231, 0.8768388 , 1.15808451, 0.75439602,\n",
            "       0.90548778, 0.8853935 , 0.63843179, 0.63551033, 0.80657756,\n",
            "       0.86939639, 0.66458052, 0.71015853, 0.98159319, 0.67434543,\n",
            "       0.62285471, 0.69211686, 0.73795903, 0.77567118, 0.75192147,\n",
            "       0.46415925, 0.45307934, 0.48942471, 0.57367247, 0.5518117 ,\n",
            "       0.45561317, 0.64142799, 0.58449107, 0.36775607, 0.44709939,\n",
            "       0.37292141, 0.47742018, 0.48038238, 0.41485056, 0.45572093,\n",
            "       0.39309034, 0.34107766, 0.36909676, 0.36279622, 0.35023731,\n",
            "       0.39409408, 0.39514256, 0.38495493, 0.40753016, 0.37448061,\n",
            "       0.40596312, 0.3547467 , 0.43357411, 0.38619399, 0.38813961,\n",
            "       0.39045057, 0.41463694, 0.3487103 , 0.35101467, 0.33390051,\n",
            "       0.34542489, 0.36648101, 0.36530775, 0.35734597, 0.36367124,\n",
            "       0.35189992, 0.34472728, 0.34275562, 0.35179687, 0.35628679,\n",
            "       0.35194764, 0.33613381, 0.3522813 , 0.36797574, 0.36452708,\n",
            "       0.37453303, 0.36632472, 0.35407275, 0.35510811, 0.37291777,\n",
            "       0.35329685, 0.37258849, 0.3596327 , 0.37167972, 0.37243861,\n",
            "       0.38023359, 0.35199553, 0.35344142, 0.36755151, 0.35836834,\n",
            "       0.35599712, 0.35985643, 0.36201614, 0.35736108, 0.35841039,\n",
            "       0.36518463, 0.36943892, 0.36624932, 0.3628391 , 0.36281943,\n",
            "       0.36851233, 0.3740285 , 0.36143878, 0.38417286, 0.3762829 ,\n",
            "       0.35935313, 0.36690712, 0.35444233, 0.36925125, 0.37213659,\n",
            "       0.36541107, 0.37186453, 0.37104961, 0.35882875, 0.37652409,\n",
            "       0.36537334, 0.37899083, 0.36350381, 0.37925407, 0.3580831 ])\n",
            "Validation Accuracy Set:  array([0.41429999, 0.54809999, 0.59920001, 0.53189999, 0.64749998,\n",
            "       0.69089997, 0.74189997, 0.70309997, 0.64699996, 0.73640001,\n",
            "       0.7087    , 0.70829999, 0.78179997, 0.78319997, 0.72490001,\n",
            "       0.71700001, 0.77469999, 0.76459998, 0.6857    , 0.77459997,\n",
            "       0.79719996, 0.76980001, 0.76279998, 0.76629996, 0.76339996,\n",
            "       0.84779996, 0.8477    , 0.84129995, 0.81949997, 0.81979996,\n",
            "       0.84779996, 0.79049999, 0.80769998, 0.88229996, 0.85829997,\n",
            "       0.87979996, 0.85249996, 0.85420001, 0.87109995, 0.85969996,\n",
            "       0.87219995, 0.89219999, 0.89019996, 0.88799995, 0.89559996,\n",
            "       0.8926    , 0.88189995, 0.88849998, 0.88249999, 0.88709998,\n",
            "       0.88369995, 0.89459997, 0.87709999, 0.88629997, 0.88699996,\n",
            "       0.88379997, 0.87909997, 0.90499997, 0.9034    , 0.90919995,\n",
            "       0.9048    , 0.90169996, 0.903     , 0.9059    , 0.90399998,\n",
            "       0.90419996, 0.90899998, 0.9095    , 0.90789998, 0.90969998,\n",
            "       0.90979999, 0.91229999, 0.90989995, 0.90709996, 0.91049999,\n",
            "       0.90889996, 0.9084    , 0.90819997, 0.90829998, 0.90959996,\n",
            "       0.91219997, 0.90869999, 0.90919995, 0.90919995, 0.91399997,\n",
            "       0.91239995, 0.91069996, 0.91349995, 0.91049999, 0.90999997,\n",
            "       0.91229999, 0.91339999, 0.91149998, 0.91339999, 0.91099995,\n",
            "       0.9113    , 0.91079998, 0.91279995, 0.91249996, 0.90999997,\n",
            "       0.9109    , 0.91179997, 0.91289997, 0.90779996, 0.90979999,\n",
            "       0.91099995, 0.91039997, 0.91339999, 0.91169995, 0.91209996,\n",
            "       0.91159999, 0.91109997, 0.90979999, 0.91169995, 0.9091    ,\n",
            "       0.91149998, 0.91079998, 0.91319996, 0.91060001, 0.91599995])\n",
            "Test Loss Set:  array([1.60591662, 1.1914351 , 1.27395296, 1.61479247, 1.15463042,\n",
            "       0.9112978 , 0.7439096 , 0.91708374, 1.1029551 , 0.74615741,\n",
            "       0.95354801, 0.92271948, 0.61847419, 0.65734327, 0.83164448,\n",
            "       0.88968611, 0.67372102, 0.73523706, 0.97652495, 0.68480676,\n",
            "       0.5937553 , 0.69509816, 0.75287068, 0.8124947 , 0.78880024,\n",
            "       0.46034014, 0.46721992, 0.47797444, 0.604581  , 0.5462262 ,\n",
            "       0.41720402, 0.66754413, 0.55222332, 0.35296655, 0.45391065,\n",
            "       0.36314747, 0.48926198, 0.4686349 , 0.4405002 , 0.46224856,\n",
            "       0.39932123, 0.33536184, 0.37126353, 0.35161412, 0.35590288,\n",
            "       0.41079843, 0.39812738, 0.39554238, 0.41176298, 0.37277153,\n",
            "       0.41893753, 0.35542965, 0.4392381 , 0.37396297, 0.3828617 ,\n",
            "       0.40712425, 0.42115387, 0.33073315, 0.35888204, 0.34095594,\n",
            "       0.35057032, 0.36620033, 0.36187476, 0.3737314 , 0.35962859,\n",
            "       0.36073065, 0.35075137, 0.35714161, 0.35152066, 0.36065724,\n",
            "       0.35375023, 0.36053643, 0.35544458, 0.35669088, 0.3641853 ,\n",
            "       0.36475533, 0.37937796, 0.36674395, 0.36566937, 0.36105937,\n",
            "       0.36656183, 0.36568514, 0.36543134, 0.36367676, 0.37226641,\n",
            "       0.36605671, 0.36457106, 0.36656725, 0.36508381, 0.36835787,\n",
            "       0.36713141, 0.36508238, 0.36344811, 0.36629426, 0.36584523,\n",
            "       0.36961344, 0.36795497, 0.36867648, 0.36686686, 0.36811981,\n",
            "       0.36344513, 0.36942673, 0.36982876, 0.36895025, 0.37065941,\n",
            "       0.36663896, 0.37236652, 0.37170222, 0.3675074 , 0.36788741,\n",
            "       0.3720966 , 0.36677676, 0.36981419, 0.37307525, 0.37006655,\n",
            "       0.36813325, 0.37113562, 0.37028101, 0.37077329, 0.36784762])\n",
            "Test Accuracy Set:  array([0.4276    , 0.56639999, 0.58829999, 0.53960001, 0.6505    ,\n",
            "       0.6961    , 0.74869996, 0.70039999, 0.67109996, 0.74430001,\n",
            "       0.70120001, 0.70910001, 0.78839999, 0.78119999, 0.73499995,\n",
            "       0.71939999, 0.77389997, 0.7683    , 0.70660001, 0.7816    ,\n",
            "       0.80239999, 0.77399999, 0.7622    , 0.76299995, 0.76099998,\n",
            "       0.84630001, 0.85079998, 0.8484    , 0.8161    , 0.82309997,\n",
            "       0.86299998, 0.79339999, 0.82409996, 0.88940001, 0.86059999,\n",
            "       0.8836    , 0.85459995, 0.85509998, 0.87469995, 0.86139995,\n",
            "       0.87619996, 0.89779997, 0.89219999, 0.89379996, 0.89679998,\n",
            "       0.88789999, 0.88789999, 0.88909996, 0.88519996, 0.89219999,\n",
            "       0.8829    , 0.89839995, 0.88189995, 0.89719999, 0.89279997,\n",
            "       0.88549995, 0.87979996, 0.90999997, 0.903     , 0.90849996,\n",
            "       0.91079998, 0.90789998, 0.9102    , 0.90469998, 0.91119999,\n",
            "       0.91109997, 0.91319996, 0.91159999, 0.9127    , 0.9127    ,\n",
            "       0.91339999, 0.91060001, 0.91399997, 0.91459996, 0.91249996,\n",
            "       0.91349995, 0.90959996, 0.91259998, 0.91429996, 0.9152    ,\n",
            "       0.91419995, 0.91419995, 0.91289997, 0.9152    , 0.91419995,\n",
            "       0.9149    , 0.91499996, 0.91599995, 0.9149    , 0.91499996,\n",
            "       0.91569996, 0.91659999, 0.91499996, 0.91569996, 0.9152    ,\n",
            "       0.9145    , 0.91419995, 0.91509998, 0.91439998, 0.91459996,\n",
            "       0.9152    , 0.91479999, 0.91479999, 0.91469997, 0.91479999,\n",
            "       0.91589999, 0.91419995, 0.91539997, 0.91469997, 0.91549999,\n",
            "       0.9149    , 0.91389996, 0.9149    , 0.91569996, 0.91529995,\n",
            "       0.91539997, 0.91579998, 0.91479999, 0.91609997, 0.91529995])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xiZpYlx1KF4i"
      },
      "source": [
        "##NAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EKxB_FGjJNp",
        "colab": {}
      },
      "source": [
        "def decayed_stats_NAG(lr, momentum, decay, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_NAG = np.zeros(epochs)\n",
        "  train_accuracy_NAG = np.zeros(epochs)\n",
        "  valid_loss_NAG = np.zeros(epochs)\n",
        "  valid_accuracy_NAG = np.zeros(epochs)\n",
        "  test_accuracy_NAG = np.zeros(epochs)\n",
        "  test_loss_NAG = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  #Initilise validation set error as criteria at change point\n",
        "  error = 100\n",
        "  prev_error = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      #Creates a deep copy of the parameter and gradient tensors and makes them shareable to enable re-use of Resnet modules multiple times\n",
        "      model = copy.deepcopy(model)\n",
        "      optimiser = optim.SGD(model.parameters(), lr = lr, momentum = momentum, nesterov = True, weight_decay=0.0005)\n",
        "\n",
        "      #Train the model on training data\n",
        "      trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'],  verbose=0).to(device)\n",
        "      trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "      result = trial.run(epochs=1)\n",
        "\n",
        "      #At change points, update the hyperparameters depending on whether validation error reduced by more than 0.2% or not\n",
        "      if epoch%change == 0:\n",
        "        if ((prev_error-error)/prev_error < 0.002) and epoch!=0:\n",
        "        \n",
        "          if lr/decay < 0.001:\n",
        "           lr = 0.001\n",
        "          else:\n",
        "              lr=lr/decay\n",
        "\n",
        "        else:\n",
        "            lr=lr\n",
        "\n",
        "        prev_error = error\n",
        "\n",
        "      #Compute the metrics on Test Dataset\n",
        "      trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "      #Store the metrics at each epoch\n",
        "      train_loss_NAG[epoch] = result[0]['loss']\n",
        "      train_accuracy_NAG[epoch] = result[0]['acc']\n",
        "      valid_loss_NAG[epoch] = result[0]['val_loss']\n",
        "      valid_accuracy_NAG[epoch] = result[0]['val_acc']\n",
        "      test_accuracy_NAG[epoch] = result[0]['test_acc']\n",
        "      test_loss_NAG[epoch] = result[0]['test_loss']\n",
        "\n",
        "      error = 1 - result[0]['val_acc']\n",
        "\n",
        "      print(epoch, lr, momentum, result)\n",
        "\n",
        "  return lr, train_loss_NAG, train_accuracy_NAG, valid_loss_NAG, valid_accuracy_NAG, test_loss_NAG, test_accuracy_NAG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-NIYNEv8pHS",
        "colab": {}
      },
      "source": [
        "final_lr_NAG, train_loss_NAG, train_accuracy_NAG, valid_loss_NAG, valid_accuracy_NAG, test_loss_NAG, test_accuracy_NAG = decayed_stats_NAG(0.27, 0.5, 2, 120, 4) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-y6FP0cmMOCh"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-teRGnavlVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for NAG on Batch Size 128 and Decayed Hyperparameter Schedule\n",
        "\n",
        "\n",
        "# final_lr_NAG = 0.001\n",
        "# train_loss_NAG = np.array([1.65498281, 1.19095516, 0.95103908, 0.7941938 , 0.7077682 ,\n",
        "#        0.64279735, 0.59541982, 0.56061363, 0.53479171, 0.51115042,\n",
        "#        0.49727473, 0.4761408 , 0.46657565, 0.45412764, 0.44297096,\n",
        "#        0.43352497, 0.42456883, 0.41604123, 0.40915355, 0.39653465,\n",
        "#        0.40108058, 0.39015427, 0.38441992, 0.37495151, 0.3743757 ,\n",
        "#        0.26932809, 0.25699767, 0.25294313, 0.25303888, 0.25109008,\n",
        "#        0.24842639, 0.25559685, 0.24500842, 0.17854191, 0.16089088,\n",
        "#        0.15504552, 0.15424499, 0.15798311, 0.15427405, 0.15110721,\n",
        "#        0.14913288, 0.10288537, 0.09263112, 0.08910806, 0.08320157,\n",
        "#        0.08016934, 0.07846639, 0.07685111, 0.07601637, 0.07926019,\n",
        "#        0.07767212, 0.07647746, 0.07217151, 0.05037781, 0.0445978 ,\n",
        "#        0.03975864, 0.03660988, 0.03536596, 0.03785622, 0.03356153,\n",
        "#        0.03723579, 0.02757312, 0.02255934, 0.02269465, 0.02097191,\n",
        "#        0.01999737, 0.01962481, 0.01898594, 0.01954941, 0.01777128,\n",
        "#        0.01776557, 0.01667634, 0.01786612, 0.01656728, 0.01734448,\n",
        "#        0.01636587, 0.01664862, 0.01397069, 0.01255884, 0.01288573,\n",
        "#        0.01240814, 0.01136595, 0.01115855, 0.01172929, 0.01123302,\n",
        "#        0.01094872, 0.01054471, 0.01083193, 0.0106322 , 0.01033903,\n",
        "#        0.00988976, 0.00981284, 0.01016284, 0.01002517, 0.00988318,\n",
        "#        0.01003138, 0.0096426 , 0.01032017, 0.00911949, 0.00975328,\n",
        "#        0.01024343, 0.01067175, 0.00918417, 0.01006736, 0.00984796,\n",
        "#        0.00997536, 0.0094613 , 0.00913448, 0.00894703, 0.00994386,\n",
        "#        0.00923446, 0.00911251, 0.00887064, 0.00929435, 0.00946052,\n",
        "#        0.0090104 , 0.00939149, 0.00923066, 0.00933375, 0.00866602])\n",
        "# train_accuracy_NAG = np.array([0.38322499, 0.57045001, 0.65959996, 0.72227496, 0.75339997,\n",
        "#        0.77634996, 0.79427499, 0.80675   , 0.81612498, 0.82279998,\n",
        "#        0.828125  , 0.83652496, 0.83939999, 0.84332496, 0.84794998,\n",
        "#        0.85152495, 0.8527    , 0.85904998, 0.85907495, 0.8646    ,\n",
        "#        0.86212498, 0.86519998, 0.86792499, 0.86947501, 0.87107497,\n",
        "#        0.90939999, 0.91235   , 0.91289997, 0.91297495, 0.91359997,\n",
        "#        0.91404998, 0.91275001, 0.913275  , 0.9386    , 0.94322497,\n",
        "#        0.94714999, 0.94645   , 0.94569999, 0.94674999, 0.94749999,\n",
        "#        0.9483    , 0.96599996, 0.96799999, 0.97084999, 0.97232497,\n",
        "#        0.97277498, 0.97442496, 0.97472495, 0.97409999, 0.97347498,\n",
        "#        0.97262496, 0.97382498, 0.97567499, 0.98404998, 0.98559999,\n",
        "#        0.988675  , 0.989375  , 0.98939997, 0.98817497, 0.98989999,\n",
        "#        0.98864996, 0.99199998, 0.99447495, 0.99399996, 0.99474996,\n",
        "#        0.995175  , 0.99537498, 0.99507499, 0.99514997, 0.99542499,\n",
        "#        0.99547499, 0.99599999, 0.99519998, 0.99572498, 0.995875  ,\n",
        "#        0.99597496, 0.99602497, 0.99699998, 0.99734998, 0.99719995,\n",
        "#        0.99724996, 0.99792498, 0.997675  , 0.99769998, 0.99782497,\n",
        "#        0.99787498, 0.99814999, 0.99799997, 0.99774998, 0.99812496,\n",
        "#        0.9982    , 0.99834996, 0.99812496, 0.99807495, 0.99829996,\n",
        "#        0.998025  , 0.99839997, 0.99792498, 0.99834996, 0.998025  ,\n",
        "#        0.99799997, 0.99785   , 0.99857497, 0.9982    , 0.99817497,\n",
        "#        0.99799997, 0.99839997, 0.99847496, 0.99844998, 0.99794996,\n",
        "#        0.99832499, 0.99847496, 0.99857497, 0.99829996, 0.99817497,\n",
        "#        0.99859995, 0.99839997, 0.99829996, 0.99829996, 0.998375  ])\n",
        "# valid_loss_NAG = np.array([2.43370032, 2.07764578, 1.08591318, 1.88404095, 1.20477533,\n",
        "#        1.66792166, 1.09705317, 1.05795324, 1.00582433, 0.69387591,\n",
        "#        0.92108822, 0.76599711, 0.86084229, 0.8571682 , 0.66815871,\n",
        "#        0.66984707, 0.62791109, 1.02968764, 0.63766313, 0.62579542,\n",
        "#        0.63390791, 0.82056248, 0.64718723, 0.77173358, 0.5760501 ,\n",
        "#        0.47471383, 0.49938038, 0.57749677, 0.47257906, 0.5926953 ,\n",
        "#        0.51867038, 1.19575071, 0.48249006, 0.36488047, 0.4137879 ,\n",
        "#        0.38315776, 0.43242574, 0.38242114, 0.42796785, 0.47504538,\n",
        "#        0.5046556 , 0.34323752, 0.34468853, 0.35997367, 0.34765089,\n",
        "#        0.40294367, 0.37902296, 0.35789633, 0.40669543, 0.37892824,\n",
        "#        0.39904183, 0.41160879, 0.42361972, 0.33774972, 0.35001588,\n",
        "#        0.34624416, 0.34446266, 0.35768208, 0.34616089, 0.46559221,\n",
        "#        0.35933656, 0.33211288, 0.33268976, 0.35651031, 0.35327908,\n",
        "#        0.35609546, 0.35999909, 0.3590824 , 0.36918741, 0.35285121,\n",
        "#        0.3611415 , 0.35182405, 0.36398819, 0.35823604, 0.36397129,\n",
        "#        0.3526538 , 0.36445954, 0.34973401, 0.3585313 , 0.35679063,\n",
        "#        0.34759465, 0.35573617, 0.35352412, 0.36208469, 0.38350415,\n",
        "#        0.3534385 , 0.35224676, 0.35962462, 0.34470886, 0.36835364,\n",
        "#        0.36348164, 0.3563129 , 0.35723841, 0.36069801, 0.3590453 ,\n",
        "#        0.36444351, 0.37466085, 0.35277653, 0.36181396, 0.36140046,\n",
        "#        0.37865382, 0.36188245, 0.35130569, 0.35969234, 0.35804081,\n",
        "#        0.35583276, 0.36718154, 0.34666842, 0.36262122, 0.35175753,\n",
        "#        0.35575011, 0.35831729, 0.36647952, 0.36152342, 0.35624683,\n",
        "#        0.36875924, 0.36934987, 0.36310107, 0.36247832, 0.35439059])\n",
        "# valid_accuracy_NAG = np.array([0.2841    , 0.37559998, 0.62720001, 0.48719999, 0.61229998,\n",
        "#        0.53659999, 0.65689999, 0.6609    , 0.68979996, 0.764     ,\n",
        "#        0.70209998, 0.74430001, 0.70819998, 0.71529996, 0.77429998,\n",
        "#        0.77739996, 0.78779995, 0.67269999, 0.7895    , 0.79029995,\n",
        "#        0.78429997, 0.74189997, 0.77789998, 0.74349999, 0.8064    ,\n",
        "#        0.83559996, 0.8344    , 0.80769998, 0.84509999, 0.8125    ,\n",
        "#        0.8351    , 0.70109999, 0.84499997, 0.87849998, 0.86829996,\n",
        "#        0.875     , 0.86539996, 0.88019997, 0.86949998, 0.85049999,\n",
        "#        0.8538    , 0.89829999, 0.89409995, 0.89069998, 0.89399999,\n",
        "#        0.8854    , 0.88439995, 0.89579999, 0.88089997, 0.88909996,\n",
        "#        0.8836    , 0.88339996, 0.87829995, 0.90329999, 0.9012    ,\n",
        "#        0.90549999, 0.9059    , 0.90169996, 0.90399998, 0.8786    ,\n",
        "#        0.89949995, 0.91159999, 0.9109    , 0.90599996, 0.90799999,\n",
        "#        0.90739995, 0.90799999, 0.90719998, 0.90599996, 0.9077    ,\n",
        "#        0.9095    , 0.90929997, 0.91239995, 0.91060001, 0.90929997,\n",
        "#        0.90939999, 0.9077    , 0.91229999, 0.9077    , 0.90919995,\n",
        "#        0.91329998, 0.90929997, 0.91339999, 0.91099995, 0.90929997,\n",
        "#        0.91099995, 0.91329998, 0.90939999, 0.91029996, 0.91039997,\n",
        "#        0.90959996, 0.91259998, 0.9131    , 0.90979999, 0.91249996,\n",
        "#        0.91039997, 0.91109997, 0.9102    , 0.91099995, 0.91189998,\n",
        "#        0.9102    , 0.9095    , 0.91399997, 0.91369998, 0.91259998,\n",
        "#        0.91409999, 0.91389996, 0.91189998, 0.91039997, 0.9113    ,\n",
        "#        0.9131    , 0.91179997, 0.91060001, 0.91179997, 0.91239995,\n",
        "#        0.91060001, 0.91179997, 0.91249996, 0.90989995, 0.9127    ])\n",
        "# test_loss_NAG = np.array([2.38101649, 2.12079334, 1.11320114, 2.01935101, 1.28585875,\n",
        "#        1.72639275, 1.10419047, 1.07455182, 1.08552146, 0.67305392,\n",
        "#        0.93668413, 0.74991602, 0.85994822, 0.85551012, 0.67778784,\n",
        "#        0.66764951, 0.59594762, 1.08347821, 0.63060296, 0.60810035,\n",
        "#        0.65692502, 0.83070624, 0.69839317, 0.7392531 , 0.56354284,\n",
        "#        0.4546285 , 0.5059424 , 0.56227612, 0.48127472, 0.57329577,\n",
        "#        0.53440589, 1.27739286, 0.47351685, 0.33890143, 0.40794551,\n",
        "#        0.37220439, 0.42351413, 0.38053894, 0.40536484, 0.482297  ,\n",
        "#        0.50183839, 0.33935869, 0.34211558, 0.35724568, 0.33201322,\n",
        "#        0.39018527, 0.38851708, 0.36242512, 0.42196062, 0.36930948,\n",
        "#        0.39829147, 0.42458341, 0.42044702, 0.34034133, 0.33551016,\n",
        "#        0.35134944, 0.34641218, 0.36415759, 0.36082074, 0.47179443,\n",
        "#        0.34267551, 0.33213431, 0.33686176, 0.34692481, 0.33935139,\n",
        "#        0.34856793, 0.35638323, 0.34989411, 0.35645932, 0.34521192,\n",
        "#        0.3639757 , 0.35345978, 0.3559278 , 0.35998875, 0.37299931,\n",
        "#        0.36366838, 0.35822359, 0.36023167, 0.35914043, 0.35298786,\n",
        "#        0.35777399, 0.35278058, 0.35706979, 0.35490134, 0.3599326 ,\n",
        "#        0.35641596, 0.35768768, 0.35645041, 0.35665259, 0.35700625,\n",
        "#        0.35901663, 0.35643762, 0.35707346, 0.35851136, 0.35803655,\n",
        "#        0.35970584, 0.35975415, 0.3574332 , 0.35826299, 0.35891455,\n",
        "#        0.35882199, 0.36028185, 0.36100969, 0.35644937, 0.35529009,\n",
        "#        0.35846287, 0.3592627 , 0.35711148, 0.35545266, 0.35924554,\n",
        "#        0.35711151, 0.35984382, 0.36143178, 0.35904789, 0.36267146,\n",
        "#        0.36300373, 0.36386409, 0.36137807, 0.36226055, 0.36112747])\n",
        "# test_accuracy_NAG = np.array([0.3125    , 0.38819999, 0.62919998, 0.48859999, 0.60609996,\n",
        "#        0.56229997, 0.66209996, 0.66759998, 0.68369997, 0.77059996,\n",
        "#        0.70949996, 0.74949998, 0.71520001, 0.71489996, 0.7687    ,\n",
        "#        0.78310001, 0.80089998, 0.67659998, 0.79960001, 0.7974    ,\n",
        "#        0.7881    , 0.74259996, 0.7701    , 0.75400001, 0.8082    ,\n",
        "#        0.84609997, 0.83399999, 0.8168    , 0.84549999, 0.81949997,\n",
        "#        0.84209996, 0.69839996, 0.85299999, 0.89139998, 0.87349999,\n",
        "#        0.884     , 0.86659998, 0.88729995, 0.87629998, 0.85479999,\n",
        "#        0.85529995, 0.89819998, 0.89679998, 0.89359999, 0.90209997,\n",
        "#        0.88929999, 0.8915    , 0.8951    , 0.88330001, 0.89589995,\n",
        "#        0.89279997, 0.88049996, 0.884     , 0.90619999, 0.90739995,\n",
        "#        0.9084    , 0.91049999, 0.90399998, 0.90749997, 0.88349998,\n",
        "#        0.90779996, 0.91429996, 0.91189998, 0.90969998, 0.91349995,\n",
        "#        0.91009998, 0.90799999, 0.91099995, 0.90979999, 0.91289997,\n",
        "#        0.91119999, 0.91060001, 0.91329998, 0.90869999, 0.90979999,\n",
        "#        0.91209996, 0.91069996, 0.91179997, 0.91409999, 0.91419995,\n",
        "#        0.91299999, 0.91329998, 0.9131    , 0.91319996, 0.91409999,\n",
        "#        0.91339999, 0.91339999, 0.91339999, 0.91159999, 0.91369998,\n",
        "#        0.91259998, 0.91369998, 0.9149    , 0.91429996, 0.91329998,\n",
        "#        0.91419995, 0.91359997, 0.91359997, 0.91279995, 0.91399997,\n",
        "#        0.91339999, 0.91209996, 0.91339999, 0.91289997, 0.91509998,\n",
        "#        0.91399997, 0.91399997, 0.91469997, 0.9131    , 0.91319996,\n",
        "#        0.91279995, 0.91299999, 0.91369998, 0.91469997, 0.91319996,\n",
        "#        0.91299999, 0.91359997, 0.91389996, 0.91349995, 0.91289997])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WCFrkMNqK9OB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b01f261-b6e2-4073-9f27-909ed626953e"
      },
      "source": [
        "print(\"Final Learning Rate reached at the end of training: \", final_lr_HeavyBall)\n",
        "print(\"Training Loss Set: \",repr(train_loss_NAG))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_NAG))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_NAG))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_NAG))\n",
        "print(\"Test Loss Set: \", repr(test_loss_NAG))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_NAG))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Learning Rate reached at the end of training:  0.001\n",
            "Training Loss Set:  array([1.65498281, 1.19095516, 0.95103908, 0.7941938 , 0.7077682 ,\n",
            "       0.64279735, 0.59541982, 0.56061363, 0.53479171, 0.51115042,\n",
            "       0.49727473, 0.4761408 , 0.46657565, 0.45412764, 0.44297096,\n",
            "       0.43352497, 0.42456883, 0.41604123, 0.40915355, 0.39653465,\n",
            "       0.40108058, 0.39015427, 0.38441992, 0.37495151, 0.3743757 ,\n",
            "       0.26932809, 0.25699767, 0.25294313, 0.25303888, 0.25109008,\n",
            "       0.24842639, 0.25559685, 0.24500842, 0.17854191, 0.16089088,\n",
            "       0.15504552, 0.15424499, 0.15798311, 0.15427405, 0.15110721,\n",
            "       0.14913288, 0.10288537, 0.09263112, 0.08910806, 0.08320157,\n",
            "       0.08016934, 0.07846639, 0.07685111, 0.07601637, 0.07926019,\n",
            "       0.07767212, 0.07647746, 0.07217151, 0.05037781, 0.0445978 ,\n",
            "       0.03975864, 0.03660988, 0.03536596, 0.03785622, 0.03356153,\n",
            "       0.03723579, 0.02757312, 0.02255934, 0.02269465, 0.02097191,\n",
            "       0.01999737, 0.01962481, 0.01898594, 0.01954941, 0.01777128,\n",
            "       0.01776557, 0.01667634, 0.01786612, 0.01656728, 0.01734448,\n",
            "       0.01636587, 0.01664862, 0.01397069, 0.01255884, 0.01288573,\n",
            "       0.01240814, 0.01136595, 0.01115855, 0.01172929, 0.01123302,\n",
            "       0.01094872, 0.01054471, 0.01083193, 0.0106322 , 0.01033903,\n",
            "       0.00988976, 0.00981284, 0.01016284, 0.01002517, 0.00988318,\n",
            "       0.01003138, 0.0096426 , 0.01032017, 0.00911949, 0.00975328,\n",
            "       0.01024343, 0.01067175, 0.00918417, 0.01006736, 0.00984796,\n",
            "       0.00997536, 0.0094613 , 0.00913448, 0.00894703, 0.00994386,\n",
            "       0.00923446, 0.00911251, 0.00887064, 0.00929435, 0.00946052,\n",
            "       0.0090104 , 0.00939149, 0.00923066, 0.00933375, 0.00866602])\n",
            "Training Accuracy Set:  array([0.38322499, 0.57045001, 0.65959996, 0.72227496, 0.75339997,\n",
            "       0.77634996, 0.79427499, 0.80675   , 0.81612498, 0.82279998,\n",
            "       0.828125  , 0.83652496, 0.83939999, 0.84332496, 0.84794998,\n",
            "       0.85152495, 0.8527    , 0.85904998, 0.85907495, 0.8646    ,\n",
            "       0.86212498, 0.86519998, 0.86792499, 0.86947501, 0.87107497,\n",
            "       0.90939999, 0.91235   , 0.91289997, 0.91297495, 0.91359997,\n",
            "       0.91404998, 0.91275001, 0.913275  , 0.9386    , 0.94322497,\n",
            "       0.94714999, 0.94645   , 0.94569999, 0.94674999, 0.94749999,\n",
            "       0.9483    , 0.96599996, 0.96799999, 0.97084999, 0.97232497,\n",
            "       0.97277498, 0.97442496, 0.97472495, 0.97409999, 0.97347498,\n",
            "       0.97262496, 0.97382498, 0.97567499, 0.98404998, 0.98559999,\n",
            "       0.988675  , 0.989375  , 0.98939997, 0.98817497, 0.98989999,\n",
            "       0.98864996, 0.99199998, 0.99447495, 0.99399996, 0.99474996,\n",
            "       0.995175  , 0.99537498, 0.99507499, 0.99514997, 0.99542499,\n",
            "       0.99547499, 0.99599999, 0.99519998, 0.99572498, 0.995875  ,\n",
            "       0.99597496, 0.99602497, 0.99699998, 0.99734998, 0.99719995,\n",
            "       0.99724996, 0.99792498, 0.997675  , 0.99769998, 0.99782497,\n",
            "       0.99787498, 0.99814999, 0.99799997, 0.99774998, 0.99812496,\n",
            "       0.9982    , 0.99834996, 0.99812496, 0.99807495, 0.99829996,\n",
            "       0.998025  , 0.99839997, 0.99792498, 0.99834996, 0.998025  ,\n",
            "       0.99799997, 0.99785   , 0.99857497, 0.9982    , 0.99817497,\n",
            "       0.99799997, 0.99839997, 0.99847496, 0.99844998, 0.99794996,\n",
            "       0.99832499, 0.99847496, 0.99857497, 0.99829996, 0.99817497,\n",
            "       0.99859995, 0.99839997, 0.99829996, 0.99829996, 0.998375  ])\n",
            "Validation Loss Set:  array([2.43370032, 2.07764578, 1.08591318, 1.88404095, 1.20477533,\n",
            "       1.66792166, 1.09705317, 1.05795324, 1.00582433, 0.69387591,\n",
            "       0.92108822, 0.76599711, 0.86084229, 0.8571682 , 0.66815871,\n",
            "       0.66984707, 0.62791109, 1.02968764, 0.63766313, 0.62579542,\n",
            "       0.63390791, 0.82056248, 0.64718723, 0.77173358, 0.5760501 ,\n",
            "       0.47471383, 0.49938038, 0.57749677, 0.47257906, 0.5926953 ,\n",
            "       0.51867038, 1.19575071, 0.48249006, 0.36488047, 0.4137879 ,\n",
            "       0.38315776, 0.43242574, 0.38242114, 0.42796785, 0.47504538,\n",
            "       0.5046556 , 0.34323752, 0.34468853, 0.35997367, 0.34765089,\n",
            "       0.40294367, 0.37902296, 0.35789633, 0.40669543, 0.37892824,\n",
            "       0.39904183, 0.41160879, 0.42361972, 0.33774972, 0.35001588,\n",
            "       0.34624416, 0.34446266, 0.35768208, 0.34616089, 0.46559221,\n",
            "       0.35933656, 0.33211288, 0.33268976, 0.35651031, 0.35327908,\n",
            "       0.35609546, 0.35999909, 0.3590824 , 0.36918741, 0.35285121,\n",
            "       0.3611415 , 0.35182405, 0.36398819, 0.35823604, 0.36397129,\n",
            "       0.3526538 , 0.36445954, 0.34973401, 0.3585313 , 0.35679063,\n",
            "       0.34759465, 0.35573617, 0.35352412, 0.36208469, 0.38350415,\n",
            "       0.3534385 , 0.35224676, 0.35962462, 0.34470886, 0.36835364,\n",
            "       0.36348164, 0.3563129 , 0.35723841, 0.36069801, 0.3590453 ,\n",
            "       0.36444351, 0.37466085, 0.35277653, 0.36181396, 0.36140046,\n",
            "       0.37865382, 0.36188245, 0.35130569, 0.35969234, 0.35804081,\n",
            "       0.35583276, 0.36718154, 0.34666842, 0.36262122, 0.35175753,\n",
            "       0.35575011, 0.35831729, 0.36647952, 0.36152342, 0.35624683,\n",
            "       0.36875924, 0.36934987, 0.36310107, 0.36247832, 0.35439059])\n",
            "Validation Accuracy Set:  array([0.2841    , 0.37559998, 0.62720001, 0.48719999, 0.61229998,\n",
            "       0.53659999, 0.65689999, 0.6609    , 0.68979996, 0.764     ,\n",
            "       0.70209998, 0.74430001, 0.70819998, 0.71529996, 0.77429998,\n",
            "       0.77739996, 0.78779995, 0.67269999, 0.7895    , 0.79029995,\n",
            "       0.78429997, 0.74189997, 0.77789998, 0.74349999, 0.8064    ,\n",
            "       0.83559996, 0.8344    , 0.80769998, 0.84509999, 0.8125    ,\n",
            "       0.8351    , 0.70109999, 0.84499997, 0.87849998, 0.86829996,\n",
            "       0.875     , 0.86539996, 0.88019997, 0.86949998, 0.85049999,\n",
            "       0.8538    , 0.89829999, 0.89409995, 0.89069998, 0.89399999,\n",
            "       0.8854    , 0.88439995, 0.89579999, 0.88089997, 0.88909996,\n",
            "       0.8836    , 0.88339996, 0.87829995, 0.90329999, 0.9012    ,\n",
            "       0.90549999, 0.9059    , 0.90169996, 0.90399998, 0.8786    ,\n",
            "       0.89949995, 0.91159999, 0.9109    , 0.90599996, 0.90799999,\n",
            "       0.90739995, 0.90799999, 0.90719998, 0.90599996, 0.9077    ,\n",
            "       0.9095    , 0.90929997, 0.91239995, 0.91060001, 0.90929997,\n",
            "       0.90939999, 0.9077    , 0.91229999, 0.9077    , 0.90919995,\n",
            "       0.91329998, 0.90929997, 0.91339999, 0.91099995, 0.90929997,\n",
            "       0.91099995, 0.91329998, 0.90939999, 0.91029996, 0.91039997,\n",
            "       0.90959996, 0.91259998, 0.9131    , 0.90979999, 0.91249996,\n",
            "       0.91039997, 0.91109997, 0.9102    , 0.91099995, 0.91189998,\n",
            "       0.9102    , 0.9095    , 0.91399997, 0.91369998, 0.91259998,\n",
            "       0.91409999, 0.91389996, 0.91189998, 0.91039997, 0.9113    ,\n",
            "       0.9131    , 0.91179997, 0.91060001, 0.91179997, 0.91239995,\n",
            "       0.91060001, 0.91179997, 0.91249996, 0.90989995, 0.9127    ])\n",
            "Test Loss Set:  array([2.38101649, 2.12079334, 1.11320114, 2.01935101, 1.28585875,\n",
            "       1.72639275, 1.10419047, 1.07455182, 1.08552146, 0.67305392,\n",
            "       0.93668413, 0.74991602, 0.85994822, 0.85551012, 0.67778784,\n",
            "       0.66764951, 0.59594762, 1.08347821, 0.63060296, 0.60810035,\n",
            "       0.65692502, 0.83070624, 0.69839317, 0.7392531 , 0.56354284,\n",
            "       0.4546285 , 0.5059424 , 0.56227612, 0.48127472, 0.57329577,\n",
            "       0.53440589, 1.27739286, 0.47351685, 0.33890143, 0.40794551,\n",
            "       0.37220439, 0.42351413, 0.38053894, 0.40536484, 0.482297  ,\n",
            "       0.50183839, 0.33935869, 0.34211558, 0.35724568, 0.33201322,\n",
            "       0.39018527, 0.38851708, 0.36242512, 0.42196062, 0.36930948,\n",
            "       0.39829147, 0.42458341, 0.42044702, 0.34034133, 0.33551016,\n",
            "       0.35134944, 0.34641218, 0.36415759, 0.36082074, 0.47179443,\n",
            "       0.34267551, 0.33213431, 0.33686176, 0.34692481, 0.33935139,\n",
            "       0.34856793, 0.35638323, 0.34989411, 0.35645932, 0.34521192,\n",
            "       0.3639757 , 0.35345978, 0.3559278 , 0.35998875, 0.37299931,\n",
            "       0.36366838, 0.35822359, 0.36023167, 0.35914043, 0.35298786,\n",
            "       0.35777399, 0.35278058, 0.35706979, 0.35490134, 0.3599326 ,\n",
            "       0.35641596, 0.35768768, 0.35645041, 0.35665259, 0.35700625,\n",
            "       0.35901663, 0.35643762, 0.35707346, 0.35851136, 0.35803655,\n",
            "       0.35970584, 0.35975415, 0.3574332 , 0.35826299, 0.35891455,\n",
            "       0.35882199, 0.36028185, 0.36100969, 0.35644937, 0.35529009,\n",
            "       0.35846287, 0.3592627 , 0.35711148, 0.35545266, 0.35924554,\n",
            "       0.35711151, 0.35984382, 0.36143178, 0.35904789, 0.36267146,\n",
            "       0.36300373, 0.36386409, 0.36137807, 0.36226055, 0.36112747])\n",
            "Test Accuracy Set:  array([0.3125    , 0.38819999, 0.62919998, 0.48859999, 0.60609996,\n",
            "       0.56229997, 0.66209996, 0.66759998, 0.68369997, 0.77059996,\n",
            "       0.70949996, 0.74949998, 0.71520001, 0.71489996, 0.7687    ,\n",
            "       0.78310001, 0.80089998, 0.67659998, 0.79960001, 0.7974    ,\n",
            "       0.7881    , 0.74259996, 0.7701    , 0.75400001, 0.8082    ,\n",
            "       0.84609997, 0.83399999, 0.8168    , 0.84549999, 0.81949997,\n",
            "       0.84209996, 0.69839996, 0.85299999, 0.89139998, 0.87349999,\n",
            "       0.884     , 0.86659998, 0.88729995, 0.87629998, 0.85479999,\n",
            "       0.85529995, 0.89819998, 0.89679998, 0.89359999, 0.90209997,\n",
            "       0.88929999, 0.8915    , 0.8951    , 0.88330001, 0.89589995,\n",
            "       0.89279997, 0.88049996, 0.884     , 0.90619999, 0.90739995,\n",
            "       0.9084    , 0.91049999, 0.90399998, 0.90749997, 0.88349998,\n",
            "       0.90779996, 0.91429996, 0.91189998, 0.90969998, 0.91349995,\n",
            "       0.91009998, 0.90799999, 0.91099995, 0.90979999, 0.91289997,\n",
            "       0.91119999, 0.91060001, 0.91329998, 0.90869999, 0.90979999,\n",
            "       0.91209996, 0.91069996, 0.91179997, 0.91409999, 0.91419995,\n",
            "       0.91299999, 0.91329998, 0.9131    , 0.91319996, 0.91409999,\n",
            "       0.91339999, 0.91339999, 0.91339999, 0.91159999, 0.91369998,\n",
            "       0.91259998, 0.91369998, 0.9149    , 0.91429996, 0.91329998,\n",
            "       0.91419995, 0.91359997, 0.91359997, 0.91279995, 0.91399997,\n",
            "       0.91339999, 0.91209996, 0.91339999, 0.91289997, 0.91509998,\n",
            "       0.91399997, 0.91399997, 0.91469997, 0.9131    , 0.91319996,\n",
            "       0.91279995, 0.91299999, 0.91369998, 0.91469997, 0.91319996,\n",
            "       0.91299999, 0.91359997, 0.91389996, 0.91349995, 0.91289997])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZqsK5uKZLDNW"
      },
      "source": [
        "##ASGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L4gwW0NbDLXp",
        "colab": {}
      },
      "source": [
        "def decayed_stats_ASGD(lr, decay, epochs, change):\n",
        "    \n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_ASGD = np.zeros(epochs)\n",
        "  train_accuracy_ASGD = np.zeros(epochs)\n",
        "  valid_loss_ASGD = np.zeros(epochs)\n",
        "  valid_accuracy_ASGD = np.zeros(epochs)\n",
        "  test_accuracy_ASGD = np.zeros(epochs)\n",
        "  test_loss_ASGD = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  #Initilise validation set error as criteria at change point\n",
        "  error = 100\n",
        "  prev_error = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #Creates a deep copy of the parameter and gradient tensors and makes them shareable to enable re-use of Resnet modules multiple times\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = AccSGD(model.parameters(),  lr = lr, weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation error reduced by more than 0.2% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_error-error)/prev_error < 0.002) and epoch!=0:\n",
        "        \n",
        "        if lr/decay < 0.001:\n",
        "         lr = 0.001\n",
        "        else:\n",
        "            lr=lr/decay\n",
        "\n",
        "      else:\n",
        "          lr=lr\n",
        "\n",
        "      prev_error = error\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    test_metric = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_ASGD[epoch] = result[0]['loss']\n",
        "    train_accuracy_ASGD[epoch] = result[0]['acc']\n",
        "    valid_loss_ASGD[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_ASGD[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_ASGD[epoch] = test_metric['test_acc']\n",
        "    test_loss_ASGD[epoch] = test_metric['test_loss']\n",
        "\n",
        "    error = 1 - result[0]['val_acc']\n",
        "\n",
        "    print(epoch, lr, result)\n",
        "\n",
        "  return lr, train_loss_ASGD, train_accuracy_ASGD, valid_loss_ASGD, valid_accuracy_ASGD, test_accuracy_ASGD, test_loss_ASGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XedrNBL8x3N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54eca1ef-1b45-4b8b-9462-eba48cd66491"
      },
      "source": [
        " final_lr_ASGD, train_loss_ASGD, train_accuracy_ASGD, valid_loss_ASGD, valid_accuracy_ASGD, test_accuracy_ASGD, test_loss_ASGD = decayed_stats_ASGD(0.27, 2, 120, 4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.27 [{'running_loss': 1.366351842880249, 'running_acc': 0.4976562261581421, 'loss': 1.6082143783569336, 'acc': 0.39922499656677246, 'val_loss': 1.4198079109191895, 'val_acc': 0.48019999265670776, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4496428966522217, 'test_acc': 0.4851999878883362}]\n",
            "1 0.27 [{'running_loss': 1.0021413564682007, 'running_acc': 0.6368749737739563, 'loss': 1.154274821281433, 'acc': 0.5811749696731567, 'val_loss': 1.448433756828308, 'val_acc': 0.4966000020503998, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.480893850326538, 'test_acc': 0.49539998173713684}]\n",
            "2 0.27 [{'running_loss': 0.8454849720001221, 'running_acc': 0.69984370470047, 'loss': 0.8919152021408081, 'acc': 0.6808750033378601, 'val_loss': 1.159263014793396, 'val_acc': 0.6132999658584595, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.1134583950042725, 'test_acc': 0.6301000118255615}]\n",
            "3 0.27 [{'running_loss': 0.7061928510665894, 'running_acc': 0.7534374594688416, 'loss': 0.7365151643753052, 'acc': 0.7405499815940857, 'val_loss': 1.4406299591064453, 'val_acc': 0.5633999705314636, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.575953722000122, 'test_acc': 0.5557999610900879}]\n",
            "4 0.27 [{'running_loss': 0.6315942406654358, 'running_acc': 0.7787500023841858, 'loss': 0.6425415873527527, 'acc': 0.7764250040054321, 'val_loss': 0.7595189213752747, 'val_acc': 0.7393999695777893, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7278971076011658, 'test_acc': 0.755899965763092}]\n",
            "5 0.27 [{'running_loss': 0.5774360299110413, 'running_acc': 0.8017187118530273, 'loss': 0.5869538187980652, 'acc': 0.7955249547958374, 'val_loss': 0.8053403496742249, 'val_acc': 0.7269999980926514, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8252835869789124, 'test_acc': 0.724299967288971}]\n",
            "6 0.27 [{'running_loss': 0.5380777716636658, 'running_acc': 0.8112499713897705, 'loss': 0.5398129224777222, 'acc': 0.8133749961853027, 'val_loss': 0.7700886726379395, 'val_acc': 0.7428999543190002, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7692699432373047, 'test_acc': 0.7430999875068665}]\n",
            "7 0.27 [{'running_loss': 0.5307631492614746, 'running_acc': 0.8193749785423279, 'loss': 0.5163500905036926, 'acc': 0.8220999836921692, 'val_loss': 0.7033489942550659, 'val_acc': 0.762999951839447, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.6945512890815735, 'test_acc': 0.774399995803833}]\n",
            "8 0.27 [{'running_loss': 0.5268058776855469, 'running_acc': 0.8243749737739563, 'loss': 0.49240896105766296, 'acc': 0.8314499855041504, 'val_loss': 0.8914806842803955, 'val_acc': 0.7040999531745911, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8983945250511169, 'test_acc': 0.7143999934196472}]\n",
            "9 0.27 [{'running_loss': 0.48840609192848206, 'running_acc': 0.832812488079071, 'loss': 0.4770280420780182, 'acc': 0.8366249799728394, 'val_loss': 1.0768409967422485, 'val_acc': 0.6890999674797058, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.247747540473938, 'test_acc': 0.6609999537467957}]\n",
            "10 0.27 [{'running_loss': 0.4958815574645996, 'running_acc': 0.8293749690055847, 'loss': 0.4631292521953583, 'acc': 0.8421750068664551, 'val_loss': 0.6961407661437988, 'val_acc': 0.7683999538421631, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.677227258682251, 'test_acc': 0.7795000076293945}]\n",
            "11 0.27 [{'running_loss': 0.4399206042289734, 'running_acc': 0.8509374856948853, 'loss': 0.44990473985671997, 'acc': 0.8453249931335449, 'val_loss': 0.7510907053947449, 'val_acc': 0.7576000094413757, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7601318955421448, 'test_acc': 0.7587999701499939}]\n",
            "12 0.135 [{'running_loss': 0.4544771611690521, 'running_acc': 0.8389062285423279, 'loss': 0.4332960844039917, 'acc': 0.8506249785423279, 'val_loss': 0.9598408341407776, 'val_acc': 0.7003999948501587, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.0111100673675537, 'test_acc': 0.7051999568939209}]\n",
            "13 0.135 [{'running_loss': 0.3499586284160614, 'running_acc': 0.8818749785423279, 'loss': 0.3509184718132019, 'acc': 0.8794999718666077, 'val_loss': 0.4638715982437134, 'val_acc': 0.8402000069618225, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.47217339277267456, 'test_acc': 0.8392999768257141}]\n",
            "14 0.135 [{'running_loss': 0.34715044498443604, 'running_acc': 0.8860937356948853, 'loss': 0.3286237120628357, 'acc': 0.8885999917984009, 'val_loss': 0.49261268973350525, 'val_acc': 0.8279999494552612, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4851045310497284, 'test_acc': 0.8366000056266785}]\n",
            "15 0.135 [{'running_loss': 0.33218085765838623, 'running_acc': 0.8860937356948853, 'loss': 0.3234526515007019, 'acc': 0.8891749978065491, 'val_loss': 0.5189409255981445, 'val_acc': 0.8222999572753906, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5226535201072693, 'test_acc': 0.82669997215271}]\n",
            "16 0.135 [{'running_loss': 0.3410283327102661, 'running_acc': 0.8815624713897705, 'loss': 0.32188889384269714, 'acc': 0.887749969959259, 'val_loss': 0.5312867760658264, 'val_acc': 0.823699951171875, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5377060174942017, 'test_acc': 0.8271999955177307}]\n",
            "17 0.135 [{'running_loss': 0.3355984389781952, 'running_acc': 0.8853124976158142, 'loss': 0.31670093536376953, 'acc': 0.8908499479293823, 'val_loss': 0.47830554842948914, 'val_acc': 0.8348999619483948, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4692085087299347, 'test_acc': 0.8409000039100647}]\n",
            "18 0.135 [{'running_loss': 0.3331795036792755, 'running_acc': 0.8840624690055847, 'loss': 0.31035685539245605, 'acc': 0.8937749862670898, 'val_loss': 0.5281068682670593, 'val_acc': 0.8256999850273132, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.523408055305481, 'test_acc': 0.8303999900817871}]\n",
            "19 0.135 [{'running_loss': 0.3361736536026001, 'running_acc': 0.8843749761581421, 'loss': 0.3064839541912079, 'acc': 0.8948249816894531, 'val_loss': 0.5109646916389465, 'val_acc': 0.8276000022888184, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5320477485656738, 'test_acc': 0.8233999609947205}]\n",
            "20 0.135 [{'running_loss': 0.33508795499801636, 'running_acc': 0.8814062476158142, 'loss': 0.30995649099349976, 'acc': 0.8933249711990356, 'val_loss': 0.5719826221466064, 'val_acc': 0.8136000037193298, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5996291637420654, 'test_acc': 0.8125}]\n",
            "21 0.135 [{'running_loss': 0.32023733854293823, 'running_acc': 0.8898437023162842, 'loss': 0.3032361567020416, 'acc': 0.8976749777793884, 'val_loss': 0.586381196975708, 'val_acc': 0.8157999515533447, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5919583439826965, 'test_acc': 0.8141999840736389}]\n",
            "22 0.135 [{'running_loss': 0.32887980341911316, 'running_acc': 0.8864062428474426, 'loss': 0.29808545112609863, 'acc': 0.8984499573707581, 'val_loss': 0.5175817012786865, 'val_acc': 0.8288999795913696, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5553893446922302, 'test_acc': 0.8210999965667725}]\n",
            "23 0.135 [{'running_loss': 0.3166760802268982, 'running_acc': 0.8882812261581421, 'loss': 0.2886452376842499, 'acc': 0.8998499512672424, 'val_loss': 0.5045980215072632, 'val_acc': 0.8315999507904053, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5256479382514954, 'test_acc': 0.8277999758720398}]\n",
            "24 0.135 [{'running_loss': 0.3115748465061188, 'running_acc': 0.8909375071525574, 'loss': 0.29403170943260193, 'acc': 0.8986999988555908, 'val_loss': 0.6019018292427063, 'val_acc': 0.8012999892234802, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.6230214238166809, 'test_acc': 0.8029999732971191}]\n",
            "25 0.135 [{'running_loss': 0.31267252564430237, 'running_acc': 0.89124995470047, 'loss': 0.29094311594963074, 'acc': 0.8981999754905701, 'val_loss': 0.5261386036872864, 'val_acc': 0.8295999765396118, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.543447732925415, 'test_acc': 0.8259999752044678}]\n",
            "26 0.135 [{'running_loss': 0.2977180778980255, 'running_acc': 0.8979687094688416, 'loss': 0.28376466035842896, 'acc': 0.9027999639511108, 'val_loss': 0.7672665119171143, 'val_acc': 0.7687999606132507, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8643358945846558, 'test_acc': 0.7529000043869019}]\n",
            "27 0.135 [{'running_loss': 0.3066946268081665, 'running_acc': 0.8929687142372131, 'loss': 0.2806338369846344, 'acc': 0.9020749926567078, 'val_loss': 0.6470785737037659, 'val_acc': 0.7953000068664551, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.6800988912582397, 'test_acc': 0.7938999533653259}]\n",
            "28 0.0675 [{'running_loss': 0.2954058051109314, 'running_acc': 0.8968749642372131, 'loss': 0.27863022685050964, 'acc': 0.90482497215271, 'val_loss': 0.6830039024353027, 'val_acc': 0.7799999713897705, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7289692759513855, 'test_acc': 0.7664999961853027}]\n",
            "29 0.0675 [{'running_loss': 0.2168205827474594, 'running_acc': 0.9251562356948853, 'loss': 0.21512600779533386, 'acc': 0.9265499711036682, 'val_loss': 0.35423535108566284, 'val_acc': 0.8826999664306641, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3463067412376404, 'test_acc': 0.8863999843597412}]\n",
            "30 0.0675 [{'running_loss': 0.19326025247573853, 'running_acc': 0.9292187094688416, 'loss': 0.18763262033462524, 'acc': 0.934874951839447, 'val_loss': 0.371306836605072, 'val_acc': 0.8769999742507935, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39778807759284973, 'test_acc': 0.8761000037193298}]\n",
            "31 0.0675 [{'running_loss': 0.20421867072582245, 'running_acc': 0.9287499785423279, 'loss': 0.18402789533138275, 'acc': 0.9370249509811401, 'val_loss': 0.43484458327293396, 'val_acc': 0.8592000007629395, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4646139144897461, 'test_acc': 0.8585000038146973}]\n",
            "32 0.0675 [{'running_loss': 0.18307115137577057, 'running_acc': 0.9354687333106995, 'loss': 0.17870379984378815, 'acc': 0.9367499947547913, 'val_loss': 0.41291847825050354, 'val_acc': 0.8651999831199646, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42857155203819275, 'test_acc': 0.8658999800682068}]\n",
            "33 0.0675 [{'running_loss': 0.18983978033065796, 'running_acc': 0.9329687356948853, 'loss': 0.18029658496379852, 'acc': 0.9363749623298645, 'val_loss': 0.4193393588066101, 'val_acc': 0.8676999807357788, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40801215171813965, 'test_acc': 0.8726999759674072}]\n",
            "34 0.0675 [{'running_loss': 0.203327938914299, 'running_acc': 0.9276562333106995, 'loss': 0.1783747673034668, 'acc': 0.9375999569892883, 'val_loss': 0.3897993266582489, 'val_acc': 0.876800000667572, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39912593364715576, 'test_acc': 0.8775999546051025}]\n",
            "35 0.0675 [{'running_loss': 0.1889045089483261, 'running_acc': 0.9334374666213989, 'loss': 0.17855633795261383, 'acc': 0.9383499622344971, 'val_loss': 0.49749070405960083, 'val_acc': 0.8434999585151672, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5032053589820862, 'test_acc': 0.8511999845504761}]\n",
            "36 0.03375 [{'running_loss': 0.20240134000778198, 'running_acc': 0.9315624833106995, 'loss': 0.1801753044128418, 'acc': 0.9384499788284302, 'val_loss': 0.4237794280052185, 'val_acc': 0.8641999959945679, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40487587451934814, 'test_acc': 0.8694999814033508}]\n",
            "37 0.03375 [{'running_loss': 0.13196296989917755, 'running_acc': 0.9560937285423279, 'loss': 0.1312975138425827, 'acc': 0.9563249945640564, 'val_loss': 0.33936911821365356, 'val_acc': 0.8899999856948853, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3459033668041229, 'test_acc': 0.8955000042915344}]\n",
            "38 0.03375 [{'running_loss': 0.12228967994451523, 'running_acc': 0.9574999809265137, 'loss': 0.11169088631868362, 'acc': 0.9618499875068665, 'val_loss': 0.3256699740886688, 'val_acc': 0.8989999890327454, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33879363536834717, 'test_acc': 0.8983999490737915}]\n",
            "39 0.03375 [{'running_loss': 0.11137951910495758, 'running_acc': 0.9620312452316284, 'loss': 0.10716209560632706, 'acc': 0.9642499685287476, 'val_loss': 0.33780649304389954, 'val_acc': 0.8991000056266785, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3365284204483032, 'test_acc': 0.9003999829292297}]\n",
            "40 0.03375 [{'running_loss': 0.1130807027220726, 'running_acc': 0.9634374976158142, 'loss': 0.10249453783035278, 'acc': 0.9652999639511108, 'val_loss': 0.3783266544342041, 'val_acc': 0.8880999684333801, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3852356970310211, 'test_acc': 0.8898999691009521}]\n",
            "41 0.03375 [{'running_loss': 0.10350484400987625, 'running_acc': 0.9635937213897705, 'loss': 0.0980869010090828, 'acc': 0.9671749472618103, 'val_loss': 0.3722614347934723, 'val_acc': 0.88919997215271, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37826916575431824, 'test_acc': 0.8924999833106995}]\n",
            "42 0.03375 [{'running_loss': 0.11453189700841904, 'running_acc': 0.9603124856948853, 'loss': 0.09995165467262268, 'acc': 0.9665749669075012, 'val_loss': 0.3650814890861511, 'val_acc': 0.8866999745368958, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.378912091255188, 'test_acc': 0.8895999789237976}]\n",
            "43 0.03375 [{'running_loss': 0.11257222294807434, 'running_acc': 0.9599999785423279, 'loss': 0.10082995146512985, 'acc': 0.9650499820709229, 'val_loss': 0.36529019474983215, 'val_acc': 0.8901999592781067, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37720680236816406, 'test_acc': 0.8950999975204468}]\n",
            "44 0.016875 [{'running_loss': 0.10754083096981049, 'running_acc': 0.9629687070846558, 'loss': 0.0978216752409935, 'acc': 0.9661999940872192, 'val_loss': 0.36745262145996094, 'val_acc': 0.8928999900817871, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37962567806243896, 'test_acc': 0.8915999531745911}]\n",
            "45 0.016875 [{'running_loss': 0.07355842739343643, 'running_acc': 0.9759374856948853, 'loss': 0.07594840973615646, 'acc': 0.9753499627113342, 'val_loss': 0.3124791383743286, 'val_acc': 0.9044999480247498, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33198556303977966, 'test_acc': 0.9057999849319458}]\n",
            "46 0.016875 [{'running_loss': 0.06061164289712906, 'running_acc': 0.9803124666213989, 'loss': 0.05883841961622238, 'acc': 0.9810749888420105, 'val_loss': 0.32780778408050537, 'val_acc': 0.9047999978065491, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33652812242507935, 'test_acc': 0.90829998254776}]\n",
            "47 0.016875 [{'running_loss': 0.05768077075481415, 'running_acc': 0.9809374809265137, 'loss': 0.05379705876111984, 'acc': 0.9827999472618103, 'val_loss': 0.3401676416397095, 'val_acc': 0.9031999707221985, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3363257944583893, 'test_acc': 0.9079999923706055}]\n",
            "48 0.016875 [{'running_loss': 0.05916066840291023, 'running_acc': 0.9799999594688416, 'loss': 0.05268338322639465, 'acc': 0.983074963092804, 'val_loss': 0.3231799006462097, 'val_acc': 0.9077000021934509, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3329150378704071, 'test_acc': 0.9108999967575073}]\n",
            "49 0.016875 [{'running_loss': 0.05424065887928009, 'running_acc': 0.9828124642372131, 'loss': 0.051870182156562805, 'acc': 0.9826249480247498, 'val_loss': 0.34630244970321655, 'val_acc': 0.9034000039100647, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3272547721862793, 'test_acc': 0.9089999794960022}]\n",
            "50 0.016875 [{'running_loss': 0.05826347693800926, 'running_acc': 0.981249988079071, 'loss': 0.04847634211182594, 'acc': 0.9849749803543091, 'val_loss': 0.3354332745075226, 'val_acc': 0.905299961566925, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3372492492198944, 'test_acc': 0.9066999554634094}]\n",
            "51 0.016875 [{'running_loss': 0.04438029229640961, 'running_acc': 0.9864062070846558, 'loss': 0.04811835289001465, 'acc': 0.984749972820282, 'val_loss': 0.3466692268848419, 'val_acc': 0.9021999835968018, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3401961922645569, 'test_acc': 0.9075999855995178}]\n",
            "52 0.0084375 [{'running_loss': 0.05093821883201599, 'running_acc': 0.984375, 'loss': 0.04750502482056618, 'acc': 0.9846999645233154, 'val_loss': 0.3665419816970825, 'val_acc': 0.8959999680519104, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3554278016090393, 'test_acc': 0.9006999731063843}]\n",
            "53 0.0084375 [{'running_loss': 0.03608271852135658, 'running_acc': 0.9901562333106995, 'loss': 0.03628474473953247, 'acc': 0.9885749816894531, 'val_loss': 0.3272080719470978, 'val_acc': 0.9098999500274658, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.32785436511039734, 'test_acc': 0.9097999930381775}]\n",
            "54 0.0084375 [{'running_loss': 0.034811221063137054, 'running_acc': 0.9903124570846558, 'loss': 0.030207017436623573, 'acc': 0.9915249943733215, 'val_loss': 0.3286280632019043, 'val_acc': 0.9116999506950378, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33649492263793945, 'test_acc': 0.9111999869346619}]\n",
            "55 0.0084375 [{'running_loss': 0.028900180011987686, 'running_acc': 0.9918749928474426, 'loss': 0.02722971700131893, 'acc': 0.9924249649047852, 'val_loss': 0.3272720277309418, 'val_acc': 0.9097999930381775, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3290961980819702, 'test_acc': 0.914900004863739}]\n",
            "56 0.0084375 [{'running_loss': 0.02623053640127182, 'running_acc': 0.9920312166213989, 'loss': 0.026782507076859474, 'acc': 0.9922749996185303, 'val_loss': 0.3297306001186371, 'val_acc': 0.9099999666213989, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3393014073371887, 'test_acc': 0.91239994764328}]\n",
            "57 0.0084375 [{'running_loss': 0.02870222181081772, 'running_acc': 0.9921875, 'loss': 0.024956833571195602, 'acc': 0.9934249520301819, 'val_loss': 0.33631259202957153, 'val_acc': 0.9125999808311462, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3293289840221405, 'test_acc': 0.9134999513626099}]\n",
            "58 0.0084375 [{'running_loss': 0.028485869988799095, 'running_acc': 0.9917187094688416, 'loss': 0.0251713078469038, 'acc': 0.9924499988555908, 'val_loss': 0.34131717681884766, 'val_acc': 0.9067999720573425, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3285464942455292, 'test_acc': 0.9156999588012695}]\n",
            "59 0.0084375 [{'running_loss': 0.025780176743865013, 'running_acc': 0.9912499785423279, 'loss': 0.023247629404067993, 'acc': 0.9933499693870544, 'val_loss': 0.34904396533966064, 'val_acc': 0.9093999862670898, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34455835819244385, 'test_acc': 0.9129999876022339}]\n",
            "60 0.00421875 [{'running_loss': 0.024648793041706085, 'running_acc': 0.9926562309265137, 'loss': 0.022149059921503067, 'acc': 0.9933750033378601, 'val_loss': 0.3479446470737457, 'val_acc': 0.9106999635696411, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33836275339126587, 'test_acc': 0.913599967956543}]\n",
            "61 0.00421875 [{'running_loss': 0.017507554963231087, 'running_acc': 0.9945312142372131, 'loss': 0.018688498064875603, 'acc': 0.9946749806404114, 'val_loss': 0.32959234714508057, 'val_acc': 0.9170999526977539, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3331122398376465, 'test_acc': 0.9158999919891357}]\n",
            "62 0.00421875 [{'running_loss': 0.019412383437156677, 'running_acc': 0.9948437213897705, 'loss': 0.017498813569545746, 'acc': 0.9954749941825867, 'val_loss': 0.33200666308403015, 'val_acc': 0.9158999919891357, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33062270283699036, 'test_acc': 0.91839998960495}]\n",
            "63 0.00421875 [{'running_loss': 0.02083936147391796, 'running_acc': 0.9940624833106995, 'loss': 0.017219239845871925, 'acc': 0.995324969291687, 'val_loss': 0.33294788002967834, 'val_acc': 0.9152999520301819, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33806174993515015, 'test_acc': 0.9159999489784241}]\n",
            "64 0.00421875 [{'running_loss': 0.015428383834660053, 'running_acc': 0.9959374666213989, 'loss': 0.015495456755161285, 'acc': 0.9962499737739563, 'val_loss': 0.3511580526828766, 'val_acc': 0.9138000011444092, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3388471007347107, 'test_acc': 0.9164999723434448}]\n",
            "65 0.00421875 [{'running_loss': 0.018217340111732483, 'running_acc': 0.9950000047683716, 'loss': 0.015946529805660248, 'acc': 0.9957249760627747, 'val_loss': 0.35419762134552, 'val_acc': 0.9124999642372131, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3450342118740082, 'test_acc': 0.9150999784469604}]\n",
            "66 0.00421875 [{'running_loss': 0.016702622175216675, 'running_acc': 0.9948437213897705, 'loss': 0.0139137152582407, 'acc': 0.9963749647140503, 'val_loss': 0.34556296467781067, 'val_acc': 0.9163999557495117, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34387916326522827, 'test_acc': 0.916700005531311}]\n",
            "67 0.00421875 [{'running_loss': 0.01315805409103632, 'running_acc': 0.9965624809265137, 'loss': 0.01422601006925106, 'acc': 0.9961499571800232, 'val_loss': 0.3422459065914154, 'val_acc': 0.9138000011444092, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33602049946784973, 'test_acc': 0.9161999821662903}]\n",
            "68 0.002109375 [{'running_loss': 0.016787996515631676, 'running_acc': 0.9951562285423279, 'loss': 0.014839773997664452, 'acc': 0.9964749813079834, 'val_loss': 0.3619064688682556, 'val_acc': 0.9111999869346619, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34572723507881165, 'test_acc': 0.9151999950408936}]\n",
            "69 0.002109375 [{'running_loss': 0.012099516578018665, 'running_acc': 0.9971874952316284, 'loss': 0.013131151907145977, 'acc': 0.996749997138977, 'val_loss': 0.33593499660491943, 'val_acc': 0.9172999858856201, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3398279845714569, 'test_acc': 0.9158999919891357}]\n",
            "70 0.002109375 [{'running_loss': 0.01282190065830946, 'running_acc': 0.99609375, 'loss': 0.012224062345921993, 'acc': 0.9971500039100647, 'val_loss': 0.3400978147983551, 'val_acc': 0.9156999588012695, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3397422432899475, 'test_acc': 0.9174000024795532}]\n",
            "71 0.002109375 [{'running_loss': 0.012508108280599117, 'running_acc': 0.9978124499320984, 'loss': 0.011799892410635948, 'acc': 0.9975499510765076, 'val_loss': 0.35361987352371216, 'val_acc': 0.91239994764328, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3357859253883362, 'test_acc': 0.9168999791145325}]\n",
            "72 0.0010546875 [{'running_loss': 0.01099619921296835, 'running_acc': 0.9979687333106995, 'loss': 0.010872645303606987, 'acc': 0.9977999925613403, 'val_loss': 0.3522908389568329, 'val_acc': 0.9146999716758728, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34530550241470337, 'test_acc': 0.9157999753952026}]\n",
            "73 0.0010546875 [{'running_loss': 0.01269902940839529, 'running_acc': 0.99671870470047, 'loss': 0.010994418524205685, 'acc': 0.9977999925613403, 'val_loss': 0.34263673424720764, 'val_acc': 0.9150999784469604, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33949723839759827, 'test_acc': 0.9164999723434448}]\n",
            "74 0.0010546875 [{'running_loss': 0.010029398836195469, 'running_acc': 0.9982812404632568, 'loss': 0.010176283307373524, 'acc': 0.9979749917984009, 'val_loss': 0.34500977396965027, 'val_acc': 0.9168999791145325, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3443257510662079, 'test_acc': 0.916700005531311}]\n",
            "75 0.0010546875 [{'running_loss': 0.010227940045297146, 'running_acc': 0.9981249570846558, 'loss': 0.009948991239070892, 'acc': 0.9980250000953674, 'val_loss': 0.32565513253211975, 'val_acc': 0.9180999994277954, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33977508544921875, 'test_acc': 0.9185000061988831}]\n",
            "76 0.0010546875 [{'running_loss': 0.009662221185863018, 'running_acc': 0.9981249570846558, 'loss': 0.010042369365692139, 'acc': 0.9979499578475952, 'val_loss': 0.3389083445072174, 'val_acc': 0.9157999753952026, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3411156237125397, 'test_acc': 0.91839998960495}]\n",
            "77 0.0010546875 [{'running_loss': 0.010006330907344818, 'running_acc': 0.9978124499320984, 'loss': 0.010086746886372566, 'acc': 0.9978249669075012, 'val_loss': 0.34098222851753235, 'val_acc': 0.9170999526977539, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3427235782146454, 'test_acc': 0.9185000061988831}]\n",
            "78 0.0010546875 [{'running_loss': 0.008270446211099625, 'running_acc': 0.9987499713897705, 'loss': 0.009229730814695358, 'acc': 0.9983999729156494, 'val_loss': 0.3308148682117462, 'val_acc': 0.9157999753952026, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.33870288729667664, 'test_acc': 0.9180999994277954}]\n",
            "79 0.0010546875 [{'running_loss': 0.008903861977159977, 'running_acc': 0.9982812404632568, 'loss': 0.00944818090647459, 'acc': 0.9983999729156494, 'val_loss': 0.3450617492198944, 'val_acc': 0.9133999943733215, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3448614478111267, 'test_acc': 0.9175999760627747}]\n",
            "80 0.001 [{'running_loss': 0.009445225819945335, 'running_acc': 0.9976562261581421, 'loss': 0.01011776551604271, 'acc': 0.9978249669075012, 'val_loss': 0.3491414189338684, 'val_acc': 0.9138000011444092, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34315794706344604, 'test_acc': 0.9188999533653259}]\n",
            "81 0.001 [{'running_loss': 0.009189479053020477, 'running_acc': 0.9985937476158142, 'loss': 0.009235739707946777, 'acc': 0.9984249472618103, 'val_loss': 0.33858418464660645, 'val_acc': 0.9163999557495117, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3405109941959381, 'test_acc': 0.9188999533653259}]\n",
            "82 0.001 [{'running_loss': 0.009161786176264286, 'running_acc': 0.9982812404632568, 'loss': 0.008564996533095837, 'acc': 0.9984999895095825, 'val_loss': 0.33956465125083923, 'val_acc': 0.9180999994277954, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3470323085784912, 'test_acc': 0.9174999594688416}]\n",
            "83 0.001 [{'running_loss': 0.011195308528840542, 'running_acc': 0.9971874952316284, 'loss': 0.010207975283265114, 'acc': 0.9977749586105347, 'val_loss': 0.3414204716682434, 'val_acc': 0.91839998960495, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3425697088241577, 'test_acc': 0.9186999797821045}]\n",
            "84 0.001 [{'running_loss': 0.008086121641099453, 'running_acc': 0.9992187023162842, 'loss': 0.008433515205979347, 'acc': 0.9986749887466431, 'val_loss': 0.349505752325058, 'val_acc': 0.9156999588012695, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3440074324607849, 'test_acc': 0.918999969959259}]\n",
            "85 0.001 [{'running_loss': 0.009159169159829617, 'running_acc': 0.9976562261581421, 'loss': 0.009073357097804546, 'acc': 0.998324990272522, 'val_loss': 0.3353078067302704, 'val_acc': 0.9162999987602234, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3431894779205322, 'test_acc': 0.9188999533653259}]\n",
            "86 0.001 [{'running_loss': 0.011305115185678005, 'running_acc': 0.9978124499320984, 'loss': 0.009370602667331696, 'acc': 0.998199999332428, 'val_loss': 0.347953736782074, 'val_acc': 0.9168999791145325, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3409041166305542, 'test_acc': 0.9190999865531921}]\n",
            "87 0.001 [{'running_loss': 0.008833725936710835, 'running_acc': 0.9989062547683716, 'loss': 0.008809786289930344, 'acc': 0.9986249804496765, 'val_loss': 0.3621358275413513, 'val_acc': 0.9143999814987183, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34167760610580444, 'test_acc': 0.9185000061988831}]\n",
            "88 0.001 [{'running_loss': 0.00984183419495821, 'running_acc': 0.9975000023841858, 'loss': 0.008686431683599949, 'acc': 0.9984999895095825, 'val_loss': 0.3363053500652313, 'val_acc': 0.9187999963760376, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3464151918888092, 'test_acc': 0.9187999963760376}]\n",
            "89 0.001 [{'running_loss': 0.009558374062180519, 'running_acc': 0.9979687333106995, 'loss': 0.00881386362016201, 'acc': 0.9984249472618103, 'val_loss': 0.34414413571357727, 'val_acc': 0.9167999625205994, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34581458568573, 'test_acc': 0.9175999760627747}]\n",
            "90 0.001 [{'running_loss': 0.007900997996330261, 'running_acc': 0.9990624785423279, 'loss': 0.0089824628084898, 'acc': 0.9981749653816223, 'val_loss': 0.3431471884250641, 'val_acc': 0.914199948310852, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34607404470443726, 'test_acc': 0.9185000061988831}]\n",
            "91 0.001 [{'running_loss': 0.009455012157559395, 'running_acc': 0.9979687333106995, 'loss': 0.009031057357788086, 'acc': 0.9982249736785889, 'val_loss': 0.3509526252746582, 'val_acc': 0.9161999821662903, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34529972076416016, 'test_acc': 0.9190999865531921}]\n",
            "92 0.001 [{'running_loss': 0.008334371261298656, 'running_acc': 0.9987499713897705, 'loss': 0.00945593323558569, 'acc': 0.9981499910354614, 'val_loss': 0.35004115104675293, 'val_acc': 0.9145999550819397, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3489168584346771, 'test_acc': 0.9161999821662903}]\n",
            "93 0.001 [{'running_loss': 0.007913640700280666, 'running_acc': 0.9989062547683716, 'loss': 0.008428660221397877, 'acc': 0.9986499547958374, 'val_loss': 0.33479759097099304, 'val_acc': 0.9172999858856201, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.349911093711853, 'test_acc': 0.917199969291687}]\n",
            "94 0.001 [{'running_loss': 0.009979764930903912, 'running_acc': 0.9989062547683716, 'loss': 0.009043289348483086, 'acc': 0.9985249638557434, 'val_loss': 0.3523341715335846, 'val_acc': 0.9165999889373779, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35179731249809265, 'test_acc': 0.9182999730110168}]\n",
            "95 0.001 [{'running_loss': 0.00915651023387909, 'running_acc': 0.9982812404632568, 'loss': 0.008768182247877121, 'acc': 0.9982499480247498, 'val_loss': 0.354505330324173, 'val_acc': 0.9156999588012695, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.348178505897522, 'test_acc': 0.9182999730110168}]\n",
            "96 0.001 [{'running_loss': 0.008216913789510727, 'running_acc': 0.9985937476158142, 'loss': 0.00892716646194458, 'acc': 0.998324990272522, 'val_loss': 0.36294373869895935, 'val_acc': 0.9131999611854553, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3460007309913635, 'test_acc': 0.9178999662399292}]\n",
            "97 0.001 [{'running_loss': 0.00844133272767067, 'running_acc': 0.9992187023162842, 'loss': 0.008282829076051712, 'acc': 0.9985999464988708, 'val_loss': 0.3469734489917755, 'val_acc': 0.9172999858856201, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34505096077919006, 'test_acc': 0.9179999828338623}]\n",
            "98 0.001 [{'running_loss': 0.007981106638908386, 'running_acc': 0.9984374642372131, 'loss': 0.00820393767207861, 'acc': 0.9983749985694885, 'val_loss': 0.3492133915424347, 'val_acc': 0.9142999649047852, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34702154994010925, 'test_acc': 0.9169999957084656}]\n",
            "99 0.001 [{'running_loss': 0.008325796574354172, 'running_acc': 0.9987499713897705, 'loss': 0.00804231408983469, 'acc': 0.9985999464988708, 'val_loss': 0.35662928223609924, 'val_acc': 0.9143999814987183, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3490854799747467, 'test_acc': 0.9174000024795532}]\n",
            "100 0.001 [{'running_loss': 0.00875743106007576, 'running_acc': 0.9979687333106995, 'loss': 0.009086533449590206, 'acc': 0.9982249736785889, 'val_loss': 0.34393706917762756, 'val_acc': 0.916700005531311, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3498081862926483, 'test_acc': 0.9160999655723572}]\n",
            "101 0.001 [{'running_loss': 0.008179687894880772, 'running_acc': 0.9985937476158142, 'loss': 0.008176090195775032, 'acc': 0.9986749887466431, 'val_loss': 0.3540579676628113, 'val_acc': 0.9116999506950378, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35128331184387207, 'test_acc': 0.9162999987602234}]\n",
            "102 0.001 [{'running_loss': 0.01020828913897276, 'running_acc': 0.9979687333106995, 'loss': 0.00849569495767355, 'acc': 0.9984749555587769, 'val_loss': 0.3435719907283783, 'val_acc': 0.9162999987602234, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3509517312049866, 'test_acc': 0.916700005531311}]\n",
            "103 0.001 [{'running_loss': 0.008441954851150513, 'running_acc': 0.9979687333106995, 'loss': 0.008987016044557095, 'acc': 0.9980999827384949, 'val_loss': 0.3540179133415222, 'val_acc': 0.9178999662399292, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35147765278816223, 'test_acc': 0.9194999933242798}]\n",
            "104 0.001 [{'running_loss': 0.008292554877698421, 'running_acc': 0.9989062547683716, 'loss': 0.008332399651408195, 'acc': 0.9985249638557434, 'val_loss': 0.3506571352481842, 'val_acc': 0.914900004863739, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3462301790714264, 'test_acc': 0.9172999858856201}]\n",
            "105 0.001 [{'running_loss': 0.008866802789270878, 'running_acc': 0.9979687333106995, 'loss': 0.008320003747940063, 'acc': 0.99857497215271, 'val_loss': 0.35931214690208435, 'val_acc': 0.9175999760627747, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34877264499664307, 'test_acc': 0.9164999723434448}]\n",
            "106 0.001 [{'running_loss': 0.008265376091003418, 'running_acc': 0.9993749856948853, 'loss': 0.008023213595151901, 'acc': 0.9987249970436096, 'val_loss': 0.365022748708725, 'val_acc': 0.9138000011444092, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3501153886318207, 'test_acc': 0.9161999821662903}]\n",
            "107 0.001 [{'running_loss': 0.00762149365618825, 'running_acc': 0.9989062547683716, 'loss': 0.008328263647854328, 'acc': 0.9984749555587769, 'val_loss': 0.3537862300872803, 'val_acc': 0.9110999703407288, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34804201126098633, 'test_acc': 0.9174999594688416}]\n",
            "108 0.001 [{'running_loss': 0.00835733488202095, 'running_acc': 0.9982812404632568, 'loss': 0.00792628712952137, 'acc': 0.9987499713897705, 'val_loss': 0.346902459859848, 'val_acc': 0.914199948310852, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3480547368526459, 'test_acc': 0.9181999564170837}]\n",
            "109 0.001 [{'running_loss': 0.007397576235234737, 'running_acc': 0.9990624785423279, 'loss': 0.008556346409022808, 'acc': 0.9982999563217163, 'val_loss': 0.3666436970233917, 'val_acc': 0.9138999581336975, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3482155501842499, 'test_acc': 0.91839998960495}]\n",
            "110 0.001 [{'running_loss': 0.0075453296303749084, 'running_acc': 0.9989062547683716, 'loss': 0.007405107840895653, 'acc': 0.9988999962806702, 'val_loss': 0.34927451610565186, 'val_acc': 0.9154999852180481, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3476260304450989, 'test_acc': 0.9182999730110168}]\n",
            "111 0.001 [{'running_loss': 0.007854401133954525, 'running_acc': 0.9985937476158142, 'loss': 0.007944336161017418, 'acc': 0.9984749555587769, 'val_loss': 0.3601510524749756, 'val_acc': 0.9154999852180481, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34978434443473816, 'test_acc': 0.9165999889373779}]\n",
            "112 0.001 [{'running_loss': 0.0063012647442519665, 'running_acc': 0.9990624785423279, 'loss': 0.007969504222273827, 'acc': 0.9984999895095825, 'val_loss': 0.35268157720565796, 'val_acc': 0.9161999821662903, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35289254784584045, 'test_acc': 0.9172999858856201}]\n",
            "113 0.001 [{'running_loss': 0.007518311496824026, 'running_acc': 0.9982812404632568, 'loss': 0.00795793067663908, 'acc': 0.99857497215271, 'val_loss': 0.3444820046424866, 'val_acc': 0.9164999723434448, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.34758612513542175, 'test_acc': 0.9188999533653259}]\n",
            "114 0.001 [{'running_loss': 0.007326844148337841, 'running_acc': 0.9989062547683716, 'loss': 0.007613868452608585, 'acc': 0.9988999962806702, 'val_loss': 0.35828739404678345, 'val_acc': 0.9159999489784241, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3490857481956482, 'test_acc': 0.9175999760627747}]\n",
            "115 0.001 [{'running_loss': 0.009008189663290977, 'running_acc': 0.9976562261581421, 'loss': 0.008180544711649418, 'acc': 0.9982499480247498, 'val_loss': 0.3571980893611908, 'val_acc': 0.9177999496459961, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35031020641326904, 'test_acc': 0.9180999994277954}]\n",
            "116 0.001 [{'running_loss': 0.007192305289208889, 'running_acc': 0.9987499713897705, 'loss': 0.00791969895362854, 'acc': 0.9982999563217163, 'val_loss': 0.35354486107826233, 'val_acc': 0.9134999513626099, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3526015877723694, 'test_acc': 0.917199969291687}]\n",
            "117 0.001 [{'running_loss': 0.00845335703343153, 'running_acc': 0.9984374642372131, 'loss': 0.0077118598856031895, 'acc': 0.998824954032898, 'val_loss': 0.3473028242588043, 'val_acc': 0.9140999913215637, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.354510635137558, 'test_acc': 0.9179999828338623}]\n",
            "118 0.001 [{'running_loss': 0.00670405151322484, 'running_acc': 0.9989062547683716, 'loss': 0.007385777775198221, 'acc': 0.9987249970436096, 'val_loss': 0.37157273292541504, 'val_acc': 0.9159999489784241, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3536417782306671, 'test_acc': 0.9168999791145325}]\n",
            "119 0.001 [{'running_loss': 0.00855337455868721, 'running_acc': 0.9984374642372131, 'loss': 0.007514012977480888, 'acc': 0.9988499879837036, 'val_loss': 0.3580165505409241, 'val_acc': 0.9122999906539917, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3532074987888336, 'test_acc': 0.91839998960495}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FKrbU2XLfcM"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG4AH6Q5v-7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for ASGD on Batch Size 128 and Decayed Hyperparameter Schedule\n",
        "\n",
        "# final_lr_ASGD = 0.001\n",
        "# train_loss_ASGD = np.array([1.60821438, 1.15427482, 0.8919152 , 0.73651516, 0.64254159,\n",
        "#        0.58695382, 0.53981292, 0.51635009, 0.49240896, 0.47702804,\n",
        "#        0.46312925, 0.44990474, 0.43329608, 0.35091847, 0.32862371,\n",
        "#        0.32345265, 0.32188889, 0.31670094, 0.31035686, 0.30648395,\n",
        "#        0.30995649, 0.30323616, 0.29808545, 0.28864524, 0.29403171,\n",
        "#        0.29094312, 0.28376466, 0.28063384, 0.27863023, 0.21512601,\n",
        "#        0.18763262, 0.1840279 , 0.1787038 , 0.18029658, 0.17837477,\n",
        "#        0.17855634, 0.1801753 , 0.13129751, 0.11169089, 0.1071621 ,\n",
        "#        0.10249454, 0.0980869 , 0.09995165, 0.10082995, 0.09782168,\n",
        "#        0.07594841, 0.05883842, 0.05379706, 0.05268338, 0.05187018,\n",
        "#        0.04847634, 0.04811835, 0.04750502, 0.03628474, 0.03020702,\n",
        "#        0.02722972, 0.02678251, 0.02495683, 0.02517131, 0.02324763,\n",
        "#        0.02214906, 0.0186885 , 0.01749881, 0.01721924, 0.01549546,\n",
        "#        0.01594653, 0.01391372, 0.01422601, 0.01483977, 0.01313115,\n",
        "#        0.01222406, 0.01179989, 0.01087265, 0.01099442, 0.01017628,\n",
        "#        0.00994899, 0.01004237, 0.01008675, 0.00922973, 0.00944818,\n",
        "#        0.01011777, 0.00923574, 0.008565  , 0.01020798, 0.00843352,\n",
        "#        0.00907336, 0.0093706 , 0.00880979, 0.00868643, 0.00881386,\n",
        "#        0.00898246, 0.00903106, 0.00945593, 0.00842866, 0.00904329,\n",
        "#        0.00876818, 0.00892717, 0.00828283, 0.00820394, 0.00804231,\n",
        "#        0.00908653, 0.00817609, 0.00849569, 0.00898702, 0.0083324 ,\n",
        "#        0.00832   , 0.00802321, 0.00832826, 0.00792629, 0.00855635,\n",
        "#        0.00740511, 0.00794434, 0.0079695 , 0.00795793, 0.00761387,\n",
        "#        0.00818054, 0.0079197 , 0.00771186, 0.00738578, 0.00751401])\n",
        "# train_accuracy_ASGD = np.array([0.399225  , 0.58117497, 0.680875  , 0.74054998, 0.776425  ,\n",
        "#        0.79552495, 0.813375  , 0.82209998, 0.83144999, 0.83662498,\n",
        "#        0.84217501, 0.84532499, 0.85062498, 0.87949997, 0.88859999,\n",
        "#        0.889175  , 0.88774997, 0.89084995, 0.89377499, 0.89482498,\n",
        "#        0.89332497, 0.89767498, 0.89844996, 0.89984995, 0.8987    ,\n",
        "#        0.89819998, 0.90279996, 0.90207499, 0.90482497, 0.92654997,\n",
        "#        0.93487495, 0.93702495, 0.93674999, 0.93637496, 0.93759996,\n",
        "#        0.93834996, 0.93844998, 0.95632499, 0.96184999, 0.96424997,\n",
        "#        0.96529996, 0.96717495, 0.96657497, 0.96504998, 0.96619999,\n",
        "#        0.97534996, 0.98107499, 0.98279995, 0.98307496, 0.98262495,\n",
        "#        0.98497498, 0.98474997, 0.98469996, 0.98857498, 0.99152499,\n",
        "#        0.99242496, 0.992275  , 0.99342495, 0.99245   , 0.99334997,\n",
        "#        0.993375  , 0.99467498, 0.99547499, 0.99532497, 0.99624997,\n",
        "#        0.99572498, 0.99637496, 0.99614996, 0.99647498, 0.99675   ,\n",
        "#        0.99715   , 0.99754995, 0.99779999, 0.99779999, 0.99797499,\n",
        "#        0.998025  , 0.99794996, 0.99782497, 0.99839997, 0.99839997,\n",
        "#        0.99782497, 0.99842495, 0.99849999, 0.99777496, 0.99867499,\n",
        "#        0.99832499, 0.9982    , 0.99862498, 0.99849999, 0.99842495,\n",
        "#        0.99817497, 0.99822497, 0.99814999, 0.99864995, 0.99852496,\n",
        "#        0.99824995, 0.99832499, 0.99859995, 0.998375  , 0.99859995,\n",
        "#        0.99822497, 0.99867499, 0.99847496, 0.99809998, 0.99852496,\n",
        "#        0.99857497, 0.998725  , 0.99847496, 0.99874997, 0.99829996,\n",
        "#        0.9989    , 0.99847496, 0.99849999, 0.99857497, 0.9989    ,\n",
        "#        0.99824995, 0.99829996, 0.99882495, 0.998725  , 0.99884999])\n",
        "# valid_loss_ASGD = np.array([1.41980791, 1.44843376, 1.15926301, 1.44062996, 0.75951892,\n",
        "#        0.80534035, 0.77008867, 0.70334899, 0.89148068, 1.076841  ,\n",
        "#        0.69614077, 0.75109071, 0.95984083, 0.4638716 , 0.49261269,\n",
        "#        0.51894093, 0.53128678, 0.47830555, 0.52810687, 0.51096469,\n",
        "#        0.57198262, 0.5863812 , 0.5175817 , 0.50459802, 0.60190183,\n",
        "#        0.5261386 , 0.76726651, 0.64707857, 0.6830039 , 0.35423535,\n",
        "#        0.37130684, 0.43484458, 0.41291848, 0.41933936, 0.38979933,\n",
        "#        0.4974907 , 0.42377943, 0.33936912, 0.32566997, 0.33780649,\n",
        "#        0.37832665, 0.37226143, 0.36508149, 0.36529019, 0.36745262,\n",
        "#        0.31247914, 0.32780778, 0.34016764, 0.3231799 , 0.34630245,\n",
        "#        0.33543327, 0.34666923, 0.36654198, 0.32720807, 0.32862806,\n",
        "#        0.32727203, 0.3297306 , 0.33631259, 0.34131718, 0.34904397,\n",
        "#        0.34794465, 0.32959235, 0.33200666, 0.33294788, 0.35115805,\n",
        "#        0.35419762, 0.34556296, 0.34224591, 0.36190647, 0.335935  ,\n",
        "#        0.34009781, 0.35361987, 0.35229084, 0.34263673, 0.34500977,\n",
        "#        0.32565513, 0.33890834, 0.34098223, 0.33081487, 0.34506175,\n",
        "#        0.34914142, 0.33858418, 0.33956465, 0.34142047, 0.34950575,\n",
        "#        0.33530781, 0.34795374, 0.36213583, 0.33630535, 0.34414414,\n",
        "#        0.34314719, 0.35095263, 0.35004115, 0.33479759, 0.35233417,\n",
        "#        0.35450533, 0.36294374, 0.34697345, 0.34921339, 0.35662928,\n",
        "#        0.34393707, 0.35405797, 0.34357199, 0.35401791, 0.35065714,\n",
        "#        0.35931215, 0.36502275, 0.35378623, 0.34690246, 0.3666437 ,\n",
        "#        0.34927452, 0.36015105, 0.35268158, 0.344482  , 0.35828739,\n",
        "#        0.35719809, 0.35354486, 0.34730282, 0.37157273, 0.35801655])\n",
        "# valid_accuracy_ASGD = np.array([0.48019999, 0.4966    , 0.61329997, 0.56339997, 0.73939997,\n",
        "#        0.727     , 0.74289995, 0.76299995, 0.70409995, 0.68909997,\n",
        "#        0.76839995, 0.75760001, 0.70039999, 0.84020001, 0.82799995,\n",
        "#        0.82229996, 0.82369995, 0.83489996, 0.82569999, 0.8276    ,\n",
        "#        0.8136    , 0.81579995, 0.82889998, 0.83159995, 0.80129999,\n",
        "#        0.82959998, 0.76879996, 0.79530001, 0.77999997, 0.88269997,\n",
        "#        0.87699997, 0.8592    , 0.86519998, 0.86769998, 0.8768    ,\n",
        "#        0.84349996, 0.8642    , 0.88999999, 0.89899999, 0.89910001,\n",
        "#        0.88809997, 0.88919997, 0.88669997, 0.89019996, 0.89289999,\n",
        "#        0.90449995, 0.9048    , 0.90319997, 0.9077    , 0.9034    ,\n",
        "#        0.90529996, 0.90219998, 0.89599997, 0.90989995, 0.91169995,\n",
        "#        0.90979999, 0.90999997, 0.91259998, 0.90679997, 0.90939999,\n",
        "#        0.91069996, 0.91709995, 0.91589999, 0.91529995, 0.9138    ,\n",
        "#        0.91249996, 0.91639996, 0.9138    , 0.91119999, 0.91729999,\n",
        "#        0.91569996, 0.91239995, 0.91469997, 0.91509998, 0.91689998,\n",
        "#        0.9181    , 0.91579998, 0.91709995, 0.91579998, 0.91339999,\n",
        "#        0.9138    , 0.91639996, 0.9181    , 0.91839999, 0.91569996,\n",
        "#        0.9163    , 0.91689998, 0.91439998, 0.9188    , 0.91679996,\n",
        "#        0.91419995, 0.91619998, 0.91459996, 0.91729999, 0.91659999,\n",
        "#        0.91569996, 0.91319996, 0.91729999, 0.91429996, 0.91439998,\n",
        "#        0.91670001, 0.91169995, 0.9163    , 0.91789997, 0.9149    ,\n",
        "#        0.91759998, 0.9138    , 0.91109997, 0.91419995, 0.91389996,\n",
        "#        0.91549999, 0.91549999, 0.91619998, 0.91649997, 0.91599995,\n",
        "#        0.91779995, 0.91349995, 0.91409999, 0.91599995, 0.91229999])\n",
        "# test_loss_ASGD = np.array([1.4496429 , 1.48089385, 1.1134584 , 1.57595372, 0.72789711,\n",
        "#        0.82528359, 0.76926994, 0.69455129, 0.89839453, 1.24774754,\n",
        "#        0.67722726, 0.7601319 , 1.01111007, 0.47217339, 0.48510453,\n",
        "#        0.52265352, 0.53770602, 0.46920851, 0.52340806, 0.53204775,\n",
        "#        0.59962916, 0.59195834, 0.55538934, 0.52564794, 0.62302142,\n",
        "#        0.54344773, 0.86433589, 0.68009889, 0.72896928, 0.34630674,\n",
        "#        0.39778808, 0.46461391, 0.42857155, 0.40801215, 0.39912593,\n",
        "#        0.50320536, 0.40487587, 0.34590337, 0.33879364, 0.33652842,\n",
        "#        0.3852357 , 0.37826917, 0.37891209, 0.3772068 , 0.37962568,\n",
        "#        0.33198556, 0.33652812, 0.33632579, 0.33291504, 0.32725477,\n",
        "#        0.33724925, 0.34019619, 0.3554278 , 0.32785437, 0.33649492,\n",
        "#        0.3290962 , 0.33930141, 0.32932898, 0.32854649, 0.34455836,\n",
        "#        0.33836275, 0.33311224, 0.3306227 , 0.33806175, 0.3388471 ,\n",
        "#        0.34503421, 0.34387916, 0.3360205 , 0.34572724, 0.33982798,\n",
        "#        0.33974224, 0.33578593, 0.3453055 , 0.33949724, 0.34432575,\n",
        "#        0.33977509, 0.34111562, 0.34272358, 0.33870289, 0.34486145,\n",
        "#        0.34315795, 0.34051099, 0.34703231, 0.34256971, 0.34400743,\n",
        "#        0.34318948, 0.34090412, 0.34167761, 0.34641519, 0.34581459,\n",
        "#        0.34607404, 0.34529972, 0.34891686, 0.34991109, 0.35179731,\n",
        "#        0.34817851, 0.34600073, 0.34505096, 0.34702155, 0.34908548,\n",
        "#        0.34980819, 0.35128331, 0.35095173, 0.35147765, 0.34623018,\n",
        "#        0.34877264, 0.35011539, 0.34804201, 0.34805474, 0.34821555,\n",
        "#        0.34762603, 0.34978434, 0.35289255, 0.34758613, 0.34908575,\n",
        "#        0.35031021, 0.35260159, 0.35451064, 0.35364178, 0.3532075 ])\n",
        "# test_accuracy_ASGD = np.array([0.48519999, 0.49539998, 0.63010001, 0.55579996, 0.75589997,\n",
        "#        0.72429997, 0.74309999, 0.7744    , 0.71439999, 0.66099995,\n",
        "#        0.77950001, 0.75879997, 0.70519996, 0.83929998, 0.83660001,\n",
        "#        0.82669997, 0.8272    , 0.8409    , 0.83039999, 0.82339996,\n",
        "#        0.8125    , 0.81419998, 0.8211    , 0.82779998, 0.80299997,\n",
        "#        0.82599998, 0.7529    , 0.79389995, 0.7665    , 0.88639998,\n",
        "#        0.8761    , 0.8585    , 0.86589998, 0.87269998, 0.87759995,\n",
        "#        0.85119998, 0.86949998, 0.8955    , 0.89839995, 0.90039998,\n",
        "#        0.88989997, 0.89249998, 0.88959998, 0.8951    , 0.89159995,\n",
        "#        0.90579998, 0.90829998, 0.90799999, 0.9109    , 0.90899998,\n",
        "#        0.90669996, 0.90759999, 0.90069997, 0.90979999, 0.91119999,\n",
        "#        0.9149    , 0.91239995, 0.91349995, 0.91569996, 0.91299999,\n",
        "#        0.91359997, 0.91589999, 0.91839999, 0.91599995, 0.91649997,\n",
        "#        0.91509998, 0.91670001, 0.91619998, 0.9152    , 0.91589999,\n",
        "#        0.9174    , 0.91689998, 0.91579998, 0.91649997, 0.91670001,\n",
        "#        0.91850001, 0.91839999, 0.91850001, 0.9181    , 0.91759998,\n",
        "#        0.91889995, 0.91889995, 0.91749996, 0.91869998, 0.91899997,\n",
        "#        0.91889995, 0.91909999, 0.91850001, 0.9188    , 0.91759998,\n",
        "#        0.91850001, 0.91909999, 0.91619998, 0.91719997, 0.91829997,\n",
        "#        0.91829997, 0.91789997, 0.91799998, 0.917     , 0.9174    ,\n",
        "#        0.91609997, 0.9163    , 0.91670001, 0.91949999, 0.91729999,\n",
        "#        0.91649997, 0.91619998, 0.91749996, 0.91819996, 0.91839999,\n",
        "#        0.91829997, 0.91659999, 0.91729999, 0.91889995, 0.91759998,\n",
        "#        0.9181    , 0.91719997, 0.91799998, 0.91689998, 0.91839999])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xHUZxmDgbREn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53011b82-6209-4f14-9ee5-332f864c84cc"
      },
      "source": [
        "print(\"Final Learning Rate reached at the end of training: \", final_lr_ASGD)\n",
        "print(\"Training Loss Set: \", repr(train_loss_ASGD))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_ASGD))\n",
        "print(\"Validation Loss Set: \",repr(valid_loss_ASGD))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_ASGD))\n",
        "print(\"Test Loss Set: \",repr(test_loss_ASGD))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_ASGD))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Learning Rate reached at the end of training:  0.001\n",
            "Training Loss Set:  array([1.60821438, 1.15427482, 0.8919152 , 0.73651516, 0.64254159,\n",
            "       0.58695382, 0.53981292, 0.51635009, 0.49240896, 0.47702804,\n",
            "       0.46312925, 0.44990474, 0.43329608, 0.35091847, 0.32862371,\n",
            "       0.32345265, 0.32188889, 0.31670094, 0.31035686, 0.30648395,\n",
            "       0.30995649, 0.30323616, 0.29808545, 0.28864524, 0.29403171,\n",
            "       0.29094312, 0.28376466, 0.28063384, 0.27863023, 0.21512601,\n",
            "       0.18763262, 0.1840279 , 0.1787038 , 0.18029658, 0.17837477,\n",
            "       0.17855634, 0.1801753 , 0.13129751, 0.11169089, 0.1071621 ,\n",
            "       0.10249454, 0.0980869 , 0.09995165, 0.10082995, 0.09782168,\n",
            "       0.07594841, 0.05883842, 0.05379706, 0.05268338, 0.05187018,\n",
            "       0.04847634, 0.04811835, 0.04750502, 0.03628474, 0.03020702,\n",
            "       0.02722972, 0.02678251, 0.02495683, 0.02517131, 0.02324763,\n",
            "       0.02214906, 0.0186885 , 0.01749881, 0.01721924, 0.01549546,\n",
            "       0.01594653, 0.01391372, 0.01422601, 0.01483977, 0.01313115,\n",
            "       0.01222406, 0.01179989, 0.01087265, 0.01099442, 0.01017628,\n",
            "       0.00994899, 0.01004237, 0.01008675, 0.00922973, 0.00944818,\n",
            "       0.01011777, 0.00923574, 0.008565  , 0.01020798, 0.00843352,\n",
            "       0.00907336, 0.0093706 , 0.00880979, 0.00868643, 0.00881386,\n",
            "       0.00898246, 0.00903106, 0.00945593, 0.00842866, 0.00904329,\n",
            "       0.00876818, 0.00892717, 0.00828283, 0.00820394, 0.00804231,\n",
            "       0.00908653, 0.00817609, 0.00849569, 0.00898702, 0.0083324 ,\n",
            "       0.00832   , 0.00802321, 0.00832826, 0.00792629, 0.00855635,\n",
            "       0.00740511, 0.00794434, 0.0079695 , 0.00795793, 0.00761387,\n",
            "       0.00818054, 0.0079197 , 0.00771186, 0.00738578, 0.00751401])\n",
            "Training Accuracy Set:  array([0.399225  , 0.58117497, 0.680875  , 0.74054998, 0.776425  ,\n",
            "       0.79552495, 0.813375  , 0.82209998, 0.83144999, 0.83662498,\n",
            "       0.84217501, 0.84532499, 0.85062498, 0.87949997, 0.88859999,\n",
            "       0.889175  , 0.88774997, 0.89084995, 0.89377499, 0.89482498,\n",
            "       0.89332497, 0.89767498, 0.89844996, 0.89984995, 0.8987    ,\n",
            "       0.89819998, 0.90279996, 0.90207499, 0.90482497, 0.92654997,\n",
            "       0.93487495, 0.93702495, 0.93674999, 0.93637496, 0.93759996,\n",
            "       0.93834996, 0.93844998, 0.95632499, 0.96184999, 0.96424997,\n",
            "       0.96529996, 0.96717495, 0.96657497, 0.96504998, 0.96619999,\n",
            "       0.97534996, 0.98107499, 0.98279995, 0.98307496, 0.98262495,\n",
            "       0.98497498, 0.98474997, 0.98469996, 0.98857498, 0.99152499,\n",
            "       0.99242496, 0.992275  , 0.99342495, 0.99245   , 0.99334997,\n",
            "       0.993375  , 0.99467498, 0.99547499, 0.99532497, 0.99624997,\n",
            "       0.99572498, 0.99637496, 0.99614996, 0.99647498, 0.99675   ,\n",
            "       0.99715   , 0.99754995, 0.99779999, 0.99779999, 0.99797499,\n",
            "       0.998025  , 0.99794996, 0.99782497, 0.99839997, 0.99839997,\n",
            "       0.99782497, 0.99842495, 0.99849999, 0.99777496, 0.99867499,\n",
            "       0.99832499, 0.9982    , 0.99862498, 0.99849999, 0.99842495,\n",
            "       0.99817497, 0.99822497, 0.99814999, 0.99864995, 0.99852496,\n",
            "       0.99824995, 0.99832499, 0.99859995, 0.998375  , 0.99859995,\n",
            "       0.99822497, 0.99867499, 0.99847496, 0.99809998, 0.99852496,\n",
            "       0.99857497, 0.998725  , 0.99847496, 0.99874997, 0.99829996,\n",
            "       0.9989    , 0.99847496, 0.99849999, 0.99857497, 0.9989    ,\n",
            "       0.99824995, 0.99829996, 0.99882495, 0.998725  , 0.99884999])\n",
            "Validation Loss Set:  array([1.41980791, 1.44843376, 1.15926301, 1.44062996, 0.75951892,\n",
            "       0.80534035, 0.77008867, 0.70334899, 0.89148068, 1.076841  ,\n",
            "       0.69614077, 0.75109071, 0.95984083, 0.4638716 , 0.49261269,\n",
            "       0.51894093, 0.53128678, 0.47830555, 0.52810687, 0.51096469,\n",
            "       0.57198262, 0.5863812 , 0.5175817 , 0.50459802, 0.60190183,\n",
            "       0.5261386 , 0.76726651, 0.64707857, 0.6830039 , 0.35423535,\n",
            "       0.37130684, 0.43484458, 0.41291848, 0.41933936, 0.38979933,\n",
            "       0.4974907 , 0.42377943, 0.33936912, 0.32566997, 0.33780649,\n",
            "       0.37832665, 0.37226143, 0.36508149, 0.36529019, 0.36745262,\n",
            "       0.31247914, 0.32780778, 0.34016764, 0.3231799 , 0.34630245,\n",
            "       0.33543327, 0.34666923, 0.36654198, 0.32720807, 0.32862806,\n",
            "       0.32727203, 0.3297306 , 0.33631259, 0.34131718, 0.34904397,\n",
            "       0.34794465, 0.32959235, 0.33200666, 0.33294788, 0.35115805,\n",
            "       0.35419762, 0.34556296, 0.34224591, 0.36190647, 0.335935  ,\n",
            "       0.34009781, 0.35361987, 0.35229084, 0.34263673, 0.34500977,\n",
            "       0.32565513, 0.33890834, 0.34098223, 0.33081487, 0.34506175,\n",
            "       0.34914142, 0.33858418, 0.33956465, 0.34142047, 0.34950575,\n",
            "       0.33530781, 0.34795374, 0.36213583, 0.33630535, 0.34414414,\n",
            "       0.34314719, 0.35095263, 0.35004115, 0.33479759, 0.35233417,\n",
            "       0.35450533, 0.36294374, 0.34697345, 0.34921339, 0.35662928,\n",
            "       0.34393707, 0.35405797, 0.34357199, 0.35401791, 0.35065714,\n",
            "       0.35931215, 0.36502275, 0.35378623, 0.34690246, 0.3666437 ,\n",
            "       0.34927452, 0.36015105, 0.35268158, 0.344482  , 0.35828739,\n",
            "       0.35719809, 0.35354486, 0.34730282, 0.37157273, 0.35801655])\n",
            "Validation Accuracy Set:  array([0.48019999, 0.4966    , 0.61329997, 0.56339997, 0.73939997,\n",
            "       0.727     , 0.74289995, 0.76299995, 0.70409995, 0.68909997,\n",
            "       0.76839995, 0.75760001, 0.70039999, 0.84020001, 0.82799995,\n",
            "       0.82229996, 0.82369995, 0.83489996, 0.82569999, 0.8276    ,\n",
            "       0.8136    , 0.81579995, 0.82889998, 0.83159995, 0.80129999,\n",
            "       0.82959998, 0.76879996, 0.79530001, 0.77999997, 0.88269997,\n",
            "       0.87699997, 0.8592    , 0.86519998, 0.86769998, 0.8768    ,\n",
            "       0.84349996, 0.8642    , 0.88999999, 0.89899999, 0.89910001,\n",
            "       0.88809997, 0.88919997, 0.88669997, 0.89019996, 0.89289999,\n",
            "       0.90449995, 0.9048    , 0.90319997, 0.9077    , 0.9034    ,\n",
            "       0.90529996, 0.90219998, 0.89599997, 0.90989995, 0.91169995,\n",
            "       0.90979999, 0.90999997, 0.91259998, 0.90679997, 0.90939999,\n",
            "       0.91069996, 0.91709995, 0.91589999, 0.91529995, 0.9138    ,\n",
            "       0.91249996, 0.91639996, 0.9138    , 0.91119999, 0.91729999,\n",
            "       0.91569996, 0.91239995, 0.91469997, 0.91509998, 0.91689998,\n",
            "       0.9181    , 0.91579998, 0.91709995, 0.91579998, 0.91339999,\n",
            "       0.9138    , 0.91639996, 0.9181    , 0.91839999, 0.91569996,\n",
            "       0.9163    , 0.91689998, 0.91439998, 0.9188    , 0.91679996,\n",
            "       0.91419995, 0.91619998, 0.91459996, 0.91729999, 0.91659999,\n",
            "       0.91569996, 0.91319996, 0.91729999, 0.91429996, 0.91439998,\n",
            "       0.91670001, 0.91169995, 0.9163    , 0.91789997, 0.9149    ,\n",
            "       0.91759998, 0.9138    , 0.91109997, 0.91419995, 0.91389996,\n",
            "       0.91549999, 0.91549999, 0.91619998, 0.91649997, 0.91599995,\n",
            "       0.91779995, 0.91349995, 0.91409999, 0.91599995, 0.91229999])\n",
            "Test Loss Set:  array([1.4496429 , 1.48089385, 1.1134584 , 1.57595372, 0.72789711,\n",
            "       0.82528359, 0.76926994, 0.69455129, 0.89839453, 1.24774754,\n",
            "       0.67722726, 0.7601319 , 1.01111007, 0.47217339, 0.48510453,\n",
            "       0.52265352, 0.53770602, 0.46920851, 0.52340806, 0.53204775,\n",
            "       0.59962916, 0.59195834, 0.55538934, 0.52564794, 0.62302142,\n",
            "       0.54344773, 0.86433589, 0.68009889, 0.72896928, 0.34630674,\n",
            "       0.39778808, 0.46461391, 0.42857155, 0.40801215, 0.39912593,\n",
            "       0.50320536, 0.40487587, 0.34590337, 0.33879364, 0.33652842,\n",
            "       0.3852357 , 0.37826917, 0.37891209, 0.3772068 , 0.37962568,\n",
            "       0.33198556, 0.33652812, 0.33632579, 0.33291504, 0.32725477,\n",
            "       0.33724925, 0.34019619, 0.3554278 , 0.32785437, 0.33649492,\n",
            "       0.3290962 , 0.33930141, 0.32932898, 0.32854649, 0.34455836,\n",
            "       0.33836275, 0.33311224, 0.3306227 , 0.33806175, 0.3388471 ,\n",
            "       0.34503421, 0.34387916, 0.3360205 , 0.34572724, 0.33982798,\n",
            "       0.33974224, 0.33578593, 0.3453055 , 0.33949724, 0.34432575,\n",
            "       0.33977509, 0.34111562, 0.34272358, 0.33870289, 0.34486145,\n",
            "       0.34315795, 0.34051099, 0.34703231, 0.34256971, 0.34400743,\n",
            "       0.34318948, 0.34090412, 0.34167761, 0.34641519, 0.34581459,\n",
            "       0.34607404, 0.34529972, 0.34891686, 0.34991109, 0.35179731,\n",
            "       0.34817851, 0.34600073, 0.34505096, 0.34702155, 0.34908548,\n",
            "       0.34980819, 0.35128331, 0.35095173, 0.35147765, 0.34623018,\n",
            "       0.34877264, 0.35011539, 0.34804201, 0.34805474, 0.34821555,\n",
            "       0.34762603, 0.34978434, 0.35289255, 0.34758613, 0.34908575,\n",
            "       0.35031021, 0.35260159, 0.35451064, 0.35364178, 0.3532075 ])\n",
            "Test Accuracy Set:  array([0.48519999, 0.49539998, 0.63010001, 0.55579996, 0.75589997,\n",
            "       0.72429997, 0.74309999, 0.7744    , 0.71439999, 0.66099995,\n",
            "       0.77950001, 0.75879997, 0.70519996, 0.83929998, 0.83660001,\n",
            "       0.82669997, 0.8272    , 0.8409    , 0.83039999, 0.82339996,\n",
            "       0.8125    , 0.81419998, 0.8211    , 0.82779998, 0.80299997,\n",
            "       0.82599998, 0.7529    , 0.79389995, 0.7665    , 0.88639998,\n",
            "       0.8761    , 0.8585    , 0.86589998, 0.87269998, 0.87759995,\n",
            "       0.85119998, 0.86949998, 0.8955    , 0.89839995, 0.90039998,\n",
            "       0.88989997, 0.89249998, 0.88959998, 0.8951    , 0.89159995,\n",
            "       0.90579998, 0.90829998, 0.90799999, 0.9109    , 0.90899998,\n",
            "       0.90669996, 0.90759999, 0.90069997, 0.90979999, 0.91119999,\n",
            "       0.9149    , 0.91239995, 0.91349995, 0.91569996, 0.91299999,\n",
            "       0.91359997, 0.91589999, 0.91839999, 0.91599995, 0.91649997,\n",
            "       0.91509998, 0.91670001, 0.91619998, 0.9152    , 0.91589999,\n",
            "       0.9174    , 0.91689998, 0.91579998, 0.91649997, 0.91670001,\n",
            "       0.91850001, 0.91839999, 0.91850001, 0.9181    , 0.91759998,\n",
            "       0.91889995, 0.91889995, 0.91749996, 0.91869998, 0.91899997,\n",
            "       0.91889995, 0.91909999, 0.91850001, 0.9188    , 0.91759998,\n",
            "       0.91850001, 0.91909999, 0.91619998, 0.91719997, 0.91829997,\n",
            "       0.91829997, 0.91789997, 0.91799998, 0.917     , 0.9174    ,\n",
            "       0.91609997, 0.9163    , 0.91670001, 0.91949999, 0.91729999,\n",
            "       0.91649997, 0.91619998, 0.91749996, 0.91819996, 0.91839999,\n",
            "       0.91829997, 0.91659999, 0.91729999, 0.91889995, 0.91759998,\n",
            "       0.9181    , 0.91719997, 0.91799998, 0.91689998, 0.91839999])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hkYXgeCWMn9f"
      },
      "source": [
        "##Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yxI1LltsoKcb",
        "colab": {}
      },
      "source": [
        "def decayed_stats_Adam(lr, decay, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_Adam = np.zeros(epochs)\n",
        "  train_accuracy_Adam = np.zeros(epochs)\n",
        "  valid_loss_Adam = np.zeros(epochs)\n",
        "  valid_accuracy_Adam = np.zeros(epochs)\n",
        "  test_accuracy_Adam = np.zeros(epochs)\n",
        "  test_loss_Adam = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  #Initilise validation set error as criteria at change point\n",
        "  error = 100\n",
        "  prev_error = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #Creates a deep copy of the parameter and gradient tensors and makes them shareable to enable re-use of Resnet modules multiple times\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = AccSGD(model.parameters(),  lr = lr, weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation error reduced by more than 0.2% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_error-error)/prev_error < 0.002) and epoch!=0:\n",
        "        \n",
        "        if lr/decay < 0.001:\n",
        "         lr = 0.001\n",
        "        else:\n",
        "            lr=lr/decay\n",
        "\n",
        "      else:\n",
        "          lr=lr\n",
        "\n",
        "      prev_error = error\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    test_metric = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_Adam[epoch] = result[0]['loss']\n",
        "    train_accuracy_Adam[epoch] = result[0]['acc']\n",
        "    valid_loss_Adam[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_Adam[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_Adam[epoch] = test_metric['test_acc']\n",
        "    test_loss_Adam[epoch] = test_metric['test_loss']\n",
        "\n",
        "    error = 1 - result[0]['val_acc']\n",
        "\n",
        "    print(epoch, lr, result)\n",
        "\n",
        "  return lr, train_loss_Adam, train_accuracy_Adam, valid_loss_Adam, valid_accuracy_Adam, test_accuracy_Adam, test_loss_Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7BZ8PBioezY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ac950a6-1a82-4839-b3e7-d888c0b0dcc8"
      },
      "source": [
        " final_lr_Adam, train_loss_Adam, train_accuracy_Adam, valid_loss_Adam, valid_accuracy_Adam, test_accuracy_Adam, test_loss_Adam = decayed_stats_Adam(0.27, 2, 120, 4)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.27 [{'running_loss': 1.3554655313491821, 'running_acc': 0.5068749785423279, 'loss': 1.6190502643585205, 'acc': 0.39739999175071716, 'val_loss': 1.5539230108261108, 'val_acc': 0.4513999819755554, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.5358980894088745, 'test_acc': 0.4739999771118164}]\n",
            "1 0.27 [{'running_loss': 1.0061399936676025, 'running_acc': 0.6420312523841858, 'loss': 1.1510717868804932, 'acc': 0.5834499597549438, 'val_loss': 1.2889336347579956, 'val_acc': 0.545799970626831, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.303991436958313, 'test_acc': 0.5493000149726868}]\n",
            "2 0.27 [{'running_loss': 0.7960304617881775, 'running_acc': 0.7231249809265137, 'loss': 0.8651641607284546, 'acc': 0.6962499618530273, 'val_loss': 1.0492682456970215, 'val_acc': 0.6269999742507935, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.0699517726898193, 'test_acc': 0.6196999549865723}]\n",
            "3 0.27 [{'running_loss': 0.6466895341873169, 'running_acc': 0.7760937213897705, 'loss': 0.711225152015686, 'acc': 0.7530499696731567, 'val_loss': 1.2826194763183594, 'val_acc': 0.5726999640464783, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.274521827697754, 'test_acc': 0.5874999761581421}]\n",
            "4 0.135 [{'running_loss': 0.6278682947158813, 'running_acc': 0.784375011920929, 'loss': 0.6310640573501587, 'acc': 0.7819499969482422, 'val_loss': 0.8529772162437439, 'val_acc': 0.7134000062942505, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.8612457513809204, 'test_acc': 0.7246999740600586}]\n",
            "5 0.135 [{'running_loss': 0.48916593194007874, 'running_acc': 0.83203125, 'loss': 0.5187546610832214, 'acc': 0.821774959564209, 'val_loss': 0.7329500317573547, 'val_acc': 0.7577999830245972, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7794488072395325, 'test_acc': 0.7561999559402466}]\n",
            "6 0.135 [{'running_loss': 0.47901126742362976, 'running_acc': 0.8374999761581421, 'loss': 0.47887587547302246, 'acc': 0.8356499671936035, 'val_loss': 0.7854514122009277, 'val_acc': 0.7350999712944031, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7930175065994263, 'test_acc': 0.7441999912261963}]\n",
            "7 0.135 [{'running_loss': 0.4762568175792694, 'running_acc': 0.8393749594688416, 'loss': 0.45407840609550476, 'acc': 0.8450499773025513, 'val_loss': 0.7576523423194885, 'val_acc': 0.7376999855041504, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.7436138391494751, 'test_acc': 0.7422999739646912}]\n",
            "8 0.0675 [{'running_loss': 0.45246732234954834, 'running_acc': 0.8478124737739563, 'loss': 0.43620407581329346, 'acc': 0.8503749966621399, 'val_loss': 0.6108688116073608, 'val_acc': 0.7915999889373779, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.6249763369560242, 'test_acc': 0.7930999994277954}]\n",
            "9 0.0675 [{'running_loss': 0.3654317855834961, 'running_acc': 0.8721874952316284, 'loss': 0.37062254548072815, 'acc': 0.8719499707221985, 'val_loss': 0.5573315620422363, 'val_acc': 0.814799964427948, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5664737820625305, 'test_acc': 0.8201999664306641}]\n",
            "10 0.0675 [{'running_loss': 0.3553244173526764, 'running_acc': 0.8809374570846558, 'loss': 0.3416039049625397, 'acc': 0.882474958896637, 'val_loss': 0.6174048185348511, 'val_acc': 0.802299976348877, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5903434753417969, 'test_acc': 0.8091999888420105}]\n",
            "11 0.0675 [{'running_loss': 0.35290229320526123, 'running_acc': 0.8754687309265137, 'loss': 0.32895755767822266, 'acc': 0.8851499557495117, 'val_loss': 0.5283233523368835, 'val_acc': 0.8192999958992004, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5259368419647217, 'test_acc': 0.8242999911308289}]\n",
            "12 0.03375 [{'running_loss': 0.3263029158115387, 'running_acc': 0.88671875, 'loss': 0.3238322138786316, 'acc': 0.8877750039100647, 'val_loss': 0.5580747723579407, 'val_acc': 0.8079999685287476, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.5520215630531311, 'test_acc': 0.8144999742507935}]\n",
            "13 0.03375 [{'running_loss': 0.28626447916030884, 'running_acc': 0.9032812118530273, 'loss': 0.2727443277835846, 'acc': 0.9069499969482422, 'val_loss': 0.4169236719608307, 'val_acc': 0.8592000007629395, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4254440367221832, 'test_acc': 0.8589999675750732}]\n",
            "14 0.03375 [{'running_loss': 0.2504660189151764, 'running_acc': 0.9132812023162842, 'loss': 0.2521072030067444, 'acc': 0.9132750034332275, 'val_loss': 0.41629424691200256, 'val_acc': 0.8586999773979187, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4176134765148163, 'test_acc': 0.8616999983787537}]\n",
            "15 0.03375 [{'running_loss': 0.24644888937473297, 'running_acc': 0.9164062142372131, 'loss': 0.24119094014167786, 'acc': 0.9160999655723572, 'val_loss': 0.40682491660118103, 'val_acc': 0.8614999651908875, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42152589559555054, 'test_acc': 0.8643999695777893}]\n",
            "16 0.016875 [{'running_loss': 0.24626918137073517, 'running_acc': 0.9167187213897705, 'loss': 0.23603253066539764, 'acc': 0.9187999963760376, 'val_loss': 0.46943002939224243, 'val_acc': 0.8513999581336975, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.46239230036735535, 'test_acc': 0.8554999828338623}]\n",
            "17 0.016875 [{'running_loss': 0.2077508568763733, 'running_acc': 0.9292187094688416, 'loss': 0.2035960853099823, 'acc': 0.929349958896637, 'val_loss': 0.3785433769226074, 'val_acc': 0.8769999742507935, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37883076071739197, 'test_acc': 0.8810999989509583}]\n",
            "18 0.016875 [{'running_loss': 0.19157133996486664, 'running_acc': 0.9364062547683716, 'loss': 0.18458512425422668, 'acc': 0.9375249743461609, 'val_loss': 0.39344269037246704, 'val_acc': 0.8761000037193298, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3899729549884796, 'test_acc': 0.8764999508857727}]\n",
            "19 0.016875 [{'running_loss': 0.17728061974048615, 'running_acc': 0.9375, 'loss': 0.17716895043849945, 'acc': 0.9395249485969543, 'val_loss': 0.3915542960166931, 'val_acc': 0.8739999532699585, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3850277066230774, 'test_acc': 0.8781999945640564}]\n",
            "20 0.0084375 [{'running_loss': 0.17916423082351685, 'running_acc': 0.9367187023162842, 'loss': 0.17138773202896118, 'acc': 0.9415249824523926, 'val_loss': 0.38545238971710205, 'val_acc': 0.8766999840736389, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3824847638607025, 'test_acc': 0.8822000026702881}]\n",
            "21 0.0084375 [{'running_loss': 0.15697786211967468, 'running_acc': 0.9496874809265137, 'loss': 0.15143507719039917, 'acc': 0.948449969291687, 'val_loss': 0.3543776869773865, 'val_acc': 0.8867999911308289, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35590019822120667, 'test_acc': 0.8898999691009521}]\n",
            "22 0.0084375 [{'running_loss': 0.14280486106872559, 'running_acc': 0.9509374499320984, 'loss': 0.13903997838497162, 'acc': 0.9524999856948853, 'val_loss': 0.36412313580513, 'val_acc': 0.8829999566078186, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3593185245990753, 'test_acc': 0.8870999813079834}]\n",
            "23 0.0084375 [{'running_loss': 0.13683366775512695, 'running_acc': 0.9579687118530273, 'loss': 0.13427816331386566, 'acc': 0.9551999568939209, 'val_loss': 0.34943655133247375, 'val_acc': 0.8907999992370605, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3561691641807556, 'test_acc': 0.8883999586105347}]\n",
            "24 0.00421875 [{'running_loss': 0.14338546991348267, 'running_acc': 0.9517187476158142, 'loss': 0.13107948005199432, 'acc': 0.9562999606132507, 'val_loss': 0.375366747379303, 'val_acc': 0.8848999738693237, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36803099513053894, 'test_acc': 0.8905999660491943}]\n",
            "25 0.00421875 [{'running_loss': 0.11789672821760178, 'running_acc': 0.9628124833106995, 'loss': 0.11877504736185074, 'acc': 0.9601500034332275, 'val_loss': 0.36326736211776733, 'val_acc': 0.8858999609947205, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35877934098243713, 'test_acc': 0.8923999667167664}]\n",
            "26 0.00421875 [{'running_loss': 0.11053106933832169, 'running_acc': 0.961718738079071, 'loss': 0.10861528664827347, 'acc': 0.9646499752998352, 'val_loss': 0.3451460897922516, 'val_acc': 0.8904999494552612, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36096900701522827, 'test_acc': 0.8939999938011169}]\n",
            "27 0.00421875 [{'running_loss': 0.10214964300394058, 'running_acc': 0.9662500023841858, 'loss': 0.10561910271644592, 'acc': 0.9645999670028687, 'val_loss': 0.3543495833873749, 'val_acc': 0.8946999907493591, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3715057671070099, 'test_acc': 0.8910999894142151}]\n",
            "28 0.002109375 [{'running_loss': 0.11011350154876709, 'running_acc': 0.9631249904632568, 'loss': 0.1036997064948082, 'acc': 0.9649499654769897, 'val_loss': 0.3594285547733307, 'val_acc': 0.8913999795913696, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3673858344554901, 'test_acc': 0.8940999507904053}]\n",
            "29 0.002109375 [{'running_loss': 0.1031850054860115, 'running_acc': 0.9649999737739563, 'loss': 0.0968317836523056, 'acc': 0.9681999683380127, 'val_loss': 0.3622659742832184, 'val_acc': 0.8883999586105347, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.35983794927597046, 'test_acc': 0.8946999907493591}]\n",
            "30 0.002109375 [{'running_loss': 0.08917870372533798, 'running_acc': 0.9704686999320984, 'loss': 0.09455131739377975, 'acc': 0.96875, 'val_loss': 0.3682903051376343, 'val_acc': 0.8871999979019165, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36526575684547424, 'test_acc': 0.8924999833106995}]\n",
            "31 0.002109375 [{'running_loss': 0.10110197961330414, 'running_acc': 0.9667187333106995, 'loss': 0.09369935840368271, 'acc': 0.9686749577522278, 'val_loss': 0.3616388738155365, 'val_acc': 0.8915999531745911, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36302125453948975, 'test_acc': 0.8928999900817871}]\n",
            "32 0.0010546875 [{'running_loss': 0.10374227911233902, 'running_acc': 0.9667187333106995, 'loss': 0.08912616223096848, 'acc': 0.9715499877929688, 'val_loss': 0.3643457889556885, 'val_acc': 0.8904999494552612, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3702192008495331, 'test_acc': 0.8937000036239624}]\n",
            "33 0.0010546875 [{'running_loss': 0.08517591655254364, 'running_acc': 0.97328120470047, 'loss': 0.08739349246025085, 'acc': 0.9717249870300293, 'val_loss': 0.35157299041748047, 'val_acc': 0.892799973487854, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36773258447647095, 'test_acc': 0.8931999802589417}]\n",
            "34 0.0010546875 [{'running_loss': 0.08979693055152893, 'running_acc': 0.9701562523841858, 'loss': 0.08560703694820404, 'acc': 0.972000002861023, 'val_loss': 0.3682556450366974, 'val_acc': 0.8913999795913696, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36594000458717346, 'test_acc': 0.8926999568939209}]\n",
            "35 0.0010546875 [{'running_loss': 0.09265605360269547, 'running_acc': 0.9692187309265137, 'loss': 0.08330412954092026, 'acc': 0.9731749892234802, 'val_loss': 0.3538689911365509, 'val_acc': 0.8951999545097351, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3694249093532562, 'test_acc': 0.8932999968528748}]\n",
            "36 0.001 [{'running_loss': 0.08822312206029892, 'running_acc': 0.9724999666213989, 'loss': 0.08511404693126678, 'acc': 0.9727999567985535, 'val_loss': 0.3618534803390503, 'val_acc': 0.8904999494552612, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3681085705757141, 'test_acc': 0.8938999772071838}]\n",
            "37 0.001 [{'running_loss': 0.08789442479610443, 'running_acc': 0.9712499976158142, 'loss': 0.08248338848352432, 'acc': 0.9731499552726746, 'val_loss': 0.3601987659931183, 'val_acc': 0.8908999562263489, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3665708303451538, 'test_acc': 0.8940999507904053}]\n",
            "38 0.001 [{'running_loss': 0.08625344187021255, 'running_acc': 0.973437488079071, 'loss': 0.08098318427801132, 'acc': 0.9739999771118164, 'val_loss': 0.3674616813659668, 'val_acc': 0.8866999745368958, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.36996573209762573, 'test_acc': 0.8937999606132507}]\n",
            "39 0.001 [{'running_loss': 0.08663038164377213, 'running_acc': 0.9712499976158142, 'loss': 0.08186008036136627, 'acc': 0.9731999635696411, 'val_loss': 0.35350725054740906, 'val_acc': 0.8912999629974365, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3725348711013794, 'test_acc': 0.8915999531745911}]\n",
            "40 0.001 [{'running_loss': 0.07432697713375092, 'running_acc': 0.9757812023162842, 'loss': 0.07894035428762436, 'acc': 0.9739499688148499, 'val_loss': 0.3651352524757385, 'val_acc': 0.8905999660491943, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3679094910621643, 'test_acc': 0.8933999538421631}]\n",
            "41 0.001 [{'running_loss': 0.07668096572160721, 'running_acc': 0.9739062190055847, 'loss': 0.08064407855272293, 'acc': 0.9740249514579773, 'val_loss': 0.37300339341163635, 'val_acc': 0.8906999826431274, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3662291169166565, 'test_acc': 0.8948999643325806}]\n",
            "42 0.001 [{'running_loss': 0.086742103099823, 'running_acc': 0.9707812070846558, 'loss': 0.07812033593654633, 'acc': 0.9753499627113342, 'val_loss': 0.3604978919029236, 'val_acc': 0.8910999894142151, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3721042275428772, 'test_acc': 0.894599974155426}]\n",
            "43 0.001 [{'running_loss': 0.07916481792926788, 'running_acc': 0.9745312333106995, 'loss': 0.07830063253641129, 'acc': 0.9740749597549438, 'val_loss': 0.36391782760620117, 'val_acc': 0.8919000029563904, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3663095235824585, 'test_acc': 0.8941999673843384}]\n",
            "44 0.001 [{'running_loss': 0.08358117938041687, 'running_acc': 0.97265625, 'loss': 0.07841987907886505, 'acc': 0.9738499522209167, 'val_loss': 0.36536720395088196, 'val_acc': 0.8919000029563904, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3735367953777313, 'test_acc': 0.8926999568939209}]\n",
            "45 0.001 [{'running_loss': 0.07041069120168686, 'running_acc': 0.9770312309265137, 'loss': 0.07702131569385529, 'acc': 0.9751499891281128, 'val_loss': 0.3600893020629883, 'val_acc': 0.8950999975204468, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3766980767250061, 'test_acc': 0.8922999501228333}]\n",
            "46 0.001 [{'running_loss': 0.08366698771715164, 'running_acc': 0.9718749523162842, 'loss': 0.0767635852098465, 'acc': 0.9757750034332275, 'val_loss': 0.36417028307914734, 'val_acc': 0.8912000060081482, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37265101075172424, 'test_acc': 0.8937000036239624}]\n",
            "47 0.001 [{'running_loss': 0.07789161056280136, 'running_acc': 0.9779687523841858, 'loss': 0.07391440868377686, 'acc': 0.976699948310852, 'val_loss': 0.3779512345790863, 'val_acc': 0.8876000046730042, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3765139877796173, 'test_acc': 0.8928999900817871}]\n",
            "48 0.001 [{'running_loss': 0.0692925751209259, 'running_acc': 0.9764062166213989, 'loss': 0.07263258099555969, 'acc': 0.9763249754905701, 'val_loss': 0.35970863699913025, 'val_acc': 0.8912999629974365, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3748372495174408, 'test_acc': 0.8923999667167664}]\n",
            "49 0.001 [{'running_loss': 0.07766576111316681, 'running_acc': 0.9740625023841858, 'loss': 0.07424017041921616, 'acc': 0.9758749604225159, 'val_loss': 0.35891193151474, 'val_acc': 0.8951999545097351, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3782949447631836, 'test_acc': 0.892799973487854}]\n",
            "50 0.001 [{'running_loss': 0.07783475518226624, 'running_acc': 0.9749999642372131, 'loss': 0.07528774440288544, 'acc': 0.9753499627113342, 'val_loss': 0.3794679641723633, 'val_acc': 0.8889999985694885, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3802827000617981, 'test_acc': 0.8937999606132507}]\n",
            "51 0.001 [{'running_loss': 0.07327114790678024, 'running_acc': 0.9759374856948853, 'loss': 0.07381701469421387, 'acc': 0.9757999777793884, 'val_loss': 0.37209561467170715, 'val_acc': 0.8878999948501587, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38045650720596313, 'test_acc': 0.8942999839782715}]\n",
            "52 0.001 [{'running_loss': 0.07816333323717117, 'running_acc': 0.9729687571525574, 'loss': 0.07305803894996643, 'acc': 0.9761250019073486, 'val_loss': 0.37019041180610657, 'val_acc': 0.8937000036239624, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38373541831970215, 'test_acc': 0.8928999900817871}]\n",
            "53 0.001 [{'running_loss': 0.07307065278291702, 'running_acc': 0.9756249785423279, 'loss': 0.0712125226855278, 'acc': 0.9771499633789062, 'val_loss': 0.3741600513458252, 'val_acc': 0.8901999592781067, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38895007967948914, 'test_acc': 0.8912999629974365}]\n",
            "54 0.001 [{'running_loss': 0.06870555132627487, 'running_acc': 0.9807812571525574, 'loss': 0.07144425809383392, 'acc': 0.9780749678611755, 'val_loss': 0.3778725266456604, 'val_acc': 0.8921999931335449, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.37862884998321533, 'test_acc': 0.8937999606132507}]\n",
            "55 0.001 [{'running_loss': 0.07581836730241776, 'running_acc': 0.9754687547683716, 'loss': 0.07304564118385315, 'acc': 0.9770499467849731, 'val_loss': 0.3743517994880676, 'val_acc': 0.8923999667167664, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3813170790672302, 'test_acc': 0.8931999802589417}]\n",
            "56 0.001 [{'running_loss': 0.07121208310127258, 'running_acc': 0.9784374833106995, 'loss': 0.07010454684495926, 'acc': 0.9781999588012695, 'val_loss': 0.373739629983902, 'val_acc': 0.8930999636650085, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3870812654495239, 'test_acc': 0.8928999900817871}]\n",
            "57 0.001 [{'running_loss': 0.07356148958206177, 'running_acc': 0.9770312309265137, 'loss': 0.0696987435221672, 'acc': 0.9783749580383301, 'val_loss': 0.3722180724143982, 'val_acc': 0.8888999819755554, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3840688467025757, 'test_acc': 0.8919999599456787}]\n",
            "58 0.001 [{'running_loss': 0.06602346897125244, 'running_acc': 0.9785937070846558, 'loss': 0.06940561532974243, 'acc': 0.9777500033378601, 'val_loss': 0.37554579973220825, 'val_acc': 0.8922999501228333, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3849639594554901, 'test_acc': 0.8930000066757202}]\n",
            "59 0.001 [{'running_loss': 0.06778129935264587, 'running_acc': 0.97718745470047, 'loss': 0.06817693263292313, 'acc': 0.9791749715805054, 'val_loss': 0.38647451996803284, 'val_acc': 0.8924999833106995, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3901645541191101, 'test_acc': 0.8903999924659729}]\n",
            "60 0.001 [{'running_loss': 0.06056126579642296, 'running_acc': 0.9823437333106995, 'loss': 0.06747155636548996, 'acc': 0.9787249565124512, 'val_loss': 0.3838093876838684, 'val_acc': 0.8901000022888184, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3875689208507538, 'test_acc': 0.8916999697685242}]\n",
            "61 0.001 [{'running_loss': 0.0647973120212555, 'running_acc': 0.98046875, 'loss': 0.06868007779121399, 'acc': 0.9784500002861023, 'val_loss': 0.3807874321937561, 'val_acc': 0.8895999789237976, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38593584299087524, 'test_acc': 0.8930000066757202}]\n",
            "62 0.001 [{'running_loss': 0.05993925780057907, 'running_acc': 0.981249988079071, 'loss': 0.06435985863208771, 'acc': 0.9800249934196472, 'val_loss': 0.3863361179828644, 'val_acc': 0.8925999999046326, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38640105724334717, 'test_acc': 0.8919000029563904}]\n",
            "63 0.001 [{'running_loss': 0.06813869625329971, 'running_acc': 0.9779687523841858, 'loss': 0.06774846464395523, 'acc': 0.978149950504303, 'val_loss': 0.37470191717147827, 'val_acc': 0.8894000053405762, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39031854271888733, 'test_acc': 0.8912000060081482}]\n",
            "64 0.001 [{'running_loss': 0.068678118288517, 'running_acc': 0.977343738079071, 'loss': 0.06642010062932968, 'acc': 0.9790999889373779, 'val_loss': 0.37576356530189514, 'val_acc': 0.88919997215271, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3906923234462738, 'test_acc': 0.8926999568939209}]\n",
            "65 0.001 [{'running_loss': 0.06505761295557022, 'running_acc': 0.9790624976158142, 'loss': 0.06484662741422653, 'acc': 0.9785249829292297, 'val_loss': 0.37225866317749023, 'val_acc': 0.8901999592781067, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3879931569099426, 'test_acc': 0.8930000066757202}]\n",
            "66 0.001 [{'running_loss': 0.06685865670442581, 'running_acc': 0.9789062142372131, 'loss': 0.06468741595745087, 'acc': 0.9803999662399292, 'val_loss': 0.36999815702438354, 'val_acc': 0.8934999704360962, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38981831073760986, 'test_acc': 0.892799973487854}]\n",
            "67 0.001 [{'running_loss': 0.06437662988901138, 'running_acc': 0.9789062142372131, 'loss': 0.06574946641921997, 'acc': 0.9787999987602234, 'val_loss': 0.3900328576564789, 'val_acc': 0.8876999616622925, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38783183693885803, 'test_acc': 0.8917999863624573}]\n",
            "68 0.001 [{'running_loss': 0.058464888483285904, 'running_acc': 0.9821874499320984, 'loss': 0.0632328987121582, 'acc': 0.9793999791145325, 'val_loss': 0.38740307092666626, 'val_acc': 0.890999972820282, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3912651240825653, 'test_acc': 0.8910999894142151}]\n",
            "69 0.001 [{'running_loss': 0.05601492151618004, 'running_acc': 0.9834374785423279, 'loss': 0.06237227842211723, 'acc': 0.9801249504089355, 'val_loss': 0.3835064470767975, 'val_acc': 0.8901000022888184, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3905211091041565, 'test_acc': 0.8944000005722046}]\n",
            "70 0.001 [{'running_loss': 0.06576403230428696, 'running_acc': 0.9801562428474426, 'loss': 0.06423230469226837, 'acc': 0.979574978351593, 'val_loss': 0.38386908173561096, 'val_acc': 0.8894999623298645, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39287954568862915, 'test_acc': 0.8928999900817871}]\n",
            "71 0.001 [{'running_loss': 0.05657683685421944, 'running_acc': 0.9818750023841858, 'loss': 0.06354275345802307, 'acc': 0.9801999926567078, 'val_loss': 0.37707290053367615, 'val_acc': 0.8933999538421631, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39090344309806824, 'test_acc': 0.8933999538421631}]\n",
            "72 0.001 [{'running_loss': 0.05990631878376007, 'running_acc': 0.9820312261581421, 'loss': 0.060171473771333694, 'acc': 0.9814249873161316, 'val_loss': 0.38654983043670654, 'val_acc': 0.8912999629974365, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3958849310874939, 'test_acc': 0.8934999704360962}]\n",
            "73 0.001 [{'running_loss': 0.07068892568349838, 'running_acc': 0.9768750071525574, 'loss': 0.06144639477133751, 'acc': 0.9809499979019165, 'val_loss': 0.39091429114341736, 'val_acc': 0.8894000053405762, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.38894036412239075, 'test_acc': 0.8928999900817871}]\n",
            "74 0.001 [{'running_loss': 0.05753052607178688, 'running_acc': 0.9835937023162842, 'loss': 0.058379676192998886, 'acc': 0.9821999669075012, 'val_loss': 0.38647183775901794, 'val_acc': 0.8924999833106995, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40012288093566895, 'test_acc': 0.8914999961853027}]\n",
            "75 0.001 [{'running_loss': 0.06189890578389168, 'running_acc': 0.9829687476158142, 'loss': 0.06046127527952194, 'acc': 0.9812749624252319, 'val_loss': 0.37235715985298157, 'val_acc': 0.8915999531745911, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3987939953804016, 'test_acc': 0.8919999599456787}]\n",
            "76 0.001 [{'running_loss': 0.05617833510041237, 'running_acc': 0.9834374785423279, 'loss': 0.05974278226494789, 'acc': 0.9817749857902527, 'val_loss': 0.3835068643093109, 'val_acc': 0.8924999833106995, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3927023112773895, 'test_acc': 0.8939999938011169}]\n",
            "77 0.001 [{'running_loss': 0.06056324392557144, 'running_acc': 0.9790624976158142, 'loss': 0.06054238975048065, 'acc': 0.9809249639511108, 'val_loss': 0.3938917815685272, 'val_acc': 0.8883000016212463, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.3949560523033142, 'test_acc': 0.8923999667167664}]\n",
            "78 0.001 [{'running_loss': 0.060406871140003204, 'running_acc': 0.981249988079071, 'loss': 0.05859961733222008, 'acc': 0.9815999865531921, 'val_loss': 0.39248034358024597, 'val_acc': 0.8848999738693237, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4003095030784607, 'test_acc': 0.890999972820282}]\n",
            "79 0.001 [{'running_loss': 0.054290127009153366, 'running_acc': 0.9831249713897705, 'loss': 0.05788327008485794, 'acc': 0.9813999533653259, 'val_loss': 0.37617677450180054, 'val_acc': 0.8923999667167664, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39711782336235046, 'test_acc': 0.8932999968528748}]\n",
            "80 0.001 [{'running_loss': 0.053935837000608444, 'running_acc': 0.9839062094688416, 'loss': 0.05772921442985535, 'acc': 0.9820249676704407, 'val_loss': 0.3873395025730133, 'val_acc': 0.890999972820282, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.403606653213501, 'test_acc': 0.8904999494552612}]\n",
            "81 0.001 [{'running_loss': 0.056202199310064316, 'running_acc': 0.98046875, 'loss': 0.0558108314871788, 'acc': 0.9824000000953674, 'val_loss': 0.3772315979003906, 'val_acc': 0.8941999673843384, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40602007508277893, 'test_acc': 0.8914999961853027}]\n",
            "82 0.001 [{'running_loss': 0.06113250181078911, 'running_acc': 0.9817187190055847, 'loss': 0.057459715753793716, 'acc': 0.982574999332428, 'val_loss': 0.3926519453525543, 'val_acc': 0.8910999894142151, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.39969807863235474, 'test_acc': 0.8919999599456787}]\n",
            "83 0.001 [{'running_loss': 0.053109440952539444, 'running_acc': 0.9832812547683716, 'loss': 0.054214563220739365, 'acc': 0.9831249713897705, 'val_loss': 0.3928634524345398, 'val_acc': 0.892799973487854, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.404794305562973, 'test_acc': 0.8925999999046326}]\n",
            "84 0.001 [{'running_loss': 0.05310060456395149, 'running_acc': 0.9834374785423279, 'loss': 0.057590994983911514, 'acc': 0.9815999865531921, 'val_loss': 0.4108664095401764, 'val_acc': 0.8884999752044678, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4060586988925934, 'test_acc': 0.8921999931335449}]\n",
            "85 0.001 [{'running_loss': 0.05079687386751175, 'running_acc': 0.9860936999320984, 'loss': 0.05541191250085831, 'acc': 0.9828999638557434, 'val_loss': 0.39817339181900024, 'val_acc': 0.8919000029563904, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4092523753643036, 'test_acc': 0.8908999562263489}]\n",
            "86 0.001 [{'running_loss': 0.061827048659324646, 'running_acc': 0.9776562452316284, 'loss': 0.05665549263358116, 'acc': 0.9819499850273132, 'val_loss': 0.3927079141139984, 'val_acc': 0.8905999660491943, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4077322483062744, 'test_acc': 0.8934999704360962}]\n",
            "87 0.001 [{'running_loss': 0.05075150355696678, 'running_acc': 0.9857812523841858, 'loss': 0.0534975528717041, 'acc': 0.9830999970436096, 'val_loss': 0.3858979344367981, 'val_acc': 0.8915999531745911, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4102666974067688, 'test_acc': 0.8940999507904053}]\n",
            "88 0.001 [{'running_loss': 0.054095201194286346, 'running_acc': 0.9823437333106995, 'loss': 0.05434928461909294, 'acc': 0.9828749895095825, 'val_loss': 0.39430519938468933, 'val_acc': 0.8912000060081482, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.40650367736816406, 'test_acc': 0.8912999629974365}]\n",
            "89 0.001 [{'running_loss': 0.05513034760951996, 'running_acc': 0.9815624952316284, 'loss': 0.055078376084566116, 'acc': 0.9827999472618103, 'val_loss': 0.39285582304000854, 'val_acc': 0.8905999660491943, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4114530682563782, 'test_acc': 0.8921999931335449}]\n",
            "90 0.001 [{'running_loss': 0.05021228268742561, 'running_acc': 0.9853124618530273, 'loss': 0.05349354073405266, 'acc': 0.9831499457359314, 'val_loss': 0.3897954821586609, 'val_acc': 0.8917999863624573, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41187578439712524, 'test_acc': 0.8920999765396118}]\n",
            "91 0.001 [{'running_loss': 0.05415065214037895, 'running_acc': 0.9828124642372131, 'loss': 0.05369562283158302, 'acc': 0.98294997215271, 'val_loss': 0.38303807377815247, 'val_acc': 0.8908999562263489, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41150882840156555, 'test_acc': 0.8924999833106995}]\n",
            "92 0.001 [{'running_loss': 0.050253525376319885, 'running_acc': 0.9859374761581421, 'loss': 0.050870150327682495, 'acc': 0.9843249917030334, 'val_loss': 0.41079455614089966, 'val_acc': 0.892799973487854, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41674840450286865, 'test_acc': 0.8912999629974365}]\n",
            "93 0.001 [{'running_loss': 0.05385775491595268, 'running_acc': 0.9848437309265137, 'loss': 0.05251133441925049, 'acc': 0.9837999939918518, 'val_loss': 0.3985576629638672, 'val_acc': 0.8903999924659729, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41940510272979736, 'test_acc': 0.8901000022888184}]\n",
            "94 0.001 [{'running_loss': 0.05391310155391693, 'running_acc': 0.9826562404632568, 'loss': 0.05068158730864525, 'acc': 0.9837249517440796, 'val_loss': 0.3985465466976166, 'val_acc': 0.8930000066757202, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4166725277900696, 'test_acc': 0.8912999629974365}]\n",
            "95 0.001 [{'running_loss': 0.05763356015086174, 'running_acc': 0.9821874499320984, 'loss': 0.052919793874025345, 'acc': 0.9840250015258789, 'val_loss': 0.4014923572540283, 'val_acc': 0.8890999555587769, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42000019550323486, 'test_acc': 0.8907999992370605}]\n",
            "96 0.001 [{'running_loss': 0.048894744366407394, 'running_acc': 0.9842187166213989, 'loss': 0.04924677312374115, 'acc': 0.9846499562263489, 'val_loss': 0.4029223322868347, 'val_acc': 0.8885999917984009, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4159550368785858, 'test_acc': 0.892799973487854}]\n",
            "97 0.001 [{'running_loss': 0.05598010495305061, 'running_acc': 0.9834374785423279, 'loss': 0.05095061659812927, 'acc': 0.9845499992370605, 'val_loss': 0.3953465521335602, 'val_acc': 0.8903999924659729, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41793301701545715, 'test_acc': 0.8899999856948853}]\n",
            "98 0.001 [{'running_loss': 0.05663743242621422, 'running_acc': 0.9815624952316284, 'loss': 0.04975033178925514, 'acc': 0.9845999479293823, 'val_loss': 0.39788395166397095, 'val_acc': 0.8925999999046326, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4170154333114624, 'test_acc': 0.8903999924659729}]\n",
            "99 0.001 [{'running_loss': 0.05022719129920006, 'running_acc': 0.984375, 'loss': 0.04879435524344444, 'acc': 0.9849249720573425, 'val_loss': 0.4084464907646179, 'val_acc': 0.8894999623298645, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4197910726070404, 'test_acc': 0.8912999629974365}]\n",
            "100 0.001 [{'running_loss': 0.050565168261528015, 'running_acc': 0.9837499856948853, 'loss': 0.04894706979393959, 'acc': 0.9843999743461609, 'val_loss': 0.40190771222114563, 'val_acc': 0.8904999494552612, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4187971353530884, 'test_acc': 0.8914999961853027}]\n",
            "101 0.001 [{'running_loss': 0.0484170988202095, 'running_acc': 0.9846875071525574, 'loss': 0.045151859521865845, 'acc': 0.9872750043869019, 'val_loss': 0.415625661611557, 'val_acc': 0.8906999826431274, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41858187317848206, 'test_acc': 0.8912999629974365}]\n",
            "102 0.001 [{'running_loss': 0.04780315235257149, 'running_acc': 0.9864062070846558, 'loss': 0.04926324263215065, 'acc': 0.9846749901771545, 'val_loss': 0.3916783928871155, 'val_acc': 0.8937999606132507, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.41517373919487, 'test_acc': 0.8908999562263489}]\n",
            "103 0.001 [{'running_loss': 0.04526620730757713, 'running_acc': 0.9859374761581421, 'loss': 0.047361232340335846, 'acc': 0.9855249524116516, 'val_loss': 0.41114866733551025, 'val_acc': 0.8881999850273132, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4255557656288147, 'test_acc': 0.8901000022888184}]\n",
            "104 0.001 [{'running_loss': 0.053109921514987946, 'running_acc': 0.9824999570846558, 'loss': 0.04766667261719704, 'acc': 0.9858250021934509, 'val_loss': 0.41658705472946167, 'val_acc': 0.8901000022888184, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4231497347354889, 'test_acc': 0.8898999691009521}]\n",
            "105 0.001 [{'running_loss': 0.047682005912065506, 'running_acc': 0.9867187142372131, 'loss': 0.046833403408527374, 'acc': 0.9860749840736389, 'val_loss': 0.4004037082195282, 'val_acc': 0.8908999562263489, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4284321963787079, 'test_acc': 0.8898999691009521}]\n",
            "106 0.001 [{'running_loss': 0.04145078733563423, 'running_acc': 0.9881249666213989, 'loss': 0.045367058366537094, 'acc': 0.986175000667572, 'val_loss': 0.4088059067726135, 'val_acc': 0.8896999955177307, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42084649205207825, 'test_acc': 0.889799952507019}]\n",
            "107 0.001 [{'running_loss': 0.049132488667964935, 'running_acc': 0.9854687452316284, 'loss': 0.04530050978064537, 'acc': 0.9863749742507935, 'val_loss': 0.4060986340045929, 'val_acc': 0.8894000053405762, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4212092161178589, 'test_acc': 0.8930000066757202}]\n",
            "108 0.001 [{'running_loss': 0.049559056758880615, 'running_acc': 0.9848437309265137, 'loss': 0.046523112803697586, 'acc': 0.9864499568939209, 'val_loss': 0.4087085723876953, 'val_acc': 0.8920999765396118, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4205460846424103, 'test_acc': 0.8903999924659729}]\n",
            "109 0.001 [{'running_loss': 0.04420124366879463, 'running_acc': 0.9873437285423279, 'loss': 0.04372060298919678, 'acc': 0.9873749613761902, 'val_loss': 0.4180564880371094, 'val_acc': 0.8880999684333801, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42465534806251526, 'test_acc': 0.8928999900817871}]\n",
            "110 0.001 [{'running_loss': 0.04714518040418625, 'running_acc': 0.98499995470047, 'loss': 0.045369427651166916, 'acc': 0.986175000667572, 'val_loss': 0.4011665880680084, 'val_acc': 0.8934999704360962, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42313748598098755, 'test_acc': 0.8910999894142151}]\n",
            "111 0.001 [{'running_loss': 0.04472004249691963, 'running_acc': 0.9879687428474426, 'loss': 0.04540277272462845, 'acc': 0.9860000014305115, 'val_loss': 0.40922966599464417, 'val_acc': 0.890999972820282, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4258468449115753, 'test_acc': 0.8894999623298645}]\n",
            "112 0.001 [{'running_loss': 0.04785846173763275, 'running_acc': 0.9856249690055847, 'loss': 0.04526912048459053, 'acc': 0.9860999584197998, 'val_loss': 0.4100329875946045, 'val_acc': 0.8925999999046326, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42776018381118774, 'test_acc': 0.8925999999046326}]\n",
            "113 0.001 [{'running_loss': 0.04780083894729614, 'running_acc': 0.98499995470047, 'loss': 0.04454541206359863, 'acc': 0.9861999750137329, 'val_loss': 0.41942647099494934, 'val_acc': 0.8912000060081482, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.42890751361846924, 'test_acc': 0.8895999789237976}]\n",
            "114 0.001 [{'running_loss': 0.043844807893037796, 'running_acc': 0.9878124594688416, 'loss': 0.04349318519234657, 'acc': 0.9872999787330627, 'val_loss': 0.41681647300720215, 'val_acc': 0.8919999599456787, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.43385058641433716, 'test_acc': 0.88919997215271}]\n",
            "115 0.001 [{'running_loss': 0.04220866039395332, 'running_acc': 0.9871875047683716, 'loss': 0.043654248118400574, 'acc': 0.986674964427948, 'val_loss': 0.40968912839889526, 'val_acc': 0.8894999623298645, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4292134642601013, 'test_acc': 0.88919997215271}]\n",
            "116 0.001 [{'running_loss': 0.039779212325811386, 'running_acc': 0.9879687428474426, 'loss': 0.042538098990917206, 'acc': 0.9873249530792236, 'val_loss': 0.4238751232624054, 'val_acc': 0.8888999819755554, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4303393065929413, 'test_acc': 0.8913999795913696}]\n",
            "117 0.001 [{'running_loss': 0.04441072791814804, 'running_acc': 0.9870312213897705, 'loss': 0.043021462857723236, 'acc': 0.9870499968528748, 'val_loss': 0.42625293135643005, 'val_acc': 0.88919997215271, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4275915026664734, 'test_acc': 0.8920999765396118}]\n",
            "118 0.001 [{'running_loss': 0.0462331585586071, 'running_acc': 0.9871875047683716, 'loss': 0.04379868507385254, 'acc': 0.9871249794960022, 'val_loss': 0.41082507371902466, 'val_acc': 0.8937000036239624, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.4237465262413025, 'test_acc': 0.8899999856948853}]\n",
            "119 0.001 [{'running_loss': 0.04597684368491173, 'running_acc': 0.9857812523841858, 'loss': 0.043444693088531494, 'acc': 0.986799955368042, 'val_loss': 0.41881218552589417, 'val_acc': 0.889799952507019, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 0.43588393926620483, 'test_acc': 0.88919997215271}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y2TX6HqaMzS3"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzjJwDz9wSPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for Adam on Batch Size 128 and Decayed Hyperparameter Schedule\n",
        "\n",
        "# final_lr_Adam = 0.001\n",
        "# train_loss_Adam = np.array([1.63037312, 1.18033886, 0.92124522, 0.74593353, 0.649499  ,\n",
        "#        0.58431119, 0.54541808, 0.51439738, 0.49331105, 0.47896644,\n",
        "#        0.46071315, 0.44623247, 0.43867418, 0.42844105, 0.41658249,\n",
        "#        0.40986881, 0.40467444, 0.32660127, 0.30073026, 0.29085344,\n",
        "#        0.29234371, 0.28999776, 0.28819755, 0.2858167 , 0.28251076,\n",
        "#        0.21936484, 0.19471307, 0.19317281, 0.18988737, 0.18502732,\n",
        "#        0.18650103, 0.18830603, 0.18535516, 0.18172537, 0.1821478 ,\n",
        "#        0.18266056, 0.18000871, 0.135066  , 0.11242285, 0.10386638,\n",
        "#        0.10252357, 0.10051892, 0.09701664, 0.09196085, 0.10051732,\n",
        "#        0.07228046, 0.06021492, 0.05446434, 0.04942729, 0.04877077,\n",
        "#        0.04611096, 0.04589562, 0.04714105, 0.03618029, 0.02985483,\n",
        "#        0.02656709, 0.02304123, 0.02364865, 0.02268021, 0.02181247,\n",
        "#        0.02237869, 0.02099021, 0.01981045, 0.02248288, 0.02040866,\n",
        "#        0.01705012, 0.01566005, 0.01398123, 0.01382037, 0.01297838,\n",
        "#        0.01276973, 0.01189775, 0.01228313, 0.01137058, 0.00922516,\n",
        "#        0.00977179, 0.01028183, 0.00906915, 0.00893492, 0.00901656,\n",
        "#        0.00920589, 0.00845823, 0.00842503, 0.00811068, 0.00854083,\n",
        "#        0.00798958, 0.00788236, 0.00735939, 0.0071263 , 0.00819363,\n",
        "#        0.007181  , 0.00757284, 0.00724412, 0.00778591, 0.00721631,\n",
        "#        0.00698527, 0.00676491, 0.00694828, 0.00695372, 0.00705811,\n",
        "#        0.00729424, 0.00732701, 0.00671617, 0.00712735, 0.00651296,\n",
        "#        0.00689158, 0.00671136, 0.00683861, 0.0071457 , 0.00697656,\n",
        "#        0.00650613, 0.00692365, 0.00686604, 0.00651534, 0.00622547,\n",
        "#        0.00638588, 0.00686074, 0.00701365, 0.00687658, 0.00622484])\n",
        "# train_accuracy_Adam = np.array([0.390075  , 0.57577497, 0.66949999, 0.73957497, 0.77599996,\n",
        "#        0.79565001, 0.81089997, 0.82232499, 0.83295   , 0.83282501,\n",
        "#        0.84259999, 0.84507495, 0.850425  , 0.85157496, 0.85692495,\n",
        "#        0.85864997, 0.86072499, 0.888475  , 0.89667499, 0.89954996,\n",
        "#        0.89924997, 0.89934999, 0.89942497, 0.903     , 0.90337497,\n",
        "#        0.92559999, 0.93252498, 0.93457496, 0.934475  , 0.93667495,\n",
        "#        0.936625  , 0.93564999, 0.93512499, 0.93752497, 0.93610001,\n",
        "#        0.93629998, 0.93782496, 0.95414996, 0.96267498, 0.96432495,\n",
        "#        0.965325  , 0.96599996, 0.96752501, 0.96912497, 0.96614999,\n",
        "#        0.97574997, 0.98027498, 0.98234999, 0.98427498, 0.98512495,\n",
        "#        0.98492497, 0.98477495, 0.98484999, 0.988675  , 0.99129999,\n",
        "#        0.99212497, 0.99419999, 0.99367499, 0.99357498, 0.99364996,\n",
        "#        0.99394995, 0.99432498, 0.994425  , 0.99339998, 0.99425   ,\n",
        "#        0.99564999, 0.99594998, 0.99672496, 0.99659997, 0.99707496,\n",
        "#        0.996925  , 0.9975    , 0.99719995, 0.9975    , 0.99852496,\n",
        "#        0.99782497, 0.99797499, 0.99817497, 0.99829996, 0.99829996,\n",
        "#        0.99817497, 0.99842495, 0.99852496, 0.99849999, 0.99839997,\n",
        "#        0.99874997, 0.99869996, 0.99874997, 0.99887496, 0.99855   ,\n",
        "#        0.99884999, 0.99874997, 0.99869996, 0.99849999, 0.99884999,\n",
        "#        0.99879998, 0.99904996, 0.99884999, 0.99894994, 0.99864995,\n",
        "#        0.99877495, 0.99887496, 0.99892497, 0.99894994, 0.99884999,\n",
        "#        0.99879998, 0.99897498, 0.99892497, 0.99879998, 0.99887496,\n",
        "#        0.99909997, 0.99892497, 0.99877495, 0.99887496, 0.99904996,\n",
        "#        0.99897498, 0.99879998, 0.998725  , 0.99882495, 0.99927497])\n",
        "# valid_loss_Adam = np.array([1.50543761, 1.19222295, 1.03162003, 1.5794574 , 0.80966121,\n",
        "#        0.75711215, 0.69306922, 0.72585523, 0.8965379 , 1.04301274,\n",
        "#        0.72814721, 0.73280692, 0.83490455, 0.62131929, 0.7760914 ,\n",
        "#        0.72611558, 0.63089973, 0.49938178, 0.51523685, 0.45988402,\n",
        "#        0.47406921, 0.61857086, 0.76355284, 0.49494356, 0.44980043,\n",
        "#        0.40191844, 0.35578534, 0.40480283, 0.4803206 , 0.37487879,\n",
        "#        0.49941066, 0.37194106, 0.40863952, 0.45908734, 0.40280992,\n",
        "#        0.54325849, 0.39434153, 0.33613825, 0.33893469, 0.36527094,\n",
        "#        0.33749986, 0.36911309, 0.36238733, 0.36181712, 0.3755489 ,\n",
        "#        0.33938816, 0.34158641, 0.31389222, 0.34449029, 0.35422897,\n",
        "#        0.33680391, 0.3412312 , 0.34444362, 0.33294412, 0.34146556,\n",
        "#        0.33415589, 0.3306058 , 0.34739435, 0.34571844, 0.35057172,\n",
        "#        0.32948801, 0.33901691, 0.33334926, 0.3404423 , 0.34588557,\n",
        "#        0.32538399, 0.33203316, 0.33605772, 0.32927096, 0.33608636,\n",
        "#        0.33334732, 0.34528267, 0.35082984, 0.35531309, 0.33366475,\n",
        "#        0.33979121, 0.33885267, 0.33884791, 0.34217775, 0.32993868,\n",
        "#        0.34376106, 0.34471011, 0.33191624, 0.34832358, 0.34679821,\n",
        "#        0.34462917, 0.34129137, 0.32131574, 0.34362254, 0.33366999,\n",
        "#        0.34550434, 0.33725005, 0.34642231, 0.34384438, 0.33797726,\n",
        "#        0.34195745, 0.34329823, 0.35127693, 0.33257309, 0.34635884,\n",
        "#        0.33383083, 0.32843965, 0.34224761, 0.34516147, 0.32830516,\n",
        "#        0.35290539, 0.34367836, 0.35333595, 0.34754622, 0.34935188,\n",
        "#        0.34657907, 0.34491047, 0.34374148, 0.35103148, 0.34158558,\n",
        "#        0.33374858, 0.34417945, 0.34275946, 0.34608164, 0.33730862])\n",
        "# valid_accuracy_Adam = np.array([0.4603    , 0.58090001, 0.6354    , 0.5521    , 0.7288    ,\n",
        "#        0.74879998, 0.76249999, 0.74619997, 0.7094    , 0.67549998,\n",
        "#        0.76639998, 0.75689995, 0.71379995, 0.79179996, 0.7507    ,\n",
        "#        0.75580001, 0.79079998, 0.83230001, 0.82819998, 0.84359998,\n",
        "#        0.83939999, 0.7956    , 0.75689995, 0.83309996, 0.85209996,\n",
        "#        0.86469996, 0.88019997, 0.86619997, 0.8427    , 0.87959999,\n",
        "#        0.85119998, 0.87809998, 0.86589998, 0.85399997, 0.87180001,\n",
        "#        0.83219999, 0.87399995, 0.8951    , 0.8962    , 0.88709998,\n",
        "#        0.8962    , 0.88499999, 0.89209998, 0.88609999, 0.8897    ,\n",
        "#        0.90099996, 0.90249997, 0.90779996, 0.90309995, 0.9005    ,\n",
        "#        0.90199995, 0.90619999, 0.9073    , 0.9095    , 0.90799999,\n",
        "#        0.90880001, 0.91329998, 0.91049999, 0.91229999, 0.91139996,\n",
        "#        0.91469997, 0.91299999, 0.91299999, 0.91060001, 0.91289997,\n",
        "#        0.91469997, 0.91549999, 0.91649997, 0.91689998, 0.9181    ,\n",
        "#        0.91469997, 0.91419995, 0.91539997, 0.917     , 0.91599995,\n",
        "#        0.91670001, 0.9149    , 0.91459996, 0.91689998, 0.91999996,\n",
        "#        0.9149    , 0.9145    , 0.92089999, 0.91649997, 0.91789997,\n",
        "#        0.91959995, 0.91689998, 0.91999996, 0.91579998, 0.91869998,\n",
        "#        0.91819996, 0.91679996, 0.91819996, 0.91670001, 0.91709995,\n",
        "#        0.91889995, 0.91409999, 0.91619998, 0.91979998, 0.91609997,\n",
        "#        0.91859996, 0.9217    , 0.91709995, 0.91959995, 0.91719997,\n",
        "#        0.91929996, 0.9174    , 0.91469997, 0.91709995, 0.91839999,\n",
        "#        0.91619998, 0.91850001, 0.9156    , 0.9188    , 0.91679996,\n",
        "#        0.9156    , 0.91549999, 0.91999996, 0.9181    , 0.91889995])\n",
        "# test_loss_Adam = np.array([1.46223879, 1.16137731, 1.02145219, 1.73623872, 0.77763063,\n",
        "#        0.76775396, 0.6852898 , 0.73415548, 0.96855843, 1.13558292,\n",
        "#        0.77283943, 0.74337631, 0.8162058 , 0.60288996, 0.79537541,\n",
        "#        0.77774423, 0.61050206, 0.5016852 , 0.50489777, 0.43930387,\n",
        "#        0.46668118, 0.60547125, 0.82805115, 0.49150091, 0.43563801,\n",
        "#        0.38774973, 0.34283438, 0.43154034, 0.50294882, 0.38620439,\n",
        "#        0.51459217, 0.36457121, 0.41444308, 0.46136022, 0.40921745,\n",
        "#        0.54807788, 0.40127897, 0.35424355, 0.3422575 , 0.38510349,\n",
        "#        0.33656868, 0.39873049, 0.35694504, 0.36647627, 0.38988662,\n",
        "#        0.33043757, 0.36026487, 0.32458168, 0.33394688, 0.34622553,\n",
        "#        0.3419106 , 0.35293466, 0.35482323, 0.3446489 , 0.3404946 ,\n",
        "#        0.35234904, 0.34949493, 0.35750905, 0.34861538, 0.35688055,\n",
        "#        0.35297871, 0.36294344, 0.36406967, 0.36414525, 0.36120752,\n",
        "#        0.3535451 , 0.34808517, 0.35238835, 0.34721005, 0.35092252,\n",
        "#        0.35432422, 0.35860884, 0.36137441, 0.35304889, 0.35266697,\n",
        "#        0.35670671, 0.35799691, 0.35807449, 0.3567313 , 0.35588086,\n",
        "#        0.35549158, 0.36002263, 0.35349819, 0.36321193, 0.36367354,\n",
        "#        0.35921705, 0.35703275, 0.35557705, 0.35733268, 0.35597011,\n",
        "#        0.35876846, 0.35636151, 0.35619393, 0.35398668, 0.35857388,\n",
        "#        0.35642606, 0.35891956, 0.35926166, 0.35786384, 0.35769981,\n",
        "#        0.35870624, 0.35664171, 0.35847834, 0.35785264, 0.36405164,\n",
        "#        0.36172214, 0.35987663, 0.36116499, 0.35753471, 0.35916239,\n",
        "#        0.35645956, 0.35887745, 0.35627782, 0.36278585, 0.36203104,\n",
        "#        0.36264992, 0.35739467, 0.36003786, 0.36124519, 0.35664594])\n",
        "# test_accuracy_Adam = np.array([0.47239998, 0.5909    , 0.64719999, 0.5399    , 0.74369997,\n",
        "#        0.75209999, 0.77160001, 0.74479997, 0.7008    , 0.67229998,\n",
        "#        0.7586    , 0.76059997, 0.72649997, 0.79649997, 0.75229996,\n",
        "#        0.7471    , 0.80070001, 0.8351    , 0.83599997, 0.8484    ,\n",
        "#        0.84709996, 0.8028    , 0.74839997, 0.8387    , 0.86039996,\n",
        "#        0.87169999, 0.88669997, 0.86909997, 0.83999997, 0.87899995,\n",
        "#        0.84799999, 0.88449997, 0.87189996, 0.86109996, 0.87479997,\n",
        "#        0.83669996, 0.8714    , 0.8937    , 0.89709997, 0.88549995,\n",
        "#        0.89629996, 0.88009995, 0.89359999, 0.8915    , 0.89309996,\n",
        "#        0.9073    , 0.89999998, 0.90819997, 0.90959996, 0.90569997,\n",
        "#        0.90399998, 0.9091    , 0.90549999, 0.90880001, 0.91299999,\n",
        "#        0.90929997, 0.91009998, 0.91159999, 0.91439998, 0.91039997,\n",
        "#        0.91399997, 0.9113    , 0.91299999, 0.91099995, 0.91069996,\n",
        "#        0.91469997, 0.91429996, 0.91589999, 0.91619998, 0.917     ,\n",
        "#        0.9181    , 0.91749996, 0.9156    , 0.91649997, 0.91719997,\n",
        "#        0.91839999, 0.91959995, 0.91829997, 0.91709995, 0.91959995,\n",
        "#        0.91859996, 0.91839999, 0.91889995, 0.91789997, 0.91709995,\n",
        "#        0.9181    , 0.91769999, 0.91749996, 0.91839999, 0.9181    ,\n",
        "#        0.91829997, 0.91709995, 0.91869998, 0.91939998, 0.9181    ,\n",
        "#        0.91909999, 0.9188    , 0.91799998, 0.91889995, 0.91929996,\n",
        "#        0.91829997, 0.92039996, 0.91929996, 0.91869998, 0.91709995,\n",
        "#        0.91769999, 0.91869998, 0.91829997, 0.91819996, 0.91909999,\n",
        "#        0.91979998, 0.9174    , 0.91839999, 0.9192    , 0.91829997,\n",
        "#        0.91709995, 0.91859996, 0.91819996, 0.91789997, 0.91829997])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZlauU82-9ALE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ae3eea8-cabb-4fcd-ee40-3de25a8adf53"
      },
      "source": [
        "print(\"Final Learning Rate reached at the end of training: \", final_lr_Adam)\n",
        "print(\"Training Loss Set: \", repr(train_loss_Adam))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_Adam))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_Adam))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_Adam))\n",
        "print(\"Test Loss Set: \", repr(test_loss_Adam))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_Adam))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Learning Rate reached at the end of training:  0.001\n",
            "Training Loss Set:  array([1.63037312, 1.18033886, 0.92124522, 0.74593353, 0.649499  ,\n",
            "       0.58431119, 0.54541808, 0.51439738, 0.49331105, 0.47896644,\n",
            "       0.46071315, 0.44623247, 0.43867418, 0.42844105, 0.41658249,\n",
            "       0.40986881, 0.40467444, 0.32660127, 0.30073026, 0.29085344,\n",
            "       0.29234371, 0.28999776, 0.28819755, 0.2858167 , 0.28251076,\n",
            "       0.21936484, 0.19471307, 0.19317281, 0.18988737, 0.18502732,\n",
            "       0.18650103, 0.18830603, 0.18535516, 0.18172537, 0.1821478 ,\n",
            "       0.18266056, 0.18000871, 0.135066  , 0.11242285, 0.10386638,\n",
            "       0.10252357, 0.10051892, 0.09701664, 0.09196085, 0.10051732,\n",
            "       0.07228046, 0.06021492, 0.05446434, 0.04942729, 0.04877077,\n",
            "       0.04611096, 0.04589562, 0.04714105, 0.03618029, 0.02985483,\n",
            "       0.02656709, 0.02304123, 0.02364865, 0.02268021, 0.02181247,\n",
            "       0.02237869, 0.02099021, 0.01981045, 0.02248288, 0.02040866,\n",
            "       0.01705012, 0.01566005, 0.01398123, 0.01382037, 0.01297838,\n",
            "       0.01276973, 0.01189775, 0.01228313, 0.01137058, 0.00922516,\n",
            "       0.00977179, 0.01028183, 0.00906915, 0.00893492, 0.00901656,\n",
            "       0.00920589, 0.00845823, 0.00842503, 0.00811068, 0.00854083,\n",
            "       0.00798958, 0.00788236, 0.00735939, 0.0071263 , 0.00819363,\n",
            "       0.007181  , 0.00757284, 0.00724412, 0.00778591, 0.00721631,\n",
            "       0.00698527, 0.00676491, 0.00694828, 0.00695372, 0.00705811,\n",
            "       0.00729424, 0.00732701, 0.00671617, 0.00712735, 0.00651296,\n",
            "       0.00689158, 0.00671136, 0.00683861, 0.0071457 , 0.00697656,\n",
            "       0.00650613, 0.00692365, 0.00686604, 0.00651534, 0.00622547,\n",
            "       0.00638588, 0.00686074, 0.00701365, 0.00687658, 0.00622484])\n",
            "Training Accuracy Set:  array([0.390075  , 0.57577497, 0.66949999, 0.73957497, 0.77599996,\n",
            "       0.79565001, 0.81089997, 0.82232499, 0.83295   , 0.83282501,\n",
            "       0.84259999, 0.84507495, 0.850425  , 0.85157496, 0.85692495,\n",
            "       0.85864997, 0.86072499, 0.888475  , 0.89667499, 0.89954996,\n",
            "       0.89924997, 0.89934999, 0.89942497, 0.903     , 0.90337497,\n",
            "       0.92559999, 0.93252498, 0.93457496, 0.934475  , 0.93667495,\n",
            "       0.936625  , 0.93564999, 0.93512499, 0.93752497, 0.93610001,\n",
            "       0.93629998, 0.93782496, 0.95414996, 0.96267498, 0.96432495,\n",
            "       0.965325  , 0.96599996, 0.96752501, 0.96912497, 0.96614999,\n",
            "       0.97574997, 0.98027498, 0.98234999, 0.98427498, 0.98512495,\n",
            "       0.98492497, 0.98477495, 0.98484999, 0.988675  , 0.99129999,\n",
            "       0.99212497, 0.99419999, 0.99367499, 0.99357498, 0.99364996,\n",
            "       0.99394995, 0.99432498, 0.994425  , 0.99339998, 0.99425   ,\n",
            "       0.99564999, 0.99594998, 0.99672496, 0.99659997, 0.99707496,\n",
            "       0.996925  , 0.9975    , 0.99719995, 0.9975    , 0.99852496,\n",
            "       0.99782497, 0.99797499, 0.99817497, 0.99829996, 0.99829996,\n",
            "       0.99817497, 0.99842495, 0.99852496, 0.99849999, 0.99839997,\n",
            "       0.99874997, 0.99869996, 0.99874997, 0.99887496, 0.99855   ,\n",
            "       0.99884999, 0.99874997, 0.99869996, 0.99849999, 0.99884999,\n",
            "       0.99879998, 0.99904996, 0.99884999, 0.99894994, 0.99864995,\n",
            "       0.99877495, 0.99887496, 0.99892497, 0.99894994, 0.99884999,\n",
            "       0.99879998, 0.99897498, 0.99892497, 0.99879998, 0.99887496,\n",
            "       0.99909997, 0.99892497, 0.99877495, 0.99887496, 0.99904996,\n",
            "       0.99897498, 0.99879998, 0.998725  , 0.99882495, 0.99927497])\n",
            "Validation Loss Set:  array([1.50543761, 1.19222295, 1.03162003, 1.5794574 , 0.80966121,\n",
            "       0.75711215, 0.69306922, 0.72585523, 0.8965379 , 1.04301274,\n",
            "       0.72814721, 0.73280692, 0.83490455, 0.62131929, 0.7760914 ,\n",
            "       0.72611558, 0.63089973, 0.49938178, 0.51523685, 0.45988402,\n",
            "       0.47406921, 0.61857086, 0.76355284, 0.49494356, 0.44980043,\n",
            "       0.40191844, 0.35578534, 0.40480283, 0.4803206 , 0.37487879,\n",
            "       0.49941066, 0.37194106, 0.40863952, 0.45908734, 0.40280992,\n",
            "       0.54325849, 0.39434153, 0.33613825, 0.33893469, 0.36527094,\n",
            "       0.33749986, 0.36911309, 0.36238733, 0.36181712, 0.3755489 ,\n",
            "       0.33938816, 0.34158641, 0.31389222, 0.34449029, 0.35422897,\n",
            "       0.33680391, 0.3412312 , 0.34444362, 0.33294412, 0.34146556,\n",
            "       0.33415589, 0.3306058 , 0.34739435, 0.34571844, 0.35057172,\n",
            "       0.32948801, 0.33901691, 0.33334926, 0.3404423 , 0.34588557,\n",
            "       0.32538399, 0.33203316, 0.33605772, 0.32927096, 0.33608636,\n",
            "       0.33334732, 0.34528267, 0.35082984, 0.35531309, 0.33366475,\n",
            "       0.33979121, 0.33885267, 0.33884791, 0.34217775, 0.32993868,\n",
            "       0.34376106, 0.34471011, 0.33191624, 0.34832358, 0.34679821,\n",
            "       0.34462917, 0.34129137, 0.32131574, 0.34362254, 0.33366999,\n",
            "       0.34550434, 0.33725005, 0.34642231, 0.34384438, 0.33797726,\n",
            "       0.34195745, 0.34329823, 0.35127693, 0.33257309, 0.34635884,\n",
            "       0.33383083, 0.32843965, 0.34224761, 0.34516147, 0.32830516,\n",
            "       0.35290539, 0.34367836, 0.35333595, 0.34754622, 0.34935188,\n",
            "       0.34657907, 0.34491047, 0.34374148, 0.35103148, 0.34158558,\n",
            "       0.33374858, 0.34417945, 0.34275946, 0.34608164, 0.33730862])\n",
            "Validation Accuracy Set:  array([0.4603    , 0.58090001, 0.6354    , 0.5521    , 0.7288    ,\n",
            "       0.74879998, 0.76249999, 0.74619997, 0.7094    , 0.67549998,\n",
            "       0.76639998, 0.75689995, 0.71379995, 0.79179996, 0.7507    ,\n",
            "       0.75580001, 0.79079998, 0.83230001, 0.82819998, 0.84359998,\n",
            "       0.83939999, 0.7956    , 0.75689995, 0.83309996, 0.85209996,\n",
            "       0.86469996, 0.88019997, 0.86619997, 0.8427    , 0.87959999,\n",
            "       0.85119998, 0.87809998, 0.86589998, 0.85399997, 0.87180001,\n",
            "       0.83219999, 0.87399995, 0.8951    , 0.8962    , 0.88709998,\n",
            "       0.8962    , 0.88499999, 0.89209998, 0.88609999, 0.8897    ,\n",
            "       0.90099996, 0.90249997, 0.90779996, 0.90309995, 0.9005    ,\n",
            "       0.90199995, 0.90619999, 0.9073    , 0.9095    , 0.90799999,\n",
            "       0.90880001, 0.91329998, 0.91049999, 0.91229999, 0.91139996,\n",
            "       0.91469997, 0.91299999, 0.91299999, 0.91060001, 0.91289997,\n",
            "       0.91469997, 0.91549999, 0.91649997, 0.91689998, 0.9181    ,\n",
            "       0.91469997, 0.91419995, 0.91539997, 0.917     , 0.91599995,\n",
            "       0.91670001, 0.9149    , 0.91459996, 0.91689998, 0.91999996,\n",
            "       0.9149    , 0.9145    , 0.92089999, 0.91649997, 0.91789997,\n",
            "       0.91959995, 0.91689998, 0.91999996, 0.91579998, 0.91869998,\n",
            "       0.91819996, 0.91679996, 0.91819996, 0.91670001, 0.91709995,\n",
            "       0.91889995, 0.91409999, 0.91619998, 0.91979998, 0.91609997,\n",
            "       0.91859996, 0.9217    , 0.91709995, 0.91959995, 0.91719997,\n",
            "       0.91929996, 0.9174    , 0.91469997, 0.91709995, 0.91839999,\n",
            "       0.91619998, 0.91850001, 0.9156    , 0.9188    , 0.91679996,\n",
            "       0.9156    , 0.91549999, 0.91999996, 0.9181    , 0.91889995])\n",
            "Test Loss Set:  array([1.46223879, 1.16137731, 1.02145219, 1.73623872, 0.77763063,\n",
            "       0.76775396, 0.6852898 , 0.73415548, 0.96855843, 1.13558292,\n",
            "       0.77283943, 0.74337631, 0.8162058 , 0.60288996, 0.79537541,\n",
            "       0.77774423, 0.61050206, 0.5016852 , 0.50489777, 0.43930387,\n",
            "       0.46668118, 0.60547125, 0.82805115, 0.49150091, 0.43563801,\n",
            "       0.38774973, 0.34283438, 0.43154034, 0.50294882, 0.38620439,\n",
            "       0.51459217, 0.36457121, 0.41444308, 0.46136022, 0.40921745,\n",
            "       0.54807788, 0.40127897, 0.35424355, 0.3422575 , 0.38510349,\n",
            "       0.33656868, 0.39873049, 0.35694504, 0.36647627, 0.38988662,\n",
            "       0.33043757, 0.36026487, 0.32458168, 0.33394688, 0.34622553,\n",
            "       0.3419106 , 0.35293466, 0.35482323, 0.3446489 , 0.3404946 ,\n",
            "       0.35234904, 0.34949493, 0.35750905, 0.34861538, 0.35688055,\n",
            "       0.35297871, 0.36294344, 0.36406967, 0.36414525, 0.36120752,\n",
            "       0.3535451 , 0.34808517, 0.35238835, 0.34721005, 0.35092252,\n",
            "       0.35432422, 0.35860884, 0.36137441, 0.35304889, 0.35266697,\n",
            "       0.35670671, 0.35799691, 0.35807449, 0.3567313 , 0.35588086,\n",
            "       0.35549158, 0.36002263, 0.35349819, 0.36321193, 0.36367354,\n",
            "       0.35921705, 0.35703275, 0.35557705, 0.35733268, 0.35597011,\n",
            "       0.35876846, 0.35636151, 0.35619393, 0.35398668, 0.35857388,\n",
            "       0.35642606, 0.35891956, 0.35926166, 0.35786384, 0.35769981,\n",
            "       0.35870624, 0.35664171, 0.35847834, 0.35785264, 0.36405164,\n",
            "       0.36172214, 0.35987663, 0.36116499, 0.35753471, 0.35916239,\n",
            "       0.35645956, 0.35887745, 0.35627782, 0.36278585, 0.36203104,\n",
            "       0.36264992, 0.35739467, 0.36003786, 0.36124519, 0.35664594])\n",
            "Test Accuracy Set:  array([0.47239998, 0.5909    , 0.64719999, 0.5399    , 0.74369997,\n",
            "       0.75209999, 0.77160001, 0.74479997, 0.7008    , 0.67229998,\n",
            "       0.7586    , 0.76059997, 0.72649997, 0.79649997, 0.75229996,\n",
            "       0.7471    , 0.80070001, 0.8351    , 0.83599997, 0.8484    ,\n",
            "       0.84709996, 0.8028    , 0.74839997, 0.8387    , 0.86039996,\n",
            "       0.87169999, 0.88669997, 0.86909997, 0.83999997, 0.87899995,\n",
            "       0.84799999, 0.88449997, 0.87189996, 0.86109996, 0.87479997,\n",
            "       0.83669996, 0.8714    , 0.8937    , 0.89709997, 0.88549995,\n",
            "       0.89629996, 0.88009995, 0.89359999, 0.8915    , 0.89309996,\n",
            "       0.9073    , 0.89999998, 0.90819997, 0.90959996, 0.90569997,\n",
            "       0.90399998, 0.9091    , 0.90549999, 0.90880001, 0.91299999,\n",
            "       0.90929997, 0.91009998, 0.91159999, 0.91439998, 0.91039997,\n",
            "       0.91399997, 0.9113    , 0.91299999, 0.91099995, 0.91069996,\n",
            "       0.91469997, 0.91429996, 0.91589999, 0.91619998, 0.917     ,\n",
            "       0.9181    , 0.91749996, 0.9156    , 0.91649997, 0.91719997,\n",
            "       0.91839999, 0.91959995, 0.91829997, 0.91709995, 0.91959995,\n",
            "       0.91859996, 0.91839999, 0.91889995, 0.91789997, 0.91709995,\n",
            "       0.9181    , 0.91769999, 0.91749996, 0.91839999, 0.9181    ,\n",
            "       0.91829997, 0.91709995, 0.91869998, 0.91939998, 0.9181    ,\n",
            "       0.91909999, 0.9188    , 0.91799998, 0.91889995, 0.91929996,\n",
            "       0.91829997, 0.92039996, 0.91929996, 0.91869998, 0.91709995,\n",
            "       0.91769999, 0.91869998, 0.91829997, 0.91819996, 0.91909999,\n",
            "       0.91979998, 0.9174    , 0.91839999, 0.9192    , 0.91829997,\n",
            "       0.91709995, 0.91859996, 0.91819996, 0.91789997, 0.91829997])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RrtWctvSNOXd"
      },
      "source": [
        "##Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k8cINNFXNfXD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6e03f566-0427-4834-ecd5-904b41f22a06"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Create array of epochs for X axis\n",
        "x_axis = np.zeros((120,1))\n",
        "for i in range(120):\n",
        "  x_axis[i] = i\n",
        "\n",
        "#Plot Validation Loss for each algorithm\n",
        "plt.title('Result Analysis')\n",
        "plt.plot(x_axis, valid_loss_SGD, color='green', linewidth = 0.75,label='SGD')\n",
        "plt.plot(x_axis, valid_loss_NAG[:], color='black', label='NAG', linewidth = 0.75,)\n",
        "plt.plot(x_axis, valid_loss_HeavyBall[:], color='blue', label='HB', linewidth = 0.75,)\n",
        "plt.plot(x_axis, valid_loss_ASGD[:], color='red', label='ASGD', linewidth = 0.75,)\n",
        "plt.plot(x_axis, valid_loss_Adam[:], label='Adam', linewidth = 0.75,)\n",
        "\n",
        "plt.legend()\n",
        " \n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.grid(axis='both')\n",
        "plt.savefig('Resnet128_decayed_validloss.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v_FC5kDBN-Fr",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"Resnet128_decayed_validloss.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K49r-LM7CnWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ee51f38e-5c82-4850-b8ae-5726ceea9006"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Create array of epochs for X axis\n",
        "x_axis = np.zeros((120,1))\n",
        "for i in range(120):\n",
        "  x_axis[i] = i\n",
        "\n",
        "#Plot Test Error for each algorithm\n",
        "plt.title('Result Analysis')\n",
        "plt.plot(x_axis, 1-train_accuracy_SGD[:], color='green', linewidth = 0.75,label='SGD')\n",
        "plt.plot(x_axis, 1-train_accuracy_NAG[:], color='black', label='NAG', linewidth = 0.75,)\n",
        "plt.plot(x_axis, 1-train_accuracy_HeavyBall[:], color='blue', label='HB', linewidth = 0.75,)\n",
        "plt.plot(x_axis, 1-train_accuracy_ASGD[:], color='red', label='ASGD', linewidth = 0.75,)\n",
        "plt.plot(x_axis, 1-train_accuracy_Adam[:], label='Adam', linewidth = 0.75,)\n",
        "\n",
        "plt.legend()\n",
        " \n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Test Set Error')\n",
        "plt.grid(axis='both')\n",
        "plt.savefig('Resnet128_decayed_test_error.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JAx4OUmiGUvk",
        "colab": {}
      },
      "source": [
        "files.download('Resnet128_decayed_test_error.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDAvW06ZOMjN"
      },
      "source": [
        "#Fixed Hyperparameter Schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iVNMkrTrOhVS"
      },
      "source": [
        "## SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vd8JyDixOlM6",
        "colab": {}
      },
      "source": [
        "def fixed_stats_SGD(learning_rate, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_SGD = np.zeros(epochs)\n",
        "  train_accuracy_SGD = np.zeros(epochs)\n",
        "  valid_loss_SGD = np.zeros(epochs)\n",
        "  valid_accuracy_SGD = np.zeros(epochs)\n",
        "  test_accuracy_SGD = np.zeros(epochs)\n",
        "  test_loss_SGD = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  i = 0\n",
        "\n",
        "  #Initilise validation loss as criteria at change point\n",
        "  loss = 100\n",
        "  prev_loss = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    #Creates a deep copy of the parameter and gradient tensors and makes them shareable to enable re-use of Resnet modules multiple times\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = optim.SGD(model.parameters(), lr = learning_rate[i], weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation loss reduced by more than 1% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_loss - loss)/prev_loss < 0.01 and i < 3 ) and epoch!=0:\n",
        "        i = i + 1\n",
        "    \n",
        "      prev_loss = loss\n",
        "\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    test_metric = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_SGD[epoch] = result[0]['loss']\n",
        "    train_accuracy_SGD[epoch] = result[0]['acc']\n",
        "    valid_loss_SGD[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_SGD[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_SGD[epoch] = result[0]['test_acc']\n",
        "    test_loss_SGD[epoch] = result[0]['test_loss']\n",
        "    loss = result[0]['loss']\n",
        "\n",
        "    print(epoch, learning_rate[i], result)\n",
        "\n",
        "  return train_loss_SGD, train_accuracy_SGD, valid_loss_SGD, valid_accuracy_SGD, test_loss_SGD, test_accuracy_SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLw1e_KUQtYW",
        "colab": {}
      },
      "source": [
        "epochs = 120\n",
        "learning_rate = [0.27,0.09,0.03,0.01]\n",
        "change = 4\n",
        "\n",
        "train_loss_SGD, train_accuracy_SGD, valid_loss_SGD, valid_accuracy_SGD, test_loss_SGD, test_accuracy_SGD = fixed_stats_SGD(learning_rate, epochs, change)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x16PBJXkRjap"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jegABedGGfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for SGD on Batch Size 128 and Fixed Hyperparameter Schedule\n",
        "\n",
        "# train_loss_SGD = np.array([1.72328997, 1.28034222, 1.02524304, 0.85579467, 0.75539404,\n",
        "#        0.68107015, 0.6236499 , 0.57963866, 0.54414594, 0.52074349,\n",
        "#        0.49652776, 0.47239065, 0.45474711, 0.43356821, 0.42150345,\n",
        "#        0.41060674, 0.3959339 , 0.38330072, 0.37902623, 0.36826918,\n",
        "#        0.36458308, 0.35573462, 0.34390184, 0.3355068 , 0.32942462,\n",
        "#        0.32474434, 0.31781265, 0.31708792, 0.31056657, 0.3042427 ,\n",
        "#        0.30546883, 0.30007178, 0.29094127, 0.29428235, 0.28554115,\n",
        "#        0.28313136, 0.28389776, 0.28350592, 0.27272734, 0.26988745,\n",
        "#        0.2653487 , 0.2652441 , 0.26163745, 0.25682011, 0.2572757 ,\n",
        "#        0.25776568, 0.25617465, 0.25326782, 0.2472717 , 0.24448702,\n",
        "#        0.24276999, 0.24158126, 0.24219011, 0.24379313, 0.23772949,\n",
        "#        0.23360313, 0.23707616, 0.23496374, 0.23154798, 0.22565027,\n",
        "#        0.22894743, 0.22362641, 0.22181435, 0.22139284, 0.22362503,\n",
        "#        0.21892039, 0.22059499, 0.22037318, 0.21662812, 0.12359381,\n",
        "#        0.09181412, 0.08711926, 0.08222803, 0.07588276, 0.08008949,\n",
        "#        0.07787126, 0.0735192 , 0.0733573 , 0.07251874, 0.07160074,\n",
        "#        0.0747529 , 0.07253262, 0.0740836 , 0.07158894, 0.07620709,\n",
        "#        0.04090399, 0.03011571, 0.02885712, 0.0254235 , 0.02266706,\n",
        "#        0.02285781, 0.02033324, 0.02080745, 0.01859738, 0.0192175 ,\n",
        "#        0.0187099 , 0.02019121, 0.01948284, 0.01663471, 0.01786587,\n",
        "#        0.01615327, 0.01708952, 0.01760845, 0.01834076, 0.01740243,\n",
        "#        0.01144316, 0.01041191, 0.0110108 , 0.01017185, 0.00969994,\n",
        "#        0.00932523, 0.00868319, 0.00870176, 0.00831991, 0.0086712 ,\n",
        "#        0.00764281, 0.00903481, 0.00806554, 0.00782344, 0.00789883])\n",
        "# train_accuracy_SGD = np.array([0.35377499, 0.53402501, 0.63119996, 0.69852501, 0.73439997,\n",
        "#        0.760625  , 0.78437495, 0.79727501, 0.81002498, 0.82162499,\n",
        "#        0.82802498, 0.83627498, 0.84429997, 0.85089999, 0.85600001,\n",
        "#        0.8585    , 0.86182499, 0.86722499, 0.86849999, 0.872675  ,\n",
        "#        0.87434995, 0.87612498, 0.88152498, 0.88330001, 0.887375  ,\n",
        "#        0.88589996, 0.89064997, 0.89092499, 0.89219999, 0.8951    ,\n",
        "#        0.89424998, 0.89497501, 0.90027499, 0.89794999, 0.9016    ,\n",
        "#        0.90139997, 0.89999998, 0.90234995, 0.90539998, 0.90602499,\n",
        "#        0.90732497, 0.90897501, 0.90849996, 0.91024995, 0.91125   ,\n",
        "#        0.91009998, 0.91162497, 0.91207498, 0.91312498, 0.91452497,\n",
        "#        0.91469997, 0.91579998, 0.91427499, 0.91562498, 0.91852498,\n",
        "#        0.91842496, 0.917575  , 0.91822499, 0.92014998, 0.92109996,\n",
        "#        0.91962498, 0.92232496, 0.92337495, 0.92389995, 0.92209995,\n",
        "#        0.92434996, 0.92327499, 0.92264998, 0.92364997, 0.958525  ,\n",
        "#        0.96914995, 0.97104996, 0.972525  , 0.97424996, 0.97324997,\n",
        "#        0.97297496, 0.97582495, 0.97547495, 0.974675  , 0.97589999,\n",
        "#        0.97454995, 0.97624999, 0.97472495, 0.97547495, 0.97427499,\n",
        "#        0.9874    , 0.99147499, 0.99199998, 0.9932    , 0.994075  ,\n",
        "#        0.99394995, 0.99507499, 0.99449998, 0.99544996, 0.99495   ,\n",
        "#        0.99504995, 0.99414998, 0.9946    , 0.99537498, 0.99507499,\n",
        "#        0.99589998, 0.995525  , 0.99524999, 0.99514997, 0.99527496,\n",
        "#        0.99764997, 0.99799997, 0.99757499, 0.99752498, 0.99809998,\n",
        "#        0.99799997, 0.99844998, 0.99817497, 0.99852496, 0.99827498,\n",
        "#        0.99879998, 0.99807495, 0.99839997, 0.99852496, 0.99844998])\n",
        "# valid_loss_SGD = np.array([1.91027474, 2.90158391, 1.33701563, 1.47734165, 1.20529318,\n",
        "#        1.19040835, 0.95511472, 0.86554825, 2.44057703, 1.09310603,\n",
        "#        1.11725736, 1.07295978, 1.21384883, 0.92050731, 0.91551471,\n",
        "#        0.66391701, 0.8884173 , 0.85305303, 0.8323431 , 0.67318594,\n",
        "#        1.01316047, 0.54284692, 0.60878634, 1.02722239, 0.61824822,\n",
        "#        0.9322114 , 0.69519991, 0.83323085, 0.69108528, 0.8463726 ,\n",
        "#        0.95900625, 0.53634518, 0.76449615, 0.92353332, 0.66677886,\n",
        "#        0.73852283, 0.57437968, 0.61428571, 0.94161528, 0.5844844 ,\n",
        "#        0.61660564, 0.81779432, 1.03912914, 2.2684598 , 1.09793031,\n",
        "#        0.64922535, 0.67295313, 0.57434583, 0.66547406, 0.74894935,\n",
        "#        0.48413908, 0.62966299, 0.62666821, 0.47451136, 0.6528275 ,\n",
        "#        0.65953714, 0.58581996, 0.65979582, 0.45024684, 1.71934938,\n",
        "#        0.79138261, 0.83330131, 0.53900814, 0.67211902, 1.10641682,\n",
        "#        0.65837908, 0.60451162, 1.34738147, 0.93788123, 0.32431754,\n",
        "#        0.3237319 , 0.40596497, 0.36983821, 0.40574738, 0.39500317,\n",
        "#        0.40325722, 0.37776998, 0.37115857, 0.4544352 , 0.45537022,\n",
        "#        0.37075287, 0.34635317, 0.36833486, 0.49164751, 0.47110856,\n",
        "#        0.31626311, 0.32625198, 0.33833817, 0.33883595, 0.32720268,\n",
        "#        0.34226695, 0.32315862, 0.328679  , 0.32018051, 0.34195212,\n",
        "#        0.36777773, 0.34196314, 0.37061974, 0.34722975, 0.35289198,\n",
        "#        0.36407742, 0.35999054, 0.3581436 , 0.35841548, 0.35257834,\n",
        "#        0.34072092, 0.35215187, 0.3561326 , 0.33670306, 0.35312   ,\n",
        "#        0.33465603, 0.34993514, 0.34276426, 0.34014884, 0.34501392,\n",
        "#        0.34277698, 0.33865163, 0.36083001, 0.37353891, 0.35998371])\n",
        "# valid_accuracy_SGD = np.array([0.352     , 0.28509998, 0.55379999, 0.52419996, 0.60170001,\n",
        "#        0.61129999, 0.67899996, 0.70289999, 0.48499998, 0.66979998,\n",
        "#        0.64059997, 0.66389996, 0.62159997, 0.70529997, 0.7137    ,\n",
        "#        0.77899998, 0.70289999, 0.72409999, 0.74009997, 0.77939999,\n",
        "#        0.70129997, 0.8168    , 0.78279996, 0.68720001, 0.79909998,\n",
        "#        0.70660001, 0.7665    , 0.74829996, 0.77649999, 0.7331    ,\n",
        "#        0.71759999, 0.81830001, 0.75239998, 0.70660001, 0.7942    ,\n",
        "#        0.78059995, 0.81049997, 0.80879998, 0.72279996, 0.81599998,\n",
        "#        0.80429995, 0.74289995, 0.71340001, 0.56040001, 0.70059997,\n",
        "#        0.80249995, 0.78599995, 0.8247    , 0.78929996, 0.77349997,\n",
        "#        0.84200001, 0.80689996, 0.80339998, 0.84869999, 0.80909997,\n",
        "#        0.8021    , 0.80879998, 0.7913    , 0.84859997, 0.61179996,\n",
        "#        0.76669997, 0.75529999, 0.82699996, 0.8028    , 0.70319998,\n",
        "#        0.79229999, 0.80619997, 0.69709998, 0.73069996, 0.8976    ,\n",
        "#        0.89849997, 0.88150001, 0.89029998, 0.88489997, 0.88169998,\n",
        "#        0.88449997, 0.88510001, 0.89199996, 0.87909997, 0.87540001,\n",
        "#        0.89159995, 0.89679998, 0.89129996, 0.86829996, 0.87009996,\n",
        "#        0.90639997, 0.90859997, 0.90779996, 0.90509999, 0.91279995,\n",
        "#        0.912     , 0.91179997, 0.91319996, 0.91359997, 0.91259998,\n",
        "#        0.9091    , 0.90929997, 0.90719998, 0.91159999, 0.90889996,\n",
        "#        0.91289997, 0.90819997, 0.90999997, 0.91119999, 0.91419995,\n",
        "#        0.91429996, 0.91060001, 0.91119999, 0.91529995, 0.9156    ,\n",
        "#        0.91589999, 0.91339999, 0.9149    , 0.91569996, 0.91419995,\n",
        "#        0.91549999, 0.91749996, 0.912     , 0.9163    , 0.91569996])\n",
        "# test_loss_SGD = np.array([1.73960614, 3.1996417 , 1.40677726, 1.45356357, 1.23150373,\n",
        "#        1.2122252 , 0.95685703, 0.86330646, 2.49197364, 1.17764008,\n",
        "#        1.17286038, 1.13336921, 1.20337701, 0.9085114 , 0.93988097,\n",
        "#        0.67869639, 0.90758538, 0.85155517, 0.90659547, 0.6634891 ,\n",
        "#        1.17116046, 0.51833975, 0.62781221, 1.08211303, 0.63777   ,\n",
        "#        1.01391995, 0.68476027, 0.86100841, 0.72534996, 0.89138865,\n",
        "#        1.00867093, 0.51678312, 0.79990423, 1.02534902, 0.6862939 ,\n",
        "#        0.76171464, 0.5573982 , 0.62439913, 1.01014984, 0.60724139,\n",
        "#        0.62874985, 0.84259981, 1.12568092, 2.35537696, 1.24995983,\n",
        "#        0.67104644, 0.71623415, 0.58379686, 0.71561205, 0.75700605,\n",
        "#        0.44837147, 0.62851202, 0.61275887, 0.46967492, 0.64872015,\n",
        "#        0.65285736, 0.56828934, 0.6373626 , 0.46719381, 1.97587991,\n",
        "#        0.81944937, 0.94175106, 0.52848333, 0.66508681, 1.24658942,\n",
        "#        0.66062558, 0.61226159, 1.32862103, 1.02601635, 0.32686201,\n",
        "#        0.31931633, 0.41124916, 0.3488417 , 0.38273662, 0.41722775,\n",
        "#        0.39730614, 0.39085594, 0.37718824, 0.46609357, 0.45315278,\n",
        "#        0.37766597, 0.35028589, 0.37119508, 0.51432312, 0.50119776,\n",
        "#        0.31679019, 0.32770976, 0.33715042, 0.33802903, 0.33189982,\n",
        "#        0.34539971, 0.33819944, 0.33076322, 0.34035465, 0.34468725,\n",
        "#        0.35146323, 0.34958407, 0.36170292, 0.34663108, 0.360414  ,\n",
        "#        0.3697319 , 0.37769151, 0.36231092, 0.35074231, 0.35281596,\n",
        "#        0.34845251, 0.36202264, 0.34519637, 0.34371382, 0.34745374,\n",
        "#        0.34933931, 0.34950399, 0.35223982, 0.35107252, 0.35086918,\n",
        "#        0.35127333, 0.35379073, 0.35364267, 0.35676816, 0.36380231])\n",
        "# test_accuracy_SGD = np.array([0.39769998, 0.2922    , 0.5521    , 0.54170001, 0.59899998,\n",
        "#        0.625     , 0.68549997, 0.71340001, 0.4887    , 0.66949999,\n",
        "#        0.64410001, 0.65899998, 0.63739997, 0.72039998, 0.71129996,\n",
        "#        0.78749996, 0.7177    , 0.73359996, 0.74619997, 0.7881    ,\n",
        "#        0.68159997, 0.82519996, 0.78059995, 0.68470001, 0.79969996,\n",
        "#        0.70419997, 0.78099996, 0.75269997, 0.78139997, 0.73659998,\n",
        "#        0.722     , 0.82669997, 0.74449998, 0.6929    , 0.79449999,\n",
        "#        0.78839999, 0.8193    , 0.81799996, 0.71810001, 0.8193    ,\n",
        "#        0.80320001, 0.74949998, 0.71069998, 0.56549996, 0.68110001,\n",
        "#        0.80599999, 0.78049999, 0.82709998, 0.78599995, 0.78099996,\n",
        "#        0.8563    , 0.81149995, 0.81369996, 0.8502    , 0.81439996,\n",
        "#        0.80659997, 0.81519997, 0.80249995, 0.8563    , 0.60420001,\n",
        "#        0.76989996, 0.74619997, 0.83389997, 0.81209999, 0.69619995,\n",
        "#        0.79829997, 0.81099999, 0.71539998, 0.72399998, 0.8987    ,\n",
        "#        0.90139997, 0.88379997, 0.89649999, 0.89249998, 0.87949997,\n",
        "#        0.88979995, 0.8865    , 0.89379996, 0.87720001, 0.8757    ,\n",
        "#        0.89239997, 0.8994    , 0.89559996, 0.86859995, 0.86799997,\n",
        "#        0.90979999, 0.91219997, 0.91259998, 0.91119999, 0.91189998,\n",
        "#        0.91079998, 0.91229999, 0.91249996, 0.91219997, 0.91339999,\n",
        "#        0.91209996, 0.91119999, 0.91209996, 0.91029996, 0.9113    ,\n",
        "#        0.90929997, 0.90669996, 0.91049999, 0.91459996, 0.91279995,\n",
        "#        0.912     , 0.91259998, 0.91529995, 0.91399997, 0.91479999,\n",
        "#        0.91529995, 0.91539997, 0.9163    , 0.91589999, 0.91509998,\n",
        "#        0.91670001, 0.9181    , 0.91799998, 0.91619998, 0.91529995])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRd-8r7IR8v4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85171d39-1f1e-44cf-cd1b-6affb93774dd"
      },
      "source": [
        "print(\"Training Loss Set: \", repr(train_loss_SGD))\n",
        "print(\"Training Accuracy Set: \", repr(train_accuracy_SGD))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_SGD))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_SGD))\n",
        "print(\"Test Loss Set: \", repr(test_loss_SGD))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_SGD))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss Set:  array([1.72328997, 1.28034222, 1.02524304, 0.85579467, 0.75539404,\n",
            "       0.68107015, 0.6236499 , 0.57963866, 0.54414594, 0.52074349,\n",
            "       0.49652776, 0.47239065, 0.45474711, 0.43356821, 0.42150345,\n",
            "       0.41060674, 0.3959339 , 0.38330072, 0.37902623, 0.36826918,\n",
            "       0.36458308, 0.35573462, 0.34390184, 0.3355068 , 0.32942462,\n",
            "       0.32474434, 0.31781265, 0.31708792, 0.31056657, 0.3042427 ,\n",
            "       0.30546883, 0.30007178, 0.29094127, 0.29428235, 0.28554115,\n",
            "       0.28313136, 0.28389776, 0.28350592, 0.27272734, 0.26988745,\n",
            "       0.2653487 , 0.2652441 , 0.26163745, 0.25682011, 0.2572757 ,\n",
            "       0.25776568, 0.25617465, 0.25326782, 0.2472717 , 0.24448702,\n",
            "       0.24276999, 0.24158126, 0.24219011, 0.24379313, 0.23772949,\n",
            "       0.23360313, 0.23707616, 0.23496374, 0.23154798, 0.22565027,\n",
            "       0.22894743, 0.22362641, 0.22181435, 0.22139284, 0.22362503,\n",
            "       0.21892039, 0.22059499, 0.22037318, 0.21662812, 0.12359381,\n",
            "       0.09181412, 0.08711926, 0.08222803, 0.07588276, 0.08008949,\n",
            "       0.07787126, 0.0735192 , 0.0733573 , 0.07251874, 0.07160074,\n",
            "       0.0747529 , 0.07253262, 0.0740836 , 0.07158894, 0.07620709,\n",
            "       0.04090399, 0.03011571, 0.02885712, 0.0254235 , 0.02266706,\n",
            "       0.02285781, 0.02033324, 0.02080745, 0.01859738, 0.0192175 ,\n",
            "       0.0187099 , 0.02019121, 0.01948284, 0.01663471, 0.01786587,\n",
            "       0.01615327, 0.01708952, 0.01760845, 0.01834076, 0.01740243,\n",
            "       0.01144316, 0.01041191, 0.0110108 , 0.01017185, 0.00969994,\n",
            "       0.00932523, 0.00868319, 0.00870176, 0.00831991, 0.0086712 ,\n",
            "       0.00764281, 0.00903481, 0.00806554, 0.00782344, 0.00789883])\n",
            "Training Accuracy Set:  array([0.35377499, 0.53402501, 0.63119996, 0.69852501, 0.73439997,\n",
            "       0.760625  , 0.78437495, 0.79727501, 0.81002498, 0.82162499,\n",
            "       0.82802498, 0.83627498, 0.84429997, 0.85089999, 0.85600001,\n",
            "       0.8585    , 0.86182499, 0.86722499, 0.86849999, 0.872675  ,\n",
            "       0.87434995, 0.87612498, 0.88152498, 0.88330001, 0.887375  ,\n",
            "       0.88589996, 0.89064997, 0.89092499, 0.89219999, 0.8951    ,\n",
            "       0.89424998, 0.89497501, 0.90027499, 0.89794999, 0.9016    ,\n",
            "       0.90139997, 0.89999998, 0.90234995, 0.90539998, 0.90602499,\n",
            "       0.90732497, 0.90897501, 0.90849996, 0.91024995, 0.91125   ,\n",
            "       0.91009998, 0.91162497, 0.91207498, 0.91312498, 0.91452497,\n",
            "       0.91469997, 0.91579998, 0.91427499, 0.91562498, 0.91852498,\n",
            "       0.91842496, 0.917575  , 0.91822499, 0.92014998, 0.92109996,\n",
            "       0.91962498, 0.92232496, 0.92337495, 0.92389995, 0.92209995,\n",
            "       0.92434996, 0.92327499, 0.92264998, 0.92364997, 0.958525  ,\n",
            "       0.96914995, 0.97104996, 0.972525  , 0.97424996, 0.97324997,\n",
            "       0.97297496, 0.97582495, 0.97547495, 0.974675  , 0.97589999,\n",
            "       0.97454995, 0.97624999, 0.97472495, 0.97547495, 0.97427499,\n",
            "       0.9874    , 0.99147499, 0.99199998, 0.9932    , 0.994075  ,\n",
            "       0.99394995, 0.99507499, 0.99449998, 0.99544996, 0.99495   ,\n",
            "       0.99504995, 0.99414998, 0.9946    , 0.99537498, 0.99507499,\n",
            "       0.99589998, 0.995525  , 0.99524999, 0.99514997, 0.99527496,\n",
            "       0.99764997, 0.99799997, 0.99757499, 0.99752498, 0.99809998,\n",
            "       0.99799997, 0.99844998, 0.99817497, 0.99852496, 0.99827498,\n",
            "       0.99879998, 0.99807495, 0.99839997, 0.99852496, 0.99844998])\n",
            "Validation Loss Set:  array([1.91027474, 2.90158391, 1.33701563, 1.47734165, 1.20529318,\n",
            "       1.19040835, 0.95511472, 0.86554825, 2.44057703, 1.09310603,\n",
            "       1.11725736, 1.07295978, 1.21384883, 0.92050731, 0.91551471,\n",
            "       0.66391701, 0.8884173 , 0.85305303, 0.8323431 , 0.67318594,\n",
            "       1.01316047, 0.54284692, 0.60878634, 1.02722239, 0.61824822,\n",
            "       0.9322114 , 0.69519991, 0.83323085, 0.69108528, 0.8463726 ,\n",
            "       0.95900625, 0.53634518, 0.76449615, 0.92353332, 0.66677886,\n",
            "       0.73852283, 0.57437968, 0.61428571, 0.94161528, 0.5844844 ,\n",
            "       0.61660564, 0.81779432, 1.03912914, 2.2684598 , 1.09793031,\n",
            "       0.64922535, 0.67295313, 0.57434583, 0.66547406, 0.74894935,\n",
            "       0.48413908, 0.62966299, 0.62666821, 0.47451136, 0.6528275 ,\n",
            "       0.65953714, 0.58581996, 0.65979582, 0.45024684, 1.71934938,\n",
            "       0.79138261, 0.83330131, 0.53900814, 0.67211902, 1.10641682,\n",
            "       0.65837908, 0.60451162, 1.34738147, 0.93788123, 0.32431754,\n",
            "       0.3237319 , 0.40596497, 0.36983821, 0.40574738, 0.39500317,\n",
            "       0.40325722, 0.37776998, 0.37115857, 0.4544352 , 0.45537022,\n",
            "       0.37075287, 0.34635317, 0.36833486, 0.49164751, 0.47110856,\n",
            "       0.31626311, 0.32625198, 0.33833817, 0.33883595, 0.32720268,\n",
            "       0.34226695, 0.32315862, 0.328679  , 0.32018051, 0.34195212,\n",
            "       0.36777773, 0.34196314, 0.37061974, 0.34722975, 0.35289198,\n",
            "       0.36407742, 0.35999054, 0.3581436 , 0.35841548, 0.35257834,\n",
            "       0.34072092, 0.35215187, 0.3561326 , 0.33670306, 0.35312   ,\n",
            "       0.33465603, 0.34993514, 0.34276426, 0.34014884, 0.34501392,\n",
            "       0.34277698, 0.33865163, 0.36083001, 0.37353891, 0.35998371])\n",
            "Validation Accuracy Set:  array([0.352     , 0.28509998, 0.55379999, 0.52419996, 0.60170001,\n",
            "       0.61129999, 0.67899996, 0.70289999, 0.48499998, 0.66979998,\n",
            "       0.64059997, 0.66389996, 0.62159997, 0.70529997, 0.7137    ,\n",
            "       0.77899998, 0.70289999, 0.72409999, 0.74009997, 0.77939999,\n",
            "       0.70129997, 0.8168    , 0.78279996, 0.68720001, 0.79909998,\n",
            "       0.70660001, 0.7665    , 0.74829996, 0.77649999, 0.7331    ,\n",
            "       0.71759999, 0.81830001, 0.75239998, 0.70660001, 0.7942    ,\n",
            "       0.78059995, 0.81049997, 0.80879998, 0.72279996, 0.81599998,\n",
            "       0.80429995, 0.74289995, 0.71340001, 0.56040001, 0.70059997,\n",
            "       0.80249995, 0.78599995, 0.8247    , 0.78929996, 0.77349997,\n",
            "       0.84200001, 0.80689996, 0.80339998, 0.84869999, 0.80909997,\n",
            "       0.8021    , 0.80879998, 0.7913    , 0.84859997, 0.61179996,\n",
            "       0.76669997, 0.75529999, 0.82699996, 0.8028    , 0.70319998,\n",
            "       0.79229999, 0.80619997, 0.69709998, 0.73069996, 0.8976    ,\n",
            "       0.89849997, 0.88150001, 0.89029998, 0.88489997, 0.88169998,\n",
            "       0.88449997, 0.88510001, 0.89199996, 0.87909997, 0.87540001,\n",
            "       0.89159995, 0.89679998, 0.89129996, 0.86829996, 0.87009996,\n",
            "       0.90639997, 0.90859997, 0.90779996, 0.90509999, 0.91279995,\n",
            "       0.912     , 0.91179997, 0.91319996, 0.91359997, 0.91259998,\n",
            "       0.9091    , 0.90929997, 0.90719998, 0.91159999, 0.90889996,\n",
            "       0.91289997, 0.90819997, 0.90999997, 0.91119999, 0.91419995,\n",
            "       0.91429996, 0.91060001, 0.91119999, 0.91529995, 0.9156    ,\n",
            "       0.91589999, 0.91339999, 0.9149    , 0.91569996, 0.91419995,\n",
            "       0.91549999, 0.91749996, 0.912     , 0.9163    , 0.91569996])\n",
            "Test Loss Set:  array([1.73960614, 3.1996417 , 1.40677726, 1.45356357, 1.23150373,\n",
            "       1.2122252 , 0.95685703, 0.86330646, 2.49197364, 1.17764008,\n",
            "       1.17286038, 1.13336921, 1.20337701, 0.9085114 , 0.93988097,\n",
            "       0.67869639, 0.90758538, 0.85155517, 0.90659547, 0.6634891 ,\n",
            "       1.17116046, 0.51833975, 0.62781221, 1.08211303, 0.63777   ,\n",
            "       1.01391995, 0.68476027, 0.86100841, 0.72534996, 0.89138865,\n",
            "       1.00867093, 0.51678312, 0.79990423, 1.02534902, 0.6862939 ,\n",
            "       0.76171464, 0.5573982 , 0.62439913, 1.01014984, 0.60724139,\n",
            "       0.62874985, 0.84259981, 1.12568092, 2.35537696, 1.24995983,\n",
            "       0.67104644, 0.71623415, 0.58379686, 0.71561205, 0.75700605,\n",
            "       0.44837147, 0.62851202, 0.61275887, 0.46967492, 0.64872015,\n",
            "       0.65285736, 0.56828934, 0.6373626 , 0.46719381, 1.97587991,\n",
            "       0.81944937, 0.94175106, 0.52848333, 0.66508681, 1.24658942,\n",
            "       0.66062558, 0.61226159, 1.32862103, 1.02601635, 0.32686201,\n",
            "       0.31931633, 0.41124916, 0.3488417 , 0.38273662, 0.41722775,\n",
            "       0.39730614, 0.39085594, 0.37718824, 0.46609357, 0.45315278,\n",
            "       0.37766597, 0.35028589, 0.37119508, 0.51432312, 0.50119776,\n",
            "       0.31679019, 0.32770976, 0.33715042, 0.33802903, 0.33189982,\n",
            "       0.34539971, 0.33819944, 0.33076322, 0.34035465, 0.34468725,\n",
            "       0.35146323, 0.34958407, 0.36170292, 0.34663108, 0.360414  ,\n",
            "       0.3697319 , 0.37769151, 0.36231092, 0.35074231, 0.35281596,\n",
            "       0.34845251, 0.36202264, 0.34519637, 0.34371382, 0.34745374,\n",
            "       0.34933931, 0.34950399, 0.35223982, 0.35107252, 0.35086918,\n",
            "       0.35127333, 0.35379073, 0.35364267, 0.35676816, 0.36380231])\n",
            "Test Accuracy Set:  array([0.39769998, 0.2922    , 0.5521    , 0.54170001, 0.59899998,\n",
            "       0.625     , 0.68549997, 0.71340001, 0.4887    , 0.66949999,\n",
            "       0.64410001, 0.65899998, 0.63739997, 0.72039998, 0.71129996,\n",
            "       0.78749996, 0.7177    , 0.73359996, 0.74619997, 0.7881    ,\n",
            "       0.68159997, 0.82519996, 0.78059995, 0.68470001, 0.79969996,\n",
            "       0.70419997, 0.78099996, 0.75269997, 0.78139997, 0.73659998,\n",
            "       0.722     , 0.82669997, 0.74449998, 0.6929    , 0.79449999,\n",
            "       0.78839999, 0.8193    , 0.81799996, 0.71810001, 0.8193    ,\n",
            "       0.80320001, 0.74949998, 0.71069998, 0.56549996, 0.68110001,\n",
            "       0.80599999, 0.78049999, 0.82709998, 0.78599995, 0.78099996,\n",
            "       0.8563    , 0.81149995, 0.81369996, 0.8502    , 0.81439996,\n",
            "       0.80659997, 0.81519997, 0.80249995, 0.8563    , 0.60420001,\n",
            "       0.76989996, 0.74619997, 0.83389997, 0.81209999, 0.69619995,\n",
            "       0.79829997, 0.81099999, 0.71539998, 0.72399998, 0.8987    ,\n",
            "       0.90139997, 0.88379997, 0.89649999, 0.89249998, 0.87949997,\n",
            "       0.88979995, 0.8865    , 0.89379996, 0.87720001, 0.8757    ,\n",
            "       0.89239997, 0.8994    , 0.89559996, 0.86859995, 0.86799997,\n",
            "       0.90979999, 0.91219997, 0.91259998, 0.91119999, 0.91189998,\n",
            "       0.91079998, 0.91229999, 0.91249996, 0.91219997, 0.91339999,\n",
            "       0.91209996, 0.91119999, 0.91209996, 0.91029996, 0.9113    ,\n",
            "       0.90929997, 0.90669996, 0.91049999, 0.91459996, 0.91279995,\n",
            "       0.912     , 0.91259998, 0.91529995, 0.91399997, 0.91479999,\n",
            "       0.91529995, 0.91539997, 0.9163    , 0.91589999, 0.91509998,\n",
            "       0.91670001, 0.9181    , 0.91799998, 0.91619998, 0.91529995])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ln6jxq8JSKt6"
      },
      "source": [
        "##Heavy Ball"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dPfIv4HeSEvq",
        "colab": {}
      },
      "source": [
        "def fixed_stats_HeavyBall(learning_rate, momentum, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_HeavyBall = np.zeros(epochs)\n",
        "  train_accuracy_HeavyBall = np.zeros(epochs)\n",
        "  valid_loss_HeavyBall = np.zeros(epochs)\n",
        "  valid_accuracy_HeavyBall = np.zeros(epochs)\n",
        "  test_accuracy_HeavyBall = np.zeros(epochs)\n",
        "  test_loss_HeavyBall = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  i = 0\n",
        "  m = 0\n",
        "\n",
        "  loss = 100\n",
        "  prev_loss = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = optim.SGD(model.parameters(), lr = learning_rate[i], momentum = momentum[m], weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'],  verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation loss reduced by more than 1% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_loss - loss)/prev_loss < 0.01 and i < 3 ) and epoch!=0:\n",
        "        if(m < 4):\n",
        "          m = m+1\n",
        "        else:\n",
        "          m = 0\n",
        "          i = i+1\n",
        "\n",
        "      prev_loss = loss\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_HeavyBall[epoch] = result[0]['loss']\n",
        "    train_accuracy_HeavyBall[epoch] = result[0]['acc']\n",
        "    valid_loss_HeavyBall[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_HeavyBall[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_HeavyBall[epoch] = result[0]['test_acc']\n",
        "    test_loss_HeavyBall[epoch] = result[0]['test_loss']\n",
        "    loss = result[0]['loss']\n",
        "\n",
        "    error = 1 - result[0]['val_acc']\n",
        "\n",
        "    print(epoch, learning_rate[i], momentum[m], result)\n",
        "\n",
        "  return train_loss_HeavyBall, train_accuracy_HeavyBall, valid_loss_HeavyBall, valid_accuracy_HeavyBall, test_loss_HeavyBall, test_accuracy_HeavyBall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kU-Zn_IyTcS4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "ef9a1ea9-951c-468b-926d-3e8ca65729f8"
      },
      "source": [
        "epochs = 120\n",
        "learning_rate = [0.27,0.09,0.03,0.01]\n",
        "momentum = [0.97,0.95,0.9,0.8,0.5]\n",
        "change = 4\n",
        "\n",
        "train_loss_HeavyBall, train_accuracy_HeavyBall, valid_loss_HeavyBall, valid_accuracy_HeavyBall, test_loss_HeavyBall, test_accuracy_HeavyBall = fixed_stats_HeavyBall(learning_rate, momentum, epochs, change)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.27 0.97 [{'running_loss': 1.670586109161377, 'running_acc': 0.3631249964237213, 'loss': 1.8257654905319214, 'acc': 0.3112500011920929, 'val_loss': 1.9710627794265747, 'val_acc': 0.3012000024318695, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.129056692123413, 'test_acc': 0.2802000045776367}]\n",
            "1 0.27 0.97 [{'running_loss': 1.4853495359420776, 'running_acc': 0.4593749940395355, 'loss': 1.5664836168289185, 'acc': 0.41887497901916504, 'val_loss': 1.7063645124435425, 'val_acc': 0.4171999990940094, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6810295581817627, 'test_acc': 0.4258999824523926}]\n",
            "2 0.27 0.97 [{'running_loss': 1.360421895980835, 'running_acc': 0.5021874904632568, 'loss': 1.4071134328842163, 'acc': 0.4841499924659729, 'val_loss': 1.912689208984375, 'val_acc': 0.3335999846458435, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.918276071548462, 'test_acc': 0.35510000586509705}]\n",
            "3 0.27 0.97 [{'running_loss': 1.26224684715271, 'running_acc': 0.5457812547683716, 'loss': 1.2894642353057861, 'acc': 0.5327999591827393, 'val_loss': 1.954874038696289, 'val_acc': 0.337799996137619, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.092226028442383, 'test_acc': 0.3506999909877777}]\n",
            "4 0.27 0.97 [{'running_loss': 1.2081255912780762, 'running_acc': 0.5665624737739563, 'loss': 1.1942907571792603, 'acc': 0.5707749724388123, 'val_loss': 1.6632181406021118, 'val_acc': 0.4399999976158142, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.684434413909912, 'test_acc': 0.4576999843120575}]\n",
            "5 0.27 0.97 [{'running_loss': 1.1454734802246094, 'running_acc': 0.6040624976158142, 'loss': 1.1464041471481323, 'acc': 0.5913000106811523, 'val_loss': 1.573891520500183, 'val_acc': 0.4477999806404114, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6209195852279663, 'test_acc': 0.4567999839782715}]\n",
            "6 0.27 0.97 [{'running_loss': 1.1107481718063354, 'running_acc': 0.6092187166213989, 'loss': 1.1133028268814087, 'acc': 0.6069999933242798, 'val_loss': 1.4472547769546509, 'val_acc': 0.5112000107765198, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4116449356079102, 'test_acc': 0.5399999618530273}]\n",
            "7 0.27 0.97 [{'running_loss': 1.0799620151519775, 'running_acc': 0.6245312094688416, 'loss': 1.0927375555038452, 'acc': 0.6172999739646912, 'val_loss': 1.6401960849761963, 'val_acc': 0.4729999899864197, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6263738870620728, 'test_acc': 0.4959999918937683}]\n",
            "8 0.27 0.97 [{'running_loss': 1.1344239711761475, 'running_acc': 0.6100000143051147, 'loss': 1.0828030109405518, 'acc': 0.6217249631881714, 'val_loss': 1.9056850671768188, 'val_acc': 0.3886999785900116, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.0831456184387207, 'test_acc': 0.39149999618530273}]\n",
            "9 0.27 0.97 [{'running_loss': 1.103135108947754, 'running_acc': 0.6259374618530273, 'loss': 1.0912444591522217, 'acc': 0.6206499934196472, 'val_loss': 1.5122216939926147, 'val_acc': 0.5091999769210815, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.679789662361145, 'test_acc': 0.505299985408783}]\n",
            "10 0.27 0.97 [{'running_loss': 1.091710090637207, 'running_acc': 0.6184374690055847, 'loss': 1.0828057527542114, 'acc': 0.6219499707221985, 'val_loss': 1.908751368522644, 'val_acc': 0.3944999873638153, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.9451512098312378, 'test_acc': 0.4196999967098236}]\n",
            "11 0.27 0.97 [{'running_loss': 1.0986049175262451, 'running_acc': 0.6220312118530273, 'loss': 1.0863336324691772, 'acc': 0.6186749935150146, 'val_loss': 2.1817314624786377, 'val_acc': 0.36809998750686646, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.4558353424072266, 'test_acc': 0.3596999943256378}]\n",
            "12 0.27 0.95 [{'running_loss': 1.050171971321106, 'running_acc': 0.6292187571525574, 'loss': 1.0697050094604492, 'acc': 0.625, 'val_loss': 1.5023777484893799, 'val_acc': 0.4966000020503998, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6365197896957397, 'test_acc': 0.47759997844696045}]\n",
            "13 0.27 0.95 [{'running_loss': 0.9457821249961853, 'running_acc': 0.6696875095367432, 'loss': 0.9589852094650269, 'acc': 0.663349986076355, 'val_loss': 1.9574270248413086, 'val_acc': 0.4576999843120575, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.1057136058807373, 'test_acc': 0.46320000290870667}]\n",
            "14 0.27 0.95 [{'running_loss': 0.9915191531181335, 'running_acc': 0.6512500047683716, 'loss': 0.9573342800140381, 'acc': 0.6662999987602234, 'val_loss': 1.2559027671813965, 'val_acc': 0.5546000003814697, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.2871503829956055, 'test_acc': 0.5590999722480774}]\n",
            "15 0.27 0.95 [{'running_loss': 0.9316490888595581, 'running_acc': 0.6734374761581421, 'loss': 0.9331479668617249, 'acc': 0.6742500066757202, 'val_loss': 1.4799684286117554, 'val_acc': 0.4975999891757965, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6077539920806885, 'test_acc': 0.49459999799728394}]\n",
            "16 0.27 0.95 [{'running_loss': 0.9821695685386658, 'running_acc': 0.6629687547683716, 'loss': 0.9363423585891724, 'acc': 0.6750749945640564, 'val_loss': 2.020782470703125, 'val_acc': 0.42829999327659607, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.1883885860443115, 'test_acc': 0.44099998474121094}]\n",
            "17 0.27 0.95 [{'running_loss': 0.9203491806983948, 'running_acc': 0.6860937476158142, 'loss': 0.9292882084846497, 'acc': 0.677424967288971, 'val_loss': 1.1045191287994385, 'val_acc': 0.6240000128746033, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.165299892425537, 'test_acc': 0.6261000037193298}]\n",
            "18 0.27 0.95 [{'running_loss': 0.9234908223152161, 'running_acc': 0.6770312190055847, 'loss': 0.9265178442001343, 'acc': 0.6764249801635742, 'val_loss': 1.649949312210083, 'val_acc': 0.4706999957561493, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.8280118703842163, 'test_acc': 0.46609997749328613}]\n",
            "19 0.27 0.95 [{'running_loss': 0.9369258880615234, 'running_acc': 0.6732812523841858, 'loss': 0.9214997291564941, 'acc': 0.6814000010490417, 'val_loss': 1.9425920248031616, 'val_acc': 0.4262999892234802, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.9507815837860107, 'test_acc': 0.44699999690055847}]\n",
            "20 0.27 0.95 [{'running_loss': 0.9670059084892273, 'running_acc': 0.6592187285423279, 'loss': 0.9134957194328308, 'acc': 0.6835500001907349, 'val_loss': 1.363743543624878, 'val_acc': 0.5185999870300293, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.3451839685440063, 'test_acc': 0.5342000126838684}]\n",
            "21 0.27 0.95 [{'running_loss': 0.9075196385383606, 'running_acc': 0.6915624737739563, 'loss': 0.9099603295326233, 'acc': 0.6864500045776367, 'val_loss': 1.3369989395141602, 'val_acc': 0.5317000150680542, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.345019817352295, 'test_acc': 0.5313999652862549}]\n",
            "22 0.27 0.95 [{'running_loss': 0.917210578918457, 'running_acc': 0.6885937452316284, 'loss': 0.9045954942703247, 'acc': 0.6872749924659729, 'val_loss': 1.7543972730636597, 'val_acc': 0.45019999146461487, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.7873427867889404, 'test_acc': 0.47529998421669006}]\n",
            "23 0.27 0.95 [{'running_loss': 0.9022061228752136, 'running_acc': 0.6889062523841858, 'loss': 0.8953016996383667, 'acc': 0.6932500004768372, 'val_loss': 1.6482712030410767, 'val_acc': 0.4975999891757965, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.0636684894561768, 'test_acc': 0.4805999994277954}]\n",
            "24 0.27 0.95 [{'running_loss': 0.9098663330078125, 'running_acc': 0.6839062571525574, 'loss': 0.9105799794197083, 'acc': 0.6863499879837036, 'val_loss': 1.2141082286834717, 'val_acc': 0.5857999920845032, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.185451626777649, 'test_acc': 0.6039999723434448}]\n",
            "25 0.27 0.95 [{'running_loss': 0.9218125939369202, 'running_acc': 0.6821874976158142, 'loss': 0.8980886340141296, 'acc': 0.6895749568939209, 'val_loss': 2.0598549842834473, 'val_acc': 0.37289997935295105, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 2.385073661804199, 'test_acc': 0.36079999804496765}]\n",
            "26 0.27 0.95 [{'running_loss': 0.9242663383483887, 'running_acc': 0.6793749928474426, 'loss': 0.8884036540985107, 'acc': 0.6952499747276306, 'val_loss': 1.1730455160140991, 'val_acc': 0.600600004196167, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.1740243434906006, 'test_acc': 0.6171000003814697}]\n",
            "27 0.27 0.95 [{'running_loss': 0.9367288947105408, 'running_acc': 0.6771875023841858, 'loss': 0.8983585834503174, 'acc': 0.6892249584197998, 'val_loss': 1.6384397745132446, 'val_acc': 0.4918999969959259, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6741160154342651, 'test_acc': 0.511900007724762}]\n",
            "28 0.27 0.9 [{'running_loss': 0.9380102157592773, 'running_acc': 0.6796875, 'loss': 0.9011894464492798, 'acc': 0.6866250038146973, 'val_loss': 1.5672976970672607, 'val_acc': 0.48559999465942383, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6361892223358154, 'test_acc': 0.5004000067710876}]\n",
            "29 0.27 0.9 [{'running_loss': 0.7588213682174683, 'running_acc': 0.7432812452316284, 'loss': 0.7653321027755737, 'acc': 0.7369999885559082, 'val_loss': 1.4236501455307007, 'val_acc': 0.5837000012397766, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4895257949829102, 'test_acc': 0.5891000032424927}]\n",
            "30 0.27 0.9 [{'running_loss': 0.7427684664726257, 'running_acc': 0.7451562285423279, 'loss': 0.7591084837913513, 'acc': 0.7385249733924866, 'val_loss': 1.0821428298950195, 'val_acc': 0.6256999969482422, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.0984830856323242, 'test_acc': 0.6456999778747559}]\n",
            "31 0.27 0.9 [{'running_loss': 0.7564326524734497, 'running_acc': 0.7445312142372131, 'loss': 0.7529346942901611, 'acc': 0.7428749799728394, 'val_loss': 1.3696696758270264, 'val_acc': 0.5900999903678894, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4244948625564575, 'test_acc': 0.5920000076293945}]\n",
            "32 0.27 0.9 [{'running_loss': 0.7522301077842712, 'running_acc': 0.7437499761581421, 'loss': 0.7515144348144531, 'acc': 0.7418249845504761, 'val_loss': 1.224541187286377, 'val_acc': 0.6060999631881714, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.2770359516143799, 'test_acc': 0.6103000044822693}]\n",
            "33 0.27 0.9 [{'running_loss': 0.7422922253608704, 'running_acc': 0.7401562333106995, 'loss': 0.74368816614151, 'acc': 0.7440999746322632, 'val_loss': 1.2721339464187622, 'val_acc': 0.5959999561309814, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.3256299495697021, 'test_acc': 0.5989999771118164}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e3KJ_eF3UdSH"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJlXEck9ITBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for Heavy Ball on Batch Size 128 and Fixed Hyperparameter Schedule\n",
        "\n",
        "# train_loss_HeavyBall = np.array([1.82582533, 1.58168995, 1.39923131, 1.30678999, 1.24577391,\n",
        "#        1.20475149, 1.15230918, 1.15069234, 1.13610911, 1.13038933,\n",
        "#        1.11736774, 1.10061288, 1.09278047, 1.0895834 , 1.08071959,\n",
        "#        1.09663808, 1.08669841, 0.9973284 , 0.96395904, 0.96136051,\n",
        "#        0.94820446, 0.95224327, 0.94251823, 0.92623061, 0.94187647,\n",
        "#        0.94237137, 0.92952615, 0.92424464, 0.93340695, 0.78451461,\n",
        "#        0.78118497, 0.78193796, 0.76802731, 0.76924407, 0.77931899,\n",
        "#        0.76204181, 0.7614789 , 0.76595581, 0.76314396, 0.75671583,\n",
        "#        0.75856388, 0.6424455 , 0.63427478, 0.63983202, 0.64721042,\n",
        "#        0.64317274, 0.63439983, 0.64050406, 0.63505799, 0.52279359,\n",
        "#        0.50445884, 0.50628895, 0.50306624, 0.50731665, 0.50842243,\n",
        "#        0.51166743, 0.50944424, 0.83593124, 0.75981706, 0.75622088,\n",
        "#        0.76503295, 0.66682565, 0.65993345, 0.67684996, 0.6742425 ,\n",
        "#        0.6628105 , 0.66725326, 0.6724391 , 0.66519988, 0.5674566 ,\n",
        "#        0.55653054, 0.55615294, 0.55982661, 0.56254095, 0.55800951,\n",
        "#        0.56380552, 0.56013465, 0.47910339, 0.46352682, 0.4655025 ,\n",
        "#        0.46625432, 0.46766359, 0.4667955 , 0.46550655, 0.46989211,\n",
        "#        0.39415073, 0.38187891, 0.37461978, 0.37329814, 0.37232041,\n",
        "#        0.37375495, 0.37484014, 0.37331378, 0.64056194, 0.58544505,\n",
        "#        0.5757165 , 0.57121193, 0.49576974, 0.48589569, 0.48542079,\n",
        "#        0.48758218, 0.48392385, 0.48824647, 0.4893434 , 0.48108035,\n",
        "#        0.41435134, 0.40890399, 0.40477708, 0.40385327, 0.40160975,\n",
        "#        0.40383354, 0.41171306, 0.40440843, 0.35087079, 0.34288192,\n",
        "#        0.33894122, 0.335747  , 0.33814135, 0.33768031, 0.33760607])\n",
        "# train_accuracy_HeavyBall = np.array([0.30614999, 0.41167498, 0.49002498, 0.527825  , 0.55197501,\n",
        "#        0.56862497, 0.59227496, 0.59347498, 0.59852499, 0.60034996,\n",
        "#        0.60664999, 0.61194998, 0.61645001, 0.61504996, 0.62105   ,\n",
        "#        0.61497498, 0.61932498, 0.653175  , 0.6631    , 0.66492498,\n",
        "#        0.67092496, 0.66899997, 0.674775  , 0.68004996, 0.67242497,\n",
        "#        0.67172498, 0.67887497, 0.67992496, 0.67667496, 0.73082501,\n",
        "#        0.73097497, 0.73149997, 0.73460001, 0.73729998, 0.73107499,\n",
        "#        0.73717499, 0.73614997, 0.73565   , 0.73612499, 0.74175   ,\n",
        "#        0.73979998, 0.77875   , 0.78147501, 0.77947497, 0.77757496,\n",
        "#        0.77819997, 0.78525001, 0.77794999, 0.78079998, 0.82077497,\n",
        "#        0.82792497, 0.82429999, 0.82662499, 0.825225  , 0.82817501,\n",
        "#        0.82387495, 0.82432497, 0.71169996, 0.73947495, 0.7396    ,\n",
        "#        0.73552495, 0.77057499, 0.77405   , 0.76519996, 0.76792496,\n",
        "#        0.77477497, 0.77152497, 0.76804996, 0.76867497, 0.80599999,\n",
        "#        0.80884999, 0.80925   , 0.80789995, 0.80412495, 0.81077498,\n",
        "#        0.8035    , 0.80732501, 0.83529997, 0.84217501, 0.83959997,\n",
        "#        0.83812499, 0.83969998, 0.83802497, 0.84064996, 0.8398    ,\n",
        "#        0.86422497, 0.86815   , 0.87182498, 0.87184995, 0.87297499,\n",
        "#        0.87159997, 0.86989999, 0.87097496, 0.77732497, 0.79749995,\n",
        "#        0.80067497, 0.80484998, 0.828825  , 0.83204997, 0.83342499,\n",
        "#        0.83162498, 0.8344    , 0.8326    , 0.83177495, 0.83704996,\n",
        "#        0.85817498, 0.8599    , 0.86062497, 0.86072499, 0.86149997,\n",
        "#        0.86212498, 0.85879999, 0.86102498, 0.87902498, 0.88164997,\n",
        "#        0.88414997, 0.8836    , 0.88365   , 0.88489997, 0.88474995])\n",
        "# valid_loss_HeavyBall = np.array([2.75018811, 1.71574962, 2.82945132, 1.67079818, 1.51998734,\n",
        "#        1.35055864, 1.5005182 , 1.59392488, 1.86635625, 1.45086718,\n",
        "#        1.86573184, 1.78719735, 1.35344374, 1.47723353, 1.42669988,\n",
        "#        1.62422836, 1.3200742 , 1.5677439 , 1.20499814, 1.33391452,\n",
        "#        1.28864527, 1.59699643, 1.06963634, 1.69916105, 1.25193691,\n",
        "#        1.4373771 , 1.55796218, 1.19350362, 1.498824  , 1.66483939,\n",
        "#        1.72013974, 1.02347434, 1.12083006, 0.96412534, 1.97713792,\n",
        "#        0.95503354, 1.68282676, 1.16746569, 1.06812012, 1.08315814,\n",
        "#        1.64720309, 0.8370406 , 1.57659757, 1.07713127, 1.41967869,\n",
        "#        0.85526097, 0.81355482, 0.77708638, 0.90235138, 0.79171318,\n",
        "#        1.17479622, 0.80716544, 0.66334903, 0.72195113, 0.73263597,\n",
        "#        0.65575373, 1.07714188, 0.99840009, 1.05240011, 1.3641336 ,\n",
        "#        1.3131026 , 0.92533088, 0.8629145 , 0.94274199, 0.99632567,\n",
        "#        0.74606532, 0.97323149, 0.89409715, 0.94344747, 0.68216556,\n",
        "#        0.88259184, 1.05609894, 0.7400983 , 0.66551208, 0.88437629,\n",
        "#        0.77810854, 0.76362461, 0.78451025, 0.70405829, 0.64511526,\n",
        "#        0.78695023, 0.69542724, 0.80927408, 0.67400056, 0.69772375,\n",
        "#        0.5244168 , 0.49595603, 0.56116271, 0.50989813, 0.58840936,\n",
        "#        0.48597398, 0.5034492 , 0.58486122, 1.30136979, 0.70722312,\n",
        "#        0.76770318, 0.74271905, 0.60686648, 0.64306897, 0.69252461,\n",
        "#        0.63040173, 0.63787889, 0.70988756, 0.6128006 , 0.66330993,\n",
        "#        0.53000087, 0.51877803, 0.52279437, 0.56888777, 0.64979422,\n",
        "#        0.56340659, 0.64236939, 0.64344418, 0.46631354, 0.46541727,\n",
        "#        0.52231848, 0.49729848, 0.47529858, 0.53127128, 0.48384762])\n",
        "# valid_accuracy_HeavyBall = np.array([0.20799999, 0.39209998, 0.30359998, 0.42559999, 0.49829999,\n",
        "#        0.50339997, 0.4795    , 0.39139998, 0.38839999, 0.49719998,\n",
        "#        0.42449999, 0.46429998, 0.51889998, 0.5011    , 0.50369996,\n",
        "#        0.45619997, 0.50819999, 0.4937    , 0.59119999, 0.5237    ,\n",
        "#        0.542     , 0.51099998, 0.63510001, 0.48709998, 0.57839996,\n",
        "#        0.4966    , 0.52450001, 0.59619999, 0.53469998, 0.49409997,\n",
        "#        0.4761    , 0.64969999, 0.63080001, 0.68049997, 0.41249999,\n",
        "#        0.66609997, 0.51499999, 0.61379999, 0.64109999, 0.63529998,\n",
        "#        0.50940001, 0.7141    , 0.55000001, 0.65419996, 0.55769998,\n",
        "#        0.71039999, 0.72060001, 0.73689997, 0.70109999, 0.73710001,\n",
        "#        0.62519997, 0.74000001, 0.77449995, 0.7568    , 0.74799997,\n",
        "#        0.7723    , 0.66289997, 0.65869999, 0.64849997, 0.54729998,\n",
        "#        0.5686    , 0.67449999, 0.71249998, 0.67680001, 0.65450001,\n",
        "#        0.74659997, 0.6656    , 0.6947    , 0.6918    , 0.76379997,\n",
        "#        0.71429998, 0.67949998, 0.74349999, 0.77069998, 0.70129997,\n",
        "#        0.74430001, 0.73979998, 0.73829997, 0.76139998, 0.7798    ,\n",
        "#        0.74239999, 0.76409996, 0.74589998, 0.76669997, 0.77059996,\n",
        "#        0.82249999, 0.83019996, 0.8125    , 0.82299995, 0.80189997,\n",
        "#        0.83399999, 0.82440001, 0.8053    , 0.63599998, 0.7572    ,\n",
        "#        0.73369998, 0.74979997, 0.79399997, 0.77849996, 0.76419997,\n",
        "#        0.77709997, 0.78259999, 0.76679999, 0.7863    , 0.7773    ,\n",
        "#        0.81879997, 0.8233    , 0.8251    , 0.80479997, 0.78119999,\n",
        "#        0.80849999, 0.78689998, 0.78179997, 0.83699995, 0.8409    ,\n",
        "#        0.82319999, 0.83089995, 0.8355    , 0.82269996, 0.83329999])\n",
        "# test_loss_HeavyBall = np.array([2.74179173, 1.75287473, 3.07684326, 1.66542828, 1.6060313 ,\n",
        "#        1.29601538, 1.45933628, 1.57044959, 1.9494828 , 1.62056518,\n",
        "#        2.00086713, 2.00789905, 1.34460735, 1.51407778, 1.45495141,\n",
        "#        1.6372838 , 1.27623105, 1.67650211, 1.30552137, 1.30648541,\n",
        "#        1.39581752, 1.83174026, 1.09985185, 1.91959083, 1.25591981,\n",
        "#        1.44279265, 1.81299138, 1.2626332 , 1.58753943, 1.82634687,\n",
        "#        2.0316608 , 1.01529932, 1.09081006, 0.96806842, 2.43224931,\n",
        "#        0.91154557, 1.74833429, 1.13037527, 1.10703218, 1.07996321,\n",
        "#        1.89260948, 0.87819761, 1.80436265, 1.14946258, 1.62617826,\n",
        "#        0.89227587, 0.84031039, 0.76795983, 0.95325434, 0.81576127,\n",
        "#        1.18194163, 0.81524295, 0.65284812, 0.73307592, 0.742154  ,\n",
        "#        0.64761269, 1.15048075, 0.99831086, 1.02474368, 1.43347633,\n",
        "#        1.36855435, 0.92907494, 0.90467489, 0.93781662, 1.05845714,\n",
        "#        0.74948651, 0.97597224, 0.93465078, 0.99331474, 0.71608913,\n",
        "#        0.91050094, 1.08397138, 0.71614873, 0.66705269, 0.90904778,\n",
        "#        0.75948304, 0.79614133, 0.82423121, 0.66915214, 0.62035006,\n",
        "#        0.81625408, 0.70987481, 0.85915172, 0.69067127, 0.66868514,\n",
        "#        0.49275985, 0.4859468 , 0.56619066, 0.49297449, 0.59493667,\n",
        "#        0.46615052, 0.46949875, 0.57139391, 1.38618505, 0.72877169,\n",
        "#        0.7900818 , 0.73295993, 0.59616953, 0.59902412, 0.66518527,\n",
        "#        0.63826698, 0.63722199, 0.71509445, 0.61317974, 0.63622242,\n",
        "#        0.52252167, 0.50228447, 0.50524068, 0.56495982, 0.62952518,\n",
        "#        0.54806602, 0.63784623, 0.62172896, 0.45037249, 0.44499189,\n",
        "#        0.49145675, 0.48417288, 0.46379796, 0.50466675, 0.46626917])\n",
        "# test_accuracy_HeavyBall = np.array([0.23099999, 0.41099998, 0.3021    , 0.45099998, 0.50489998,\n",
        "#        0.51159996, 0.4991    , 0.4179    , 0.39469999, 0.4788    ,\n",
        "#        0.43109998, 0.44589999, 0.52450001, 0.51480001, 0.51749998,\n",
        "#        0.47409999, 0.52099997, 0.49629998, 0.59209996, 0.54710001,\n",
        "#        0.53069997, 0.493     , 0.64139998, 0.486     , 0.5869    ,\n",
        "#        0.51249999, 0.50819999, 0.59619999, 0.53560001, 0.49899998,\n",
        "#        0.46059999, 0.65939999, 0.65209997, 0.6807    , 0.39069998,\n",
        "#        0.6825    , 0.52999997, 0.63529998, 0.64449996, 0.6372    ,\n",
        "#        0.4901    , 0.7026    , 0.5438    , 0.65929997, 0.53359997,\n",
        "#        0.71349996, 0.71829998, 0.74649996, 0.70169997, 0.7428    ,\n",
        "#        0.64050001, 0.74559999, 0.78329998, 0.7615    , 0.74899995,\n",
        "#        0.77889997, 0.6627    , 0.66299999, 0.6649    , 0.5449    ,\n",
        "#        0.56999999, 0.68129998, 0.70599997, 0.68939996, 0.64179999,\n",
        "#        0.7471    , 0.67739999, 0.69569999, 0.69389999, 0.76309997,\n",
        "#        0.70959997, 0.6904    , 0.74989998, 0.77489996, 0.704     ,\n",
        "#        0.75309998, 0.74759996, 0.7392    , 0.76809996, 0.79189998,\n",
        "#        0.73789996, 0.76569998, 0.74419999, 0.77249998, 0.77770001,\n",
        "#        0.8362    , 0.83569998, 0.81869996, 0.83579999, 0.80259997,\n",
        "#        0.84079999, 0.84149998, 0.81189996, 0.648     , 0.76010001,\n",
        "#        0.7349    , 0.75299996, 0.80089998, 0.796     , 0.77899998,\n",
        "#        0.78819996, 0.78679997, 0.76339996, 0.79229999, 0.78709996,\n",
        "#        0.82569999, 0.83309996, 0.83159995, 0.81229997, 0.79449999,\n",
        "#        0.81449997, 0.78389996, 0.79579997, 0.8488    , 0.84569997,\n",
        "#        0.833     , 0.83889997, 0.84279996, 0.83599997, 0.84200001])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AXSdnKKmUERH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08678378-557d-47ea-8e77-cf77953c8c20"
      },
      "source": [
        "print(\"Training Loss Set: \",repr(train_loss_HeavyBall))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_HeavyBall))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_HeavyBall))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_HeavyBall))\n",
        "print(\"Test Loss Set: \", repr(test_loss_HeavyBall))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_HeavyBall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss Set:  array([1.82582533, 1.58168995, 1.39923131, 1.30678999, 1.24577391,\n",
            "       1.20475149, 1.15230918, 1.15069234, 1.13610911, 1.13038933,\n",
            "       1.11736774, 1.10061288, 1.09278047, 1.0895834 , 1.08071959,\n",
            "       1.09663808, 1.08669841, 0.9973284 , 0.96395904, 0.96136051,\n",
            "       0.94820446, 0.95224327, 0.94251823, 0.92623061, 0.94187647,\n",
            "       0.94237137, 0.92952615, 0.92424464, 0.93340695, 0.78451461,\n",
            "       0.78118497, 0.78193796, 0.76802731, 0.76924407, 0.77931899,\n",
            "       0.76204181, 0.7614789 , 0.76595581, 0.76314396, 0.75671583,\n",
            "       0.75856388, 0.6424455 , 0.63427478, 0.63983202, 0.64721042,\n",
            "       0.64317274, 0.63439983, 0.64050406, 0.63505799, 0.52279359,\n",
            "       0.50445884, 0.50628895, 0.50306624, 0.50731665, 0.50842243,\n",
            "       0.51166743, 0.50944424, 0.83593124, 0.75981706, 0.75622088,\n",
            "       0.76503295, 0.66682565, 0.65993345, 0.67684996, 0.6742425 ,\n",
            "       0.6628105 , 0.66725326, 0.6724391 , 0.66519988, 0.5674566 ,\n",
            "       0.55653054, 0.55615294, 0.55982661, 0.56254095, 0.55800951,\n",
            "       0.56380552, 0.56013465, 0.47910339, 0.46352682, 0.4655025 ,\n",
            "       0.46625432, 0.46766359, 0.4667955 , 0.46550655, 0.46989211,\n",
            "       0.39415073, 0.38187891, 0.37461978, 0.37329814, 0.37232041,\n",
            "       0.37375495, 0.37484014, 0.37331378, 0.64056194, 0.58544505,\n",
            "       0.5757165 , 0.57121193, 0.49576974, 0.48589569, 0.48542079,\n",
            "       0.48758218, 0.48392385, 0.48824647, 0.4893434 , 0.48108035,\n",
            "       0.41435134, 0.40890399, 0.40477708, 0.40385327, 0.40160975,\n",
            "       0.40383354, 0.41171306, 0.40440843, 0.35087079, 0.34288192,\n",
            "       0.33894122, 0.335747  , 0.33814135, 0.33768031, 0.33760607])\n",
            "Training Accuracy Set:  array([0.30614999, 0.41167498, 0.49002498, 0.527825  , 0.55197501,\n",
            "       0.56862497, 0.59227496, 0.59347498, 0.59852499, 0.60034996,\n",
            "       0.60664999, 0.61194998, 0.61645001, 0.61504996, 0.62105   ,\n",
            "       0.61497498, 0.61932498, 0.653175  , 0.6631    , 0.66492498,\n",
            "       0.67092496, 0.66899997, 0.674775  , 0.68004996, 0.67242497,\n",
            "       0.67172498, 0.67887497, 0.67992496, 0.67667496, 0.73082501,\n",
            "       0.73097497, 0.73149997, 0.73460001, 0.73729998, 0.73107499,\n",
            "       0.73717499, 0.73614997, 0.73565   , 0.73612499, 0.74175   ,\n",
            "       0.73979998, 0.77875   , 0.78147501, 0.77947497, 0.77757496,\n",
            "       0.77819997, 0.78525001, 0.77794999, 0.78079998, 0.82077497,\n",
            "       0.82792497, 0.82429999, 0.82662499, 0.825225  , 0.82817501,\n",
            "       0.82387495, 0.82432497, 0.71169996, 0.73947495, 0.7396    ,\n",
            "       0.73552495, 0.77057499, 0.77405   , 0.76519996, 0.76792496,\n",
            "       0.77477497, 0.77152497, 0.76804996, 0.76867497, 0.80599999,\n",
            "       0.80884999, 0.80925   , 0.80789995, 0.80412495, 0.81077498,\n",
            "       0.8035    , 0.80732501, 0.83529997, 0.84217501, 0.83959997,\n",
            "       0.83812499, 0.83969998, 0.83802497, 0.84064996, 0.8398    ,\n",
            "       0.86422497, 0.86815   , 0.87182498, 0.87184995, 0.87297499,\n",
            "       0.87159997, 0.86989999, 0.87097496, 0.77732497, 0.79749995,\n",
            "       0.80067497, 0.80484998, 0.828825  , 0.83204997, 0.83342499,\n",
            "       0.83162498, 0.8344    , 0.8326    , 0.83177495, 0.83704996,\n",
            "       0.85817498, 0.8599    , 0.86062497, 0.86072499, 0.86149997,\n",
            "       0.86212498, 0.85879999, 0.86102498, 0.87902498, 0.88164997,\n",
            "       0.88414997, 0.8836    , 0.88365   , 0.88489997, 0.88474995])\n",
            "Validation Loss Set:  array([2.75018811, 1.71574962, 2.82945132, 1.67079818, 1.51998734,\n",
            "       1.35055864, 1.5005182 , 1.59392488, 1.86635625, 1.45086718,\n",
            "       1.86573184, 1.78719735, 1.35344374, 1.47723353, 1.42669988,\n",
            "       1.62422836, 1.3200742 , 1.5677439 , 1.20499814, 1.33391452,\n",
            "       1.28864527, 1.59699643, 1.06963634, 1.69916105, 1.25193691,\n",
            "       1.4373771 , 1.55796218, 1.19350362, 1.498824  , 1.66483939,\n",
            "       1.72013974, 1.02347434, 1.12083006, 0.96412534, 1.97713792,\n",
            "       0.95503354, 1.68282676, 1.16746569, 1.06812012, 1.08315814,\n",
            "       1.64720309, 0.8370406 , 1.57659757, 1.07713127, 1.41967869,\n",
            "       0.85526097, 0.81355482, 0.77708638, 0.90235138, 0.79171318,\n",
            "       1.17479622, 0.80716544, 0.66334903, 0.72195113, 0.73263597,\n",
            "       0.65575373, 1.07714188, 0.99840009, 1.05240011, 1.3641336 ,\n",
            "       1.3131026 , 0.92533088, 0.8629145 , 0.94274199, 0.99632567,\n",
            "       0.74606532, 0.97323149, 0.89409715, 0.94344747, 0.68216556,\n",
            "       0.88259184, 1.05609894, 0.7400983 , 0.66551208, 0.88437629,\n",
            "       0.77810854, 0.76362461, 0.78451025, 0.70405829, 0.64511526,\n",
            "       0.78695023, 0.69542724, 0.80927408, 0.67400056, 0.69772375,\n",
            "       0.5244168 , 0.49595603, 0.56116271, 0.50989813, 0.58840936,\n",
            "       0.48597398, 0.5034492 , 0.58486122, 1.30136979, 0.70722312,\n",
            "       0.76770318, 0.74271905, 0.60686648, 0.64306897, 0.69252461,\n",
            "       0.63040173, 0.63787889, 0.70988756, 0.6128006 , 0.66330993,\n",
            "       0.53000087, 0.51877803, 0.52279437, 0.56888777, 0.64979422,\n",
            "       0.56340659, 0.64236939, 0.64344418, 0.46631354, 0.46541727,\n",
            "       0.52231848, 0.49729848, 0.47529858, 0.53127128, 0.48384762])\n",
            "Validation Accuracy Set:  array([0.20799999, 0.39209998, 0.30359998, 0.42559999, 0.49829999,\n",
            "       0.50339997, 0.4795    , 0.39139998, 0.38839999, 0.49719998,\n",
            "       0.42449999, 0.46429998, 0.51889998, 0.5011    , 0.50369996,\n",
            "       0.45619997, 0.50819999, 0.4937    , 0.59119999, 0.5237    ,\n",
            "       0.542     , 0.51099998, 0.63510001, 0.48709998, 0.57839996,\n",
            "       0.4966    , 0.52450001, 0.59619999, 0.53469998, 0.49409997,\n",
            "       0.4761    , 0.64969999, 0.63080001, 0.68049997, 0.41249999,\n",
            "       0.66609997, 0.51499999, 0.61379999, 0.64109999, 0.63529998,\n",
            "       0.50940001, 0.7141    , 0.55000001, 0.65419996, 0.55769998,\n",
            "       0.71039999, 0.72060001, 0.73689997, 0.70109999, 0.73710001,\n",
            "       0.62519997, 0.74000001, 0.77449995, 0.7568    , 0.74799997,\n",
            "       0.7723    , 0.66289997, 0.65869999, 0.64849997, 0.54729998,\n",
            "       0.5686    , 0.67449999, 0.71249998, 0.67680001, 0.65450001,\n",
            "       0.74659997, 0.6656    , 0.6947    , 0.6918    , 0.76379997,\n",
            "       0.71429998, 0.67949998, 0.74349999, 0.77069998, 0.70129997,\n",
            "       0.74430001, 0.73979998, 0.73829997, 0.76139998, 0.7798    ,\n",
            "       0.74239999, 0.76409996, 0.74589998, 0.76669997, 0.77059996,\n",
            "       0.82249999, 0.83019996, 0.8125    , 0.82299995, 0.80189997,\n",
            "       0.83399999, 0.82440001, 0.8053    , 0.63599998, 0.7572    ,\n",
            "       0.73369998, 0.74979997, 0.79399997, 0.77849996, 0.76419997,\n",
            "       0.77709997, 0.78259999, 0.76679999, 0.7863    , 0.7773    ,\n",
            "       0.81879997, 0.8233    , 0.8251    , 0.80479997, 0.78119999,\n",
            "       0.80849999, 0.78689998, 0.78179997, 0.83699995, 0.8409    ,\n",
            "       0.82319999, 0.83089995, 0.8355    , 0.82269996, 0.83329999])\n",
            "Test Loss Set:  array([2.74179173, 1.75287473, 3.07684326, 1.66542828, 1.6060313 ,\n",
            "       1.29601538, 1.45933628, 1.57044959, 1.9494828 , 1.62056518,\n",
            "       2.00086713, 2.00789905, 1.34460735, 1.51407778, 1.45495141,\n",
            "       1.6372838 , 1.27623105, 1.67650211, 1.30552137, 1.30648541,\n",
            "       1.39581752, 1.83174026, 1.09985185, 1.91959083, 1.25591981,\n",
            "       1.44279265, 1.81299138, 1.2626332 , 1.58753943, 1.82634687,\n",
            "       2.0316608 , 1.01529932, 1.09081006, 0.96806842, 2.43224931,\n",
            "       0.91154557, 1.74833429, 1.13037527, 1.10703218, 1.07996321,\n",
            "       1.89260948, 0.87819761, 1.80436265, 1.14946258, 1.62617826,\n",
            "       0.89227587, 0.84031039, 0.76795983, 0.95325434, 0.81576127,\n",
            "       1.18194163, 0.81524295, 0.65284812, 0.73307592, 0.742154  ,\n",
            "       0.64761269, 1.15048075, 0.99831086, 1.02474368, 1.43347633,\n",
            "       1.36855435, 0.92907494, 0.90467489, 0.93781662, 1.05845714,\n",
            "       0.74948651, 0.97597224, 0.93465078, 0.99331474, 0.71608913,\n",
            "       0.91050094, 1.08397138, 0.71614873, 0.66705269, 0.90904778,\n",
            "       0.75948304, 0.79614133, 0.82423121, 0.66915214, 0.62035006,\n",
            "       0.81625408, 0.70987481, 0.85915172, 0.69067127, 0.66868514,\n",
            "       0.49275985, 0.4859468 , 0.56619066, 0.49297449, 0.59493667,\n",
            "       0.46615052, 0.46949875, 0.57139391, 1.38618505, 0.72877169,\n",
            "       0.7900818 , 0.73295993, 0.59616953, 0.59902412, 0.66518527,\n",
            "       0.63826698, 0.63722199, 0.71509445, 0.61317974, 0.63622242,\n",
            "       0.52252167, 0.50228447, 0.50524068, 0.56495982, 0.62952518,\n",
            "       0.54806602, 0.63784623, 0.62172896, 0.45037249, 0.44499189,\n",
            "       0.49145675, 0.48417288, 0.46379796, 0.50466675, 0.46626917])\n",
            "Test Accuracy Set:  array([0.23099999, 0.41099998, 0.3021    , 0.45099998, 0.50489998,\n",
            "       0.51159996, 0.4991    , 0.4179    , 0.39469999, 0.4788    ,\n",
            "       0.43109998, 0.44589999, 0.52450001, 0.51480001, 0.51749998,\n",
            "       0.47409999, 0.52099997, 0.49629998, 0.59209996, 0.54710001,\n",
            "       0.53069997, 0.493     , 0.64139998, 0.486     , 0.5869    ,\n",
            "       0.51249999, 0.50819999, 0.59619999, 0.53560001, 0.49899998,\n",
            "       0.46059999, 0.65939999, 0.65209997, 0.6807    , 0.39069998,\n",
            "       0.6825    , 0.52999997, 0.63529998, 0.64449996, 0.6372    ,\n",
            "       0.4901    , 0.7026    , 0.5438    , 0.65929997, 0.53359997,\n",
            "       0.71349996, 0.71829998, 0.74649996, 0.70169997, 0.7428    ,\n",
            "       0.64050001, 0.74559999, 0.78329998, 0.7615    , 0.74899995,\n",
            "       0.77889997, 0.6627    , 0.66299999, 0.6649    , 0.5449    ,\n",
            "       0.56999999, 0.68129998, 0.70599997, 0.68939996, 0.64179999,\n",
            "       0.7471    , 0.67739999, 0.69569999, 0.69389999, 0.76309997,\n",
            "       0.70959997, 0.6904    , 0.74989998, 0.77489996, 0.704     ,\n",
            "       0.75309998, 0.74759996, 0.7392    , 0.76809996, 0.79189998,\n",
            "       0.73789996, 0.76569998, 0.74419999, 0.77249998, 0.77770001,\n",
            "       0.8362    , 0.83569998, 0.81869996, 0.83579999, 0.80259997,\n",
            "       0.84079999, 0.84149998, 0.81189996, 0.648     , 0.76010001,\n",
            "       0.7349    , 0.75299996, 0.80089998, 0.796     , 0.77899998,\n",
            "       0.78819996, 0.78679997, 0.76339996, 0.79229999, 0.78709996,\n",
            "       0.82569999, 0.83309996, 0.83159995, 0.81229997, 0.79449999,\n",
            "       0.81449997, 0.78389996, 0.79579997, 0.8488    , 0.84569997,\n",
            "       0.833     , 0.83889997, 0.84279996, 0.83599997, 0.84200001])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iV6CkpjpUrZb"
      },
      "source": [
        "## NAG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RMaBrAqUUQdQ",
        "colab": {}
      },
      "source": [
        "def fixed_stats_NAG(learning_rate, momentum, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_NAG = np.zeros(epochs)\n",
        "  train_accuracy_NAG = np.zeros(epochs)\n",
        "  valid_loss_NAG = np.zeros(epochs)\n",
        "  valid_accuracy_NAG = np.zeros(epochs)\n",
        "  test_accuracy_NAG = np.zeros(epochs)\n",
        "  test_loss_NAG = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  i = 0\n",
        "  m = 0\n",
        "\n",
        "  loss = 100\n",
        "  prev_loss = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      model = copy.deepcopy(model)\n",
        "      optimiser = optim.SGD(model.parameters(), lr = learning_rate[i], momentum = momentum[m], nesterov = True, weight_decay=0.0005)\n",
        "\n",
        "      #Train the model on training data\n",
        "      trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'],  verbose=0).to(device)\n",
        "      trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "      result = trial.run(epochs=1)\n",
        "\n",
        "      #At change points, update the hyperparameters depending on whether validation loss reduced by more than 1% or not\n",
        "      if epoch%change == 0:\n",
        "        if ((prev_loss - loss)/prev_loss < 0.01 and i < 3 ) and epoch!=0:\n",
        "          if(m < 4):\n",
        "            m = m+1\n",
        "          else:\n",
        "            m = 0\n",
        "            i = i+1\n",
        "\n",
        "        prev_loss = loss\n",
        "\n",
        "      #Compute the metrics on Test Dataset\n",
        "      trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "      #Store the metrics at each epoch\n",
        "      train_loss_NAG[epoch] = result[0]['loss']\n",
        "      train_accuracy_NAG[epoch] = result[0]['acc']\n",
        "      valid_loss_NAG[epoch] = result[0]['val_loss']\n",
        "      valid_accuracy_NAG[epoch] = result[0]['val_acc']\n",
        "      test_accuracy_NAG[epoch] = result[0]['test_acc']\n",
        "      test_loss_NAG[epoch] = result[0]['test_loss']\n",
        "      loss = result[0]['loss']\n",
        "\n",
        "\n",
        "      print(epoch, learning_rate[i], momentum[m], result)\n",
        "    \n",
        "  return train_loss_NAG, train_accuracy_NAG, valid_loss_NAG, valid_accuracy_NAG, test_loss_NAG, test_accuracy_NAG\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_WuocHleU9Z2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "3efdbd80-46c1-4d3b-cc6c-3dddee89962b"
      },
      "source": [
        "epochs = 120\n",
        "learning_rate = [0.27,0.09,0.03,0.01]\n",
        "momentum = [0.97,0.95,0.9,0.8,0.5]\n",
        "change = 4\n",
        "\n",
        "train_loss_NAG, train_accuracy_NAG, valid_loss_NAG, valid_accuracy_NAG, test_loss_NAG, test_accuracy_NAG = fixed_stats_NAG(learning_rate, momentum, epochs, change)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.27 0.95 [{'running_loss': 1.5221710205078125, 'running_acc': 0.44484373927116394, 'loss': 1.7225241661071777, 'acc': 0.351749986410141, 'val_loss': 1.6869374513626099, 'val_acc': 0.3894999921321869, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.6687885522842407, 'test_acc': 0.4007999897003174}]\n",
            "1 0.27 0.95 [{'running_loss': 1.239623785018921, 'running_acc': 0.5553125143051147, 'loss': 1.325116753578186, 'acc': 0.5214250087738037, 'val_loss': 1.5309624671936035, 'val_acc': 0.46789997816085815, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.5332467555999756, 'test_acc': 0.4747999906539917}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-7ead85c7a92e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfinal_lr_NAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_NAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_NAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_NAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracy_NAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_NAG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy_NAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_stats_NAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-fcded781c8f4>\u001b[0m in \u001b[0;36mfixed_stats_NAG\u001b[0;34m(learning_rate, momentum, epochs, change)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackListInjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprinter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_list_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epochs, verbose)\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_start_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m                 \u001b[0mfinal_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOP_TRAINING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36m_fit_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                     \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mparam_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C2h3T-6WVnAR"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT6KPJEZIqsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for NAG on Batch Size 128 and Fixed Hyperparameter Schedule\n",
        "\n",
        "# train_loss_NAG = np.array([1.73338771, 1.4242115 , 1.21844304, 1.11848581, 1.0591284 ,\n",
        "#        1.01552749, 0.98649102, 0.97336596, 0.9614445 , 0.9584446 ,\n",
        "#        0.94488478, 0.94360769, 0.94525474, 0.92986143, 0.92368215,\n",
        "#        0.93611127, 0.92799252, 0.85230649, 0.84374267, 0.8415367 ,\n",
        "#        0.82996804, 0.82943177, 0.82744688, 0.82265061, 0.81849867,\n",
        "#        0.81851614, 0.81384581, 0.82229865, 0.81172585, 0.70236778,\n",
        "#        0.70809519, 0.70773369, 0.69817114, 0.69955695, 0.69094896,\n",
        "#        0.69388652, 0.68654251, 0.69028699, 0.68416899, 0.67945319,\n",
        "#        0.6780476 , 0.67942011, 0.67831963, 0.6791178 , 0.67613959,\n",
        "#        0.5844835 , 0.57318068, 0.58231622, 0.57926208, 0.57766259,\n",
        "#        0.57934886, 0.57532656, 0.57100379, 0.57282072, 0.57126594,\n",
        "#        0.57494515, 0.56606013, 0.4678005 , 0.45349282, 0.45693427,\n",
        "#        0.4541384 , 0.45923737, 0.45904008, 0.45859754, 0.45347667,\n",
        "#        0.72985739, 0.67406756, 0.67574579, 0.66668147, 0.60578072,\n",
        "#        0.59581637, 0.59140903, 0.59334481, 0.59609032, 0.59206969,\n",
        "#        0.5974707 , 0.5906018 , 0.507186  , 0.4911494 , 0.49957553,\n",
        "#        0.49947938, 0.49660093, 0.5024088 , 0.50187767, 0.50263739,\n",
        "#        0.42367339, 0.41739804, 0.41319108, 0.41377112, 0.41634429,\n",
        "#        0.41554892, 0.41849345, 0.413771  , 0.34611648, 0.33432186,\n",
        "#        0.33019835, 0.32444507, 0.32596794, 0.32405451, 0.32920766,\n",
        "#        0.32449937, 0.55416822, 0.51960701, 0.51276314, 0.49936682,\n",
        "#        0.44007644, 0.4252139 , 0.43075916, 0.43269679, 0.43062127,\n",
        "#        0.43362704, 0.43290573, 0.42693692, 0.36850327, 0.35642675,\n",
        "#        0.35430402, 0.35387355, 0.35408956, 0.35357308, 0.35464114])\n",
        "# train_accuracy_NAG = np.array([0.35387498, 0.47849998, 0.56559998, 0.60482496, 0.62547499,\n",
        "#        0.64487499, 0.65354997, 0.66062498, 0.662875  , 0.66609997,\n",
        "#        0.6681    , 0.67102498, 0.67062497, 0.67369998, 0.67927498,\n",
        "#        0.67264998, 0.67659998, 0.70239997, 0.708125  , 0.70737499,\n",
        "#        0.712125  , 0.7119    , 0.71237499, 0.71424997, 0.71529996,\n",
        "#        0.71774995, 0.719675  , 0.71547496, 0.71884996, 0.75439996,\n",
        "#        0.75672495, 0.75799996, 0.75929999, 0.75919998, 0.76232499,\n",
        "#        0.76069999, 0.76545   , 0.76344997, 0.76497495, 0.76444995,\n",
        "#        0.7669    , 0.76504999, 0.76620001, 0.76577497, 0.76567501,\n",
        "#        0.79964995, 0.804775  , 0.79897499, 0.80052495, 0.80237496,\n",
        "#        0.80119997, 0.801875  , 0.80355   , 0.80204999, 0.80339998,\n",
        "#        0.80282497, 0.80504996, 0.8387    , 0.84447497, 0.84284997,\n",
        "#        0.84542495, 0.84329998, 0.842875  , 0.84334999, 0.84454995,\n",
        "#        0.74627501, 0.76894999, 0.7683    , 0.77212501, 0.79237497,\n",
        "#        0.79427499, 0.796175  , 0.79522496, 0.79492497, 0.79664999,\n",
        "#        0.79604995, 0.79804999, 0.82604998, 0.83054996, 0.82979995,\n",
        "#        0.82709998, 0.82842499, 0.82685   , 0.82652497, 0.830275  ,\n",
        "#        0.85412496, 0.85724998, 0.85819995, 0.85822499, 0.85897499,\n",
        "#        0.85817498, 0.85619998, 0.85802495, 0.88249999, 0.88622499,\n",
        "#        0.88592499, 0.88879997, 0.88769996, 0.88839996, 0.8876    ,\n",
        "#        0.888825  , 0.81084996, 0.81989998, 0.82419997, 0.8272    ,\n",
        "#        0.84797496, 0.855775  , 0.85197496, 0.85132498, 0.853275  ,\n",
        "#        0.85172498, 0.84982497, 0.85289997, 0.874075  , 0.87747496,\n",
        "#        0.879125  , 0.8775    , 0.87847495, 0.87822497, 0.87852496])\n",
        "# valid_loss_NAG = np.array([1.85914195, 1.55069399, 3.17015314, 1.46516347, 2.14150524,\n",
        "#        1.29717231, 1.31080997, 2.39367342, 1.20235109, 1.57869458,\n",
        "#        1.16446877, 1.4756912 , 1.44670713, 1.10402012, 1.29759765,\n",
        "#        1.25296223, 1.21646357, 1.28301597, 1.38205779, 1.57219183,\n",
        "#        1.45331717, 1.46534216, 2.70419073, 0.99877912, 1.0893333 ,\n",
        "#        1.08502185, 1.25983834, 1.24727082, 1.53747988, 1.05223846,\n",
        "#        1.3384304 , 1.031358  , 0.94328618, 1.30655658, 2.00255942,\n",
        "#        0.92747712, 1.71909225, 1.16007221, 0.95391047, 1.69136202,\n",
        "#        0.84510875, 1.03460085, 1.31677055, 0.96936607, 1.02762997,\n",
        "#        0.92504972, 0.79860371, 0.92005742, 0.85643828, 1.21276188,\n",
        "#        1.01504862, 0.73683661, 0.69276434, 1.25804102, 1.09864831,\n",
        "#        0.87899321, 0.91368151, 0.67302036, 0.76143157, 0.67963552,\n",
        "#        1.17777836, 0.72837108, 1.05508161, 0.69455612, 0.8681187 ,\n",
        "#        1.00231719, 0.92101848, 0.8922748 , 0.77316874, 0.79222792,\n",
        "#        0.79708672, 0.88485461, 0.88883287, 0.8090536 , 0.77091044,\n",
        "#        0.72101969, 1.07084012, 0.70539838, 0.64415777, 0.60887653,\n",
        "#        0.81925261, 0.81279069, 0.84143907, 0.68309361, 0.71767855,\n",
        "#        0.57684261, 0.55087727, 0.6228283 , 0.73353803, 0.63821566,\n",
        "#        0.75288618, 0.68878907, 0.59955949, 0.48526102, 0.46917963,\n",
        "#        0.55068713, 0.60399485, 0.61787361, 0.75852507, 0.67747831,\n",
        "#        0.49872074, 0.72838259, 0.68957919, 0.64515311, 0.62788814,\n",
        "#        0.54031849, 0.61020958, 0.61913544, 0.54472554, 0.75594074,\n",
        "#        0.66069543, 0.78973335, 0.67817128, 0.54680735, 0.57065672,\n",
        "#        0.56189889, 0.53532857, 0.50061339, 0.49794775, 0.48155454])\n",
        "# valid_accuracy_NAG = np.array([0.3328    , 0.43879998, 0.23369999, 0.52059996, 0.35079998,\n",
        "#        0.56079996, 0.55540001, 0.40369999, 0.57819998, 0.48319998,\n",
        "#        0.59359998, 0.48199999, 0.49139997, 0.6257    , 0.55320001,\n",
        "#        0.59539998, 0.5751    , 0.58270001, 0.5485    , 0.52679998,\n",
        "#        0.53279996, 0.55329996, 0.37019998, 0.65619999, 0.61979997,\n",
        "#        0.62519997, 0.5909    , 0.60879999, 0.52389997, 0.6591    ,\n",
        "#        0.57819998, 0.63879997, 0.68439996, 0.59189999, 0.4418    ,\n",
        "#        0.68970001, 0.54109997, 0.6056    , 0.67619997, 0.51629996,\n",
        "#        0.70840001, 0.64019996, 0.58849996, 0.66639996, 0.66139996,\n",
        "#        0.70059997, 0.74180001, 0.69629997, 0.70999998, 0.60299999,\n",
        "#        0.68799996, 0.74109995, 0.76199996, 0.60710001, 0.65469998,\n",
        "#        0.70379996, 0.69699997, 0.77590001, 0.7475    , 0.77129996,\n",
        "#        0.60969996, 0.74449998, 0.6591    , 0.75769997, 0.72509998,\n",
        "#        0.66229999, 0.6753    , 0.70089996, 0.72939998, 0.72719997,\n",
        "#        0.72399998, 0.70899999, 0.70879996, 0.72889996, 0.73100001,\n",
        "#        0.75199997, 0.64989996, 0.75999999, 0.77950001, 0.79549998,\n",
        "#        0.7414    , 0.73399997, 0.72359997, 0.76550001, 0.76269996,\n",
        "#        0.8028    , 0.81699997, 0.79189998, 0.75379997, 0.78709996,\n",
        "#        0.75239998, 0.77359998, 0.79960001, 0.83569998, 0.84169996,\n",
        "#        0.81399995, 0.79409999, 0.79439998, 0.74519998, 0.77379996,\n",
        "#        0.83129996, 0.7536    , 0.76879996, 0.78499997, 0.78490001,\n",
        "#        0.8168    , 0.79339999, 0.79079998, 0.81479996, 0.74619997,\n",
        "#        0.77739996, 0.74449998, 0.77599996, 0.8161    , 0.80219996,\n",
        "#        0.80919999, 0.82059997, 0.82549995, 0.83339995, 0.83819997])\n",
        "# test_loss_NAG = np.array([1.80646062, 1.49461091, 3.34948468, 1.61990237, 2.44534111,\n",
        "#        1.35324061, 1.26141644, 2.67190409, 1.20533919, 1.56957066,\n",
        "#        1.21479428, 1.49359822, 1.4887495 , 1.09147072, 1.28641927,\n",
        "#        1.28524613, 1.18578088, 1.26930737, 1.41208076, 1.66762745,\n",
        "#        1.44575405, 1.5144136 , 3.15103221, 0.9667297 , 1.08334851,\n",
        "#        1.01309741, 1.34350908, 1.37422395, 1.64784014, 1.07348871,\n",
        "#        1.49413192, 1.02711213, 0.93975526, 1.50784814, 2.64408159,\n",
        "#        0.94504106, 1.96167517, 1.1734376 , 1.0501616 , 1.98579645,\n",
        "#        0.82971668, 1.06618285, 1.39204395, 1.03007996, 1.08641613,\n",
        "#        0.96000749, 0.80575633, 0.95454252, 0.91730183, 1.31744874,\n",
        "#        1.04923749, 0.70095038, 0.70138371, 1.27626026, 1.13230503,\n",
        "#        0.94424999, 0.97144127, 0.70961219, 0.75948566, 0.686369  ,\n",
        "#        1.30114269, 0.73789239, 1.14631152, 0.70972872, 0.88661516,\n",
        "#        1.00001538, 0.94861317, 0.94737428, 0.78486478, 0.74667716,\n",
        "#        0.80351865, 0.874071  , 0.92458421, 0.81814384, 0.73399168,\n",
        "#        0.72474009, 1.13150156, 0.716717  , 0.61792511, 0.62335885,\n",
        "#        0.8376072 , 0.89179212, 0.86634821, 0.65295482, 0.70569503,\n",
        "#        0.56199485, 0.5682193 , 0.61573631, 0.72841734, 0.66310853,\n",
        "#        0.75879401, 0.71755016, 0.62095988, 0.50446153, 0.45697245,\n",
        "#        0.57699984, 0.61830425, 0.67246586, 0.81702852, 0.73237222,\n",
        "#        0.49314895, 0.71838528, 0.70073241, 0.60956508, 0.63136923,\n",
        "#        0.53167379, 0.60962433, 0.63218486, 0.53317827, 0.74996883,\n",
        "#        0.7022354 , 0.82266325, 0.69184732, 0.52815533, 0.5890851 ,\n",
        "#        0.56659091, 0.54695088, 0.48032787, 0.47146174, 0.45904446])\n",
        "# test_accuracy_NAG = np.array([0.36609998, 0.45949998, 0.24599999, 0.5068    , 0.33759999,\n",
        "#        0.56379998, 0.58789998, 0.39679998, 0.58709997, 0.5029    ,\n",
        "#        0.58950001, 0.4799    , 0.49919999, 0.63749999, 0.574     ,\n",
        "#        0.60429996, 0.5887    , 0.60899997, 0.55409998, 0.52810001,\n",
        "#        0.55540001, 0.55949998, 0.37079999, 0.67129999, 0.62149996,\n",
        "#        0.65020001, 0.59819996, 0.59729999, 0.52249998, 0.65859997,\n",
        "#        0.57989997, 0.64209998, 0.68829995, 0.5826    , 0.41399997,\n",
        "#        0.69379997, 0.53389996, 0.60460001, 0.6548    , 0.5011    ,\n",
        "#        0.7191    , 0.63809997, 0.59029996, 0.65700001, 0.65549999,\n",
        "#        0.71029997, 0.74619997, 0.70269996, 0.69549996, 0.59169996,\n",
        "#        0.68399996, 0.76199996, 0.76489997, 0.61790001, 0.66249996,\n",
        "#        0.6943    , 0.69      , 0.77679998, 0.75580001, 0.7723    ,\n",
        "#        0.60389996, 0.74879998, 0.64789999, 0.76249999, 0.7342    ,\n",
        "#        0.67619997, 0.68089998, 0.69209999, 0.73379999, 0.74489999,\n",
        "#        0.72639996, 0.72119999, 0.70550001, 0.73210001, 0.7529    ,\n",
        "#        0.76249999, 0.65799999, 0.75749999, 0.79350001, 0.792     ,\n",
        "#        0.7482    , 0.72869998, 0.72670001, 0.77410001, 0.77239996,\n",
        "#        0.81110001, 0.81239998, 0.79519999, 0.76190001, 0.77999997,\n",
        "#        0.75940001, 0.773     , 0.79399997, 0.8348    , 0.84959996,\n",
        "#        0.81209999, 0.79729998, 0.78319997, 0.73979998, 0.76440001,\n",
        "#        0.83669996, 0.7604    , 0.76819998, 0.79469997, 0.79069996,\n",
        "#        0.82409996, 0.79960001, 0.792     , 0.81989998, 0.75819999,\n",
        "#        0.77599996, 0.74189997, 0.7809    , 0.82809997, 0.80239999,\n",
        "#        0.81049997, 0.8204    , 0.83879995, 0.8441    , 0.84329998])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlXN447jV9w3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0af3f161-184f-430c-cb7f-23f46a0f8231"
      },
      "source": [
        "print(\"Training Loss Set: \",repr(train_loss_NAG))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_NAG))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_NAG))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_NAG))\n",
        "print(\"Test Loss Set: \", repr(test_loss_NAG))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_NAG))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss Set:  array([1.73338771, 1.4242115 , 1.21844304, 1.11848581, 1.0591284 ,\n",
            "       1.01552749, 0.98649102, 0.97336596, 0.9614445 , 0.9584446 ,\n",
            "       0.94488478, 0.94360769, 0.94525474, 0.92986143, 0.92368215,\n",
            "       0.93611127, 0.92799252, 0.85230649, 0.84374267, 0.8415367 ,\n",
            "       0.82996804, 0.82943177, 0.82744688, 0.82265061, 0.81849867,\n",
            "       0.81851614, 0.81384581, 0.82229865, 0.81172585, 0.70236778,\n",
            "       0.70809519, 0.70773369, 0.69817114, 0.69955695, 0.69094896,\n",
            "       0.69388652, 0.68654251, 0.69028699, 0.68416899, 0.67945319,\n",
            "       0.6780476 , 0.67942011, 0.67831963, 0.6791178 , 0.67613959,\n",
            "       0.5844835 , 0.57318068, 0.58231622, 0.57926208, 0.57766259,\n",
            "       0.57934886, 0.57532656, 0.57100379, 0.57282072, 0.57126594,\n",
            "       0.57494515, 0.56606013, 0.4678005 , 0.45349282, 0.45693427,\n",
            "       0.4541384 , 0.45923737, 0.45904008, 0.45859754, 0.45347667,\n",
            "       0.72985739, 0.67406756, 0.67574579, 0.66668147, 0.60578072,\n",
            "       0.59581637, 0.59140903, 0.59334481, 0.59609032, 0.59206969,\n",
            "       0.5974707 , 0.5906018 , 0.507186  , 0.4911494 , 0.49957553,\n",
            "       0.49947938, 0.49660093, 0.5024088 , 0.50187767, 0.50263739,\n",
            "       0.42367339, 0.41739804, 0.41319108, 0.41377112, 0.41634429,\n",
            "       0.41554892, 0.41849345, 0.413771  , 0.34611648, 0.33432186,\n",
            "       0.33019835, 0.32444507, 0.32596794, 0.32405451, 0.32920766,\n",
            "       0.32449937, 0.55416822, 0.51960701, 0.51276314, 0.49936682,\n",
            "       0.44007644, 0.4252139 , 0.43075916, 0.43269679, 0.43062127,\n",
            "       0.43362704, 0.43290573, 0.42693692, 0.36850327, 0.35642675,\n",
            "       0.35430402, 0.35387355, 0.35408956, 0.35357308, 0.35464114])\n",
            "Training Accuracy Set:  array([0.35387498, 0.47849998, 0.56559998, 0.60482496, 0.62547499,\n",
            "       0.64487499, 0.65354997, 0.66062498, 0.662875  , 0.66609997,\n",
            "       0.6681    , 0.67102498, 0.67062497, 0.67369998, 0.67927498,\n",
            "       0.67264998, 0.67659998, 0.70239997, 0.708125  , 0.70737499,\n",
            "       0.712125  , 0.7119    , 0.71237499, 0.71424997, 0.71529996,\n",
            "       0.71774995, 0.719675  , 0.71547496, 0.71884996, 0.75439996,\n",
            "       0.75672495, 0.75799996, 0.75929999, 0.75919998, 0.76232499,\n",
            "       0.76069999, 0.76545   , 0.76344997, 0.76497495, 0.76444995,\n",
            "       0.7669    , 0.76504999, 0.76620001, 0.76577497, 0.76567501,\n",
            "       0.79964995, 0.804775  , 0.79897499, 0.80052495, 0.80237496,\n",
            "       0.80119997, 0.801875  , 0.80355   , 0.80204999, 0.80339998,\n",
            "       0.80282497, 0.80504996, 0.8387    , 0.84447497, 0.84284997,\n",
            "       0.84542495, 0.84329998, 0.842875  , 0.84334999, 0.84454995,\n",
            "       0.74627501, 0.76894999, 0.7683    , 0.77212501, 0.79237497,\n",
            "       0.79427499, 0.796175  , 0.79522496, 0.79492497, 0.79664999,\n",
            "       0.79604995, 0.79804999, 0.82604998, 0.83054996, 0.82979995,\n",
            "       0.82709998, 0.82842499, 0.82685   , 0.82652497, 0.830275  ,\n",
            "       0.85412496, 0.85724998, 0.85819995, 0.85822499, 0.85897499,\n",
            "       0.85817498, 0.85619998, 0.85802495, 0.88249999, 0.88622499,\n",
            "       0.88592499, 0.88879997, 0.88769996, 0.88839996, 0.8876    ,\n",
            "       0.888825  , 0.81084996, 0.81989998, 0.82419997, 0.8272    ,\n",
            "       0.84797496, 0.855775  , 0.85197496, 0.85132498, 0.853275  ,\n",
            "       0.85172498, 0.84982497, 0.85289997, 0.874075  , 0.87747496,\n",
            "       0.879125  , 0.8775    , 0.87847495, 0.87822497, 0.87852496])\n",
            "Validation Loss Set:  array([1.85914195, 1.55069399, 3.17015314, 1.46516347, 2.14150524,\n",
            "       1.29717231, 1.31080997, 2.39367342, 1.20235109, 1.57869458,\n",
            "       1.16446877, 1.4756912 , 1.44670713, 1.10402012, 1.29759765,\n",
            "       1.25296223, 1.21646357, 1.28301597, 1.38205779, 1.57219183,\n",
            "       1.45331717, 1.46534216, 2.70419073, 0.99877912, 1.0893333 ,\n",
            "       1.08502185, 1.25983834, 1.24727082, 1.53747988, 1.05223846,\n",
            "       1.3384304 , 1.031358  , 0.94328618, 1.30655658, 2.00255942,\n",
            "       0.92747712, 1.71909225, 1.16007221, 0.95391047, 1.69136202,\n",
            "       0.84510875, 1.03460085, 1.31677055, 0.96936607, 1.02762997,\n",
            "       0.92504972, 0.79860371, 0.92005742, 0.85643828, 1.21276188,\n",
            "       1.01504862, 0.73683661, 0.69276434, 1.25804102, 1.09864831,\n",
            "       0.87899321, 0.91368151, 0.67302036, 0.76143157, 0.67963552,\n",
            "       1.17777836, 0.72837108, 1.05508161, 0.69455612, 0.8681187 ,\n",
            "       1.00231719, 0.92101848, 0.8922748 , 0.77316874, 0.79222792,\n",
            "       0.79708672, 0.88485461, 0.88883287, 0.8090536 , 0.77091044,\n",
            "       0.72101969, 1.07084012, 0.70539838, 0.64415777, 0.60887653,\n",
            "       0.81925261, 0.81279069, 0.84143907, 0.68309361, 0.71767855,\n",
            "       0.57684261, 0.55087727, 0.6228283 , 0.73353803, 0.63821566,\n",
            "       0.75288618, 0.68878907, 0.59955949, 0.48526102, 0.46917963,\n",
            "       0.55068713, 0.60399485, 0.61787361, 0.75852507, 0.67747831,\n",
            "       0.49872074, 0.72838259, 0.68957919, 0.64515311, 0.62788814,\n",
            "       0.54031849, 0.61020958, 0.61913544, 0.54472554, 0.75594074,\n",
            "       0.66069543, 0.78973335, 0.67817128, 0.54680735, 0.57065672,\n",
            "       0.56189889, 0.53532857, 0.50061339, 0.49794775, 0.48155454])\n",
            "Validation Accuracy Set:  array([0.3328    , 0.43879998, 0.23369999, 0.52059996, 0.35079998,\n",
            "       0.56079996, 0.55540001, 0.40369999, 0.57819998, 0.48319998,\n",
            "       0.59359998, 0.48199999, 0.49139997, 0.6257    , 0.55320001,\n",
            "       0.59539998, 0.5751    , 0.58270001, 0.5485    , 0.52679998,\n",
            "       0.53279996, 0.55329996, 0.37019998, 0.65619999, 0.61979997,\n",
            "       0.62519997, 0.5909    , 0.60879999, 0.52389997, 0.6591    ,\n",
            "       0.57819998, 0.63879997, 0.68439996, 0.59189999, 0.4418    ,\n",
            "       0.68970001, 0.54109997, 0.6056    , 0.67619997, 0.51629996,\n",
            "       0.70840001, 0.64019996, 0.58849996, 0.66639996, 0.66139996,\n",
            "       0.70059997, 0.74180001, 0.69629997, 0.70999998, 0.60299999,\n",
            "       0.68799996, 0.74109995, 0.76199996, 0.60710001, 0.65469998,\n",
            "       0.70379996, 0.69699997, 0.77590001, 0.7475    , 0.77129996,\n",
            "       0.60969996, 0.74449998, 0.6591    , 0.75769997, 0.72509998,\n",
            "       0.66229999, 0.6753    , 0.70089996, 0.72939998, 0.72719997,\n",
            "       0.72399998, 0.70899999, 0.70879996, 0.72889996, 0.73100001,\n",
            "       0.75199997, 0.64989996, 0.75999999, 0.77950001, 0.79549998,\n",
            "       0.7414    , 0.73399997, 0.72359997, 0.76550001, 0.76269996,\n",
            "       0.8028    , 0.81699997, 0.79189998, 0.75379997, 0.78709996,\n",
            "       0.75239998, 0.77359998, 0.79960001, 0.83569998, 0.84169996,\n",
            "       0.81399995, 0.79409999, 0.79439998, 0.74519998, 0.77379996,\n",
            "       0.83129996, 0.7536    , 0.76879996, 0.78499997, 0.78490001,\n",
            "       0.8168    , 0.79339999, 0.79079998, 0.81479996, 0.74619997,\n",
            "       0.77739996, 0.74449998, 0.77599996, 0.8161    , 0.80219996,\n",
            "       0.80919999, 0.82059997, 0.82549995, 0.83339995, 0.83819997])\n",
            "Test Loss Set:  array([1.80646062, 1.49461091, 3.34948468, 1.61990237, 2.44534111,\n",
            "       1.35324061, 1.26141644, 2.67190409, 1.20533919, 1.56957066,\n",
            "       1.21479428, 1.49359822, 1.4887495 , 1.09147072, 1.28641927,\n",
            "       1.28524613, 1.18578088, 1.26930737, 1.41208076, 1.66762745,\n",
            "       1.44575405, 1.5144136 , 3.15103221, 0.9667297 , 1.08334851,\n",
            "       1.01309741, 1.34350908, 1.37422395, 1.64784014, 1.07348871,\n",
            "       1.49413192, 1.02711213, 0.93975526, 1.50784814, 2.64408159,\n",
            "       0.94504106, 1.96167517, 1.1734376 , 1.0501616 , 1.98579645,\n",
            "       0.82971668, 1.06618285, 1.39204395, 1.03007996, 1.08641613,\n",
            "       0.96000749, 0.80575633, 0.95454252, 0.91730183, 1.31744874,\n",
            "       1.04923749, 0.70095038, 0.70138371, 1.27626026, 1.13230503,\n",
            "       0.94424999, 0.97144127, 0.70961219, 0.75948566, 0.686369  ,\n",
            "       1.30114269, 0.73789239, 1.14631152, 0.70972872, 0.88661516,\n",
            "       1.00001538, 0.94861317, 0.94737428, 0.78486478, 0.74667716,\n",
            "       0.80351865, 0.874071  , 0.92458421, 0.81814384, 0.73399168,\n",
            "       0.72474009, 1.13150156, 0.716717  , 0.61792511, 0.62335885,\n",
            "       0.8376072 , 0.89179212, 0.86634821, 0.65295482, 0.70569503,\n",
            "       0.56199485, 0.5682193 , 0.61573631, 0.72841734, 0.66310853,\n",
            "       0.75879401, 0.71755016, 0.62095988, 0.50446153, 0.45697245,\n",
            "       0.57699984, 0.61830425, 0.67246586, 0.81702852, 0.73237222,\n",
            "       0.49314895, 0.71838528, 0.70073241, 0.60956508, 0.63136923,\n",
            "       0.53167379, 0.60962433, 0.63218486, 0.53317827, 0.74996883,\n",
            "       0.7022354 , 0.82266325, 0.69184732, 0.52815533, 0.5890851 ,\n",
            "       0.56659091, 0.54695088, 0.48032787, 0.47146174, 0.45904446])\n",
            "Test Accuracy Set:  array([0.36609998, 0.45949998, 0.24599999, 0.5068    , 0.33759999,\n",
            "       0.56379998, 0.58789998, 0.39679998, 0.58709997, 0.5029    ,\n",
            "       0.58950001, 0.4799    , 0.49919999, 0.63749999, 0.574     ,\n",
            "       0.60429996, 0.5887    , 0.60899997, 0.55409998, 0.52810001,\n",
            "       0.55540001, 0.55949998, 0.37079999, 0.67129999, 0.62149996,\n",
            "       0.65020001, 0.59819996, 0.59729999, 0.52249998, 0.65859997,\n",
            "       0.57989997, 0.64209998, 0.68829995, 0.5826    , 0.41399997,\n",
            "       0.69379997, 0.53389996, 0.60460001, 0.6548    , 0.5011    ,\n",
            "       0.7191    , 0.63809997, 0.59029996, 0.65700001, 0.65549999,\n",
            "       0.71029997, 0.74619997, 0.70269996, 0.69549996, 0.59169996,\n",
            "       0.68399996, 0.76199996, 0.76489997, 0.61790001, 0.66249996,\n",
            "       0.6943    , 0.69      , 0.77679998, 0.75580001, 0.7723    ,\n",
            "       0.60389996, 0.74879998, 0.64789999, 0.76249999, 0.7342    ,\n",
            "       0.67619997, 0.68089998, 0.69209999, 0.73379999, 0.74489999,\n",
            "       0.72639996, 0.72119999, 0.70550001, 0.73210001, 0.7529    ,\n",
            "       0.76249999, 0.65799999, 0.75749999, 0.79350001, 0.792     ,\n",
            "       0.7482    , 0.72869998, 0.72670001, 0.77410001, 0.77239996,\n",
            "       0.81110001, 0.81239998, 0.79519999, 0.76190001, 0.77999997,\n",
            "       0.75940001, 0.773     , 0.79399997, 0.8348    , 0.84959996,\n",
            "       0.81209999, 0.79729998, 0.78319997, 0.73979998, 0.76440001,\n",
            "       0.83669996, 0.7604    , 0.76819998, 0.79469997, 0.79069996,\n",
            "       0.82409996, 0.79960001, 0.792     , 0.81989998, 0.75819999,\n",
            "       0.77599996, 0.74189997, 0.7809    , 0.82809997, 0.80239999,\n",
            "       0.81049997, 0.8204    , 0.83879995, 0.8441    , 0.84329998])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CtjtLoryWFzo"
      },
      "source": [
        "##ASGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pKyYq3s0WDYJ",
        "colab": {}
      },
      "source": [
        "def fixed_stats_ASGD(learning_rate, epochs, change):\n",
        "\n",
        "  #Load PreResnet44   \n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_ASGD = np.zeros(epochs)\n",
        "  train_accuracy_ASGD = np.zeros(epochs)\n",
        "  valid_loss_ASGD = np.zeros(epochs)\n",
        "  valid_accuracy_ASGD = np.zeros(epochs)\n",
        "  test_accuracy_ASGD = np.zeros(epochs)\n",
        "  test_loss_ASGD = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  i = 0\n",
        "\n",
        "  loss = 100\n",
        "  prev_loss = 100\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = AccSGD(model.parameters(),  lr = learning_rate[i], weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation loss reduced by more than 1% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_loss - loss)/prev_loss < 0.01 and i < 3 ) and epoch!=0:\n",
        "        i = i + 1\n",
        "    \n",
        "      prev_loss = loss\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    test_metric = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_ASGD[epoch] = result[0]['loss']\n",
        "    train_accuracy_ASGD[epoch] = result[0]['acc']\n",
        "    valid_loss_ASGD[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_ASGD[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_ASGD[epoch] = test_metric['test_acc']\n",
        "    test_loss_ASGD[epoch] = test_metric['test_loss']\n",
        "    loss = result[0]['loss']\n",
        "\n",
        "\n",
        "    print(epoch, learning_rate[i], result)\n",
        "\n",
        "  return train_loss_ASGD, train_accuracy_ASGD, valid_loss_ASGD, valid_accuracy_ASGD, test_accuracy_ASGD, test_loss_ASGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFH-fBNoWhPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "8dcf0be2-58d0-441f-fc5b-08c460bd97ef"
      },
      "source": [
        "epochs = 120\n",
        "learning_rate = [0.27,0.09,0.03,0.01]\n",
        "change = 4\n",
        "\n",
        "train_loss_ASGD, train_accuracy_ASGD, valid_loss_ASGD, valid_accuracy_ASGD, test_accuracy_ASGD, test_loss_ASGD = fixed_stats_ASGD(learning_rate, epochs, change)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 0.27 [{'running_loss': 1.3127774000167847, 'running_acc': 0.5267187356948853, 'loss': 1.5922354459762573, 'acc': 0.404449999332428, 'val_loss': 1.7998005151748657, 'val_acc': 0.4052000045776367, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.8110830783843994, 'test_acc': 0.4274999797344208}]\n",
            "1 0.27 [{'running_loss': 0.9457440972328186, 'running_acc': 0.6623437404632568, 'loss': 1.0909357070922852, 'acc': 0.6047750115394592, 'val_loss': 1.0841064453125, 'val_acc': 0.6294999718666077, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.0678107738494873, 'test_acc': 0.6394000053405762}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-4f407bb74aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_loss_ASGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_ASGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_ASGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracy_ASGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy_ASGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_ASGD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_stats_ASGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-66-98354512d9c8>\u001b[0m in \u001b[0;36mfixed_stats_ASGD\u001b[0;34m(learning_rate, epochs, change)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackListInjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprinter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_list_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epochs, verbose)\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_start_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m                 \u001b[0mfinal_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOP_TRAINING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36m_fit_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRIC_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/metrics.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mmaps\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mto\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_for_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/metrics.py\u001b[0m in \u001b[0;36m_for_list\u001b[0;34m(self, function)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/metrics.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(metric)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mmaps\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mnames\u001b[0m \u001b[0mto\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_for_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/default.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/metrics.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mall\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_for_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/metrics.py\u001b[0m in \u001b[0;36m_for_tree\u001b[0;34m(self, function, *args)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_for_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mnode_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mnode_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/metrics.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(metric, *in_args)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mall\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_for_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/metrics/primitives.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9ZlsZHLzW-D-"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y951XT1ghcCP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for ASGD on Batch Size 128 and Fixed Hyperparameter Schedule\n",
        "\n",
        "# train_loss_ASGD = np.array([1.57502866, 1.08047605, 0.83931744, 0.70710224, 0.62324679,\n",
        "#        0.57565814, 0.54190665, 0.51347351, 0.492708  , 0.47430858,\n",
        "#        0.46395254, 0.45068899, 0.43693274, 0.42682743, 0.41902897,\n",
        "#        0.4133673 , 0.40802956, 0.40162691, 0.39993197, 0.39076176,\n",
        "#        0.38590217, 0.3791635 , 0.38191262, 0.37959614, 0.37386909,\n",
        "#        0.37054104, 0.36804637, 0.36549455, 0.36893222, 0.36379108,\n",
        "#        0.36097518, 0.35650301, 0.35703403, 0.35469306, 0.35357648,\n",
        "#        0.3506127 , 0.34584132, 0.35085249, 0.34814525, 0.34627607,\n",
        "#        0.34425431, 0.34615329, 0.34411705, 0.33760685, 0.33527744,\n",
        "#        0.33758286, 0.33743998, 0.33387458, 0.33227101, 0.331954  ,\n",
        "#        0.33520362, 0.33210775, 0.33192831, 0.23566867, 0.19699085,\n",
        "#        0.1860801 , 0.17590736, 0.17827462, 0.17650944, 0.18101799,\n",
        "#        0.18508273, 0.18198381, 0.18263648, 0.18481809, 0.18527344,\n",
        "#        0.12596865, 0.09871442, 0.08699352, 0.08026865, 0.0771133 ,\n",
        "#        0.07526734, 0.07090999, 0.07075518, 0.06757583, 0.07207527,\n",
        "#        0.07035897, 0.06824516, 0.04695909, 0.03446003, 0.03203276,\n",
        "#        0.02821558, 0.02592759, 0.02474765, 0.02391301, 0.02380933,\n",
        "#        0.02387779, 0.02405758, 0.02158333, 0.02059282, 0.01946448,\n",
        "#        0.01982384, 0.02037095, 0.02027625, 0.01977126, 0.01945576,\n",
        "#        0.01755337, 0.01970425, 0.01877963, 0.01764652, 0.01741587,\n",
        "#        0.01815439, 0.01762113, 0.02011225, 0.01928076, 0.01920538,\n",
        "#        0.01839538, 0.01925996, 0.01649323, 0.01835576, 0.01751933,\n",
        "#        0.01763589, 0.01825207, 0.01900795, 0.01718848, 0.02005251,\n",
        "#        0.01889904, 0.02090671, 0.02060201, 0.0188938 , 0.01923043])\n",
        "# train_accuracy_ASGD = np.array([0.41235   , 0.61167496, 0.70034999, 0.75492495, 0.784325  ,\n",
        "#        0.801525  , 0.81329995, 0.82512498, 0.83032501, 0.83812499,\n",
        "#        0.83942497, 0.84497499, 0.85187501, 0.85439998, 0.85492498,\n",
        "#        0.85747498, 0.85922498, 0.86049998, 0.86157501, 0.86509997,\n",
        "#        0.86619997, 0.86902499, 0.8671    , 0.86954999, 0.87217498,\n",
        "#        0.87279999, 0.87442499, 0.87509996, 0.87102497, 0.875525  ,\n",
        "#        0.87522501, 0.87819999, 0.876975  , 0.87927496, 0.87797499,\n",
        "#        0.881675  , 0.88034999, 0.879875  , 0.88207495, 0.880575  ,\n",
        "#        0.88104999, 0.88084996, 0.88325   , 0.88314998, 0.884175  ,\n",
        "#        0.88412499, 0.88442498, 0.88669997, 0.88619995, 0.88545001,\n",
        "#        0.88507497, 0.88717496, 0.88519996, 0.919725  , 0.93282497,\n",
        "#        0.93544996, 0.94059998, 0.93867499, 0.93874997, 0.93669999,\n",
        "#        0.93614995, 0.93712497, 0.9357    , 0.93689996, 0.93629998,\n",
        "#        0.95702499, 0.96709996, 0.970725  , 0.97317499, 0.974675  ,\n",
        "#        0.97489995, 0.97607499, 0.97652495, 0.97737497, 0.975725  ,\n",
        "#        0.97665   , 0.97697496, 0.98539996, 0.990475  , 0.99072498,\n",
        "#        0.99217498, 0.99259996, 0.99264997, 0.99324995, 0.99355   ,\n",
        "#        0.99309999, 0.99312496, 0.99417496, 0.99427497, 0.99469995,\n",
        "#        0.99472499, 0.99429995, 0.99422497, 0.994775  , 0.99462497,\n",
        "#        0.99497497, 0.99449998, 0.99514997, 0.99524999, 0.99539995,\n",
        "#        0.99467498, 0.995125  , 0.99355   , 0.99495   , 0.99439996,\n",
        "#        0.99479997, 0.99462497, 0.99557495, 0.99487495, 0.99495   ,\n",
        "#        0.99514997, 0.99487495, 0.99479997, 0.99549997, 0.99437499,\n",
        "#        0.99467498, 0.99392498, 0.99422497, 0.99495   , 0.99439996])\n",
        "# valid_loss_ASGD = np.array([1.46583974, 1.23354197, 1.08254826, 0.8185817 , 0.7788468 ,\n",
        "#        1.02519   , 0.74150771, 0.74274802, 0.7866537 , 0.8249622 ,\n",
        "#        0.63211095, 0.69934589, 0.65626442, 0.76150686, 0.64836478,\n",
        "#        0.66487348, 0.93374634, 0.98789614, 0.62551278, 0.75095236,\n",
        "#        0.78465217, 0.76135117, 0.52006733, 0.93070203, 0.6332227 ,\n",
        "#        0.71169192, 0.58008122, 0.57202351, 0.83296585, 0.71796763,\n",
        "#        0.58713728, 0.56171948, 0.68513405, 0.56553513, 0.68973947,\n",
        "#        0.61124575, 0.73771405, 0.58883435, 0.52784973, 0.87101662,\n",
        "#        0.54460293, 0.59627664, 0.82017761, 0.55169111, 1.28622532,\n",
        "#        0.66134191, 0.48960096, 1.1763581 , 0.5582245 , 0.70251054,\n",
        "#        0.69469929, 1.08445001, 1.29479539, 0.37902948, 0.38963139,\n",
        "#        0.35672399, 0.37396815, 0.37456968, 0.43221587, 0.3992247 ,\n",
        "#        0.377087  , 0.40713936, 0.42934626, 0.55326587, 0.41925147,\n",
        "#        0.30817056, 0.29904464, 0.30005834, 0.32259378, 0.3487964 ,\n",
        "#        0.30299953, 0.33360675, 0.33702037, 0.3300083 , 0.372536  ,\n",
        "#        0.33741745, 0.35513806, 0.29806721, 0.30561498, 0.31093735,\n",
        "#        0.31528589, 0.31181982, 0.3227196 , 0.30917776, 0.31718075,\n",
        "#        0.31704375, 0.31731725, 0.31346449, 0.32573366, 0.32350081,\n",
        "#        0.3210949 , 0.34706071, 0.33822322, 0.34547469, 0.33871698,\n",
        "#        0.3435199 , 0.33470413, 0.33373737, 0.33954042, 0.34033367,\n",
        "#        0.32833239, 0.34188691, 0.3335934 , 0.34107026, 0.34771279,\n",
        "#        0.33034062, 0.34211659, 0.35153392, 0.3511821 , 0.34947702,\n",
        "#        0.33719692, 0.34113625, 0.36744103, 0.37346277, 0.35476425,\n",
        "#        0.35106736, 0.3768543 , 0.37565958, 0.36816606, 0.36399812])\n",
        "# valid_accuracy_ASGD = np.array([0.47999999, 0.57589996, 0.6275    , 0.71679997, 0.73710001,\n",
        "#        0.66149998, 0.74849999, 0.74869996, 0.73039997, 0.73369998,\n",
        "#        0.78139997, 0.76499999, 0.77579999, 0.75389999, 0.78609997,\n",
        "#        0.77859998, 0.70969999, 0.66319996, 0.79259998, 0.76599997,\n",
        "#        0.72979999, 0.75389999, 0.8272    , 0.72529995, 0.78819996,\n",
        "#        0.76699996, 0.80409998, 0.81110001, 0.74629998, 0.76429999,\n",
        "#        0.81029999, 0.80789995, 0.7755    , 0.80759996, 0.78319997,\n",
        "#        0.80419999, 0.76249999, 0.80909997, 0.82729995, 0.72600001,\n",
        "#        0.8179    , 0.80009997, 0.74539995, 0.8186    , 0.65990001,\n",
        "#        0.78909999, 0.8362    , 0.67909998, 0.81759995, 0.77509999,\n",
        "#        0.77019995, 0.66389996, 0.62      , 0.87379998, 0.87289995,\n",
        "#        0.88139999, 0.87779999, 0.8775    , 0.86069995, 0.86859995,\n",
        "#        0.87909997, 0.86939996, 0.86559999, 0.82799995, 0.86339998,\n",
        "#        0.90789998, 0.90689999, 0.90689999, 0.9023    , 0.89679998,\n",
        "#        0.90739995, 0.9016    , 0.90289998, 0.90389997, 0.89479995,\n",
        "#        0.90449995, 0.8998    , 0.91589999, 0.91459996, 0.91569996,\n",
        "#        0.91609997, 0.91649997, 0.91479999, 0.91619998, 0.91509998,\n",
        "#        0.91649997, 0.91789997, 0.9156    , 0.91729999, 0.91469997,\n",
        "#        0.91850001, 0.91349995, 0.91579998, 0.91159999, 0.91419995,\n",
        "#        0.91499996, 0.91549999, 0.91389996, 0.91359997, 0.91259998,\n",
        "#        0.91599995, 0.9149    , 0.91649997, 0.91819996, 0.91029996,\n",
        "#        0.91859996, 0.9149    , 0.9102    , 0.9131    , 0.91469997,\n",
        "#        0.917     , 0.91389996, 0.90939999, 0.91099995, 0.91339999,\n",
        "#        0.91529995, 0.9084    , 0.90999997, 0.91179997, 0.91399997])\n",
        "# test_loss_ASGD = np.array([1.46139193, 1.21413159, 1.10805047, 0.74879998, 0.77188402,\n",
        "#        0.99513149, 0.70713514, 0.77212697, 0.83759153, 0.85060853,\n",
        "#        0.5951727 , 0.69581479, 0.63281524, 0.77506745, 0.65221119,\n",
        "#        0.67705917, 0.90669918, 1.0313102 , 0.5849098 , 0.75658691,\n",
        "#        0.79608357, 0.77845639, 0.49612346, 1.06944406, 0.6177336 ,\n",
        "#        0.73577309, 0.58551687, 0.56342524, 0.85597241, 0.71903759,\n",
        "#        0.56711817, 0.58237475, 0.69439429, 0.5326944 , 0.7041024 ,\n",
        "#        0.60766888, 0.79497564, 0.59895575, 0.52964807, 0.89498442,\n",
        "#        0.55713117, 0.57192558, 0.7926206 , 0.55483192, 1.45654261,\n",
        "#        0.62488168, 0.50312793, 1.23297381, 0.55038971, 0.70428151,\n",
        "#        0.69985455, 1.15357351, 1.35589421, 0.39515859, 0.38142303,\n",
        "#        0.32790166, 0.36983508, 0.36312824, 0.43641996, 0.37237051,\n",
        "#        0.37876382, 0.39008579, 0.42174059, 0.56796187, 0.42740116,\n",
        "#        0.30250451, 0.29151767, 0.29923809, 0.3196137 , 0.33689249,\n",
        "#        0.30202195, 0.34147519, 0.3268688 , 0.31858599, 0.35254028,\n",
        "#        0.31463802, 0.34613952, 0.28974992, 0.28964278, 0.29920614,\n",
        "#        0.30635992, 0.29643509, 0.30677819, 0.30245423, 0.30754036,\n",
        "#        0.30951664, 0.31497204, 0.3140648 , 0.31069142, 0.31887639,\n",
        "#        0.32738465, 0.32737654, 0.33885276, 0.3231248 , 0.32783681,\n",
        "#        0.33542499, 0.32506323, 0.33135107, 0.33037889, 0.33710599,\n",
        "#        0.34647122, 0.34494409, 0.32964563, 0.33943632, 0.34047982,\n",
        "#        0.3264451 , 0.33166313, 0.34356833, 0.354011  , 0.34027556,\n",
        "#        0.33592778, 0.33934313, 0.36713305, 0.36459428, 0.34930056,\n",
        "#        0.33999878, 0.36094955, 0.35093698, 0.33811966, 0.34833971])\n",
        "# test_accuracy_ASGD = np.array([0.48819998, 0.59630001, 0.6275    , 0.7493    , 0.74599999,\n",
        "#        0.68489999, 0.75779998, 0.7457    , 0.72689998, 0.73859996,\n",
        "#        0.79909998, 0.77689999, 0.78799999, 0.7525    , 0.78969997,\n",
        "#        0.77899998, 0.72490001, 0.65849996, 0.80320001, 0.76599997,\n",
        "#        0.72670001, 0.75329995, 0.83050001, 0.70559996, 0.79289997,\n",
        "#        0.76809996, 0.80159998, 0.8179    , 0.74659997, 0.76550001,\n",
        "#        0.82389998, 0.81119996, 0.78009999, 0.8215    , 0.78579998,\n",
        "#        0.80879998, 0.76440001, 0.80919999, 0.82349998, 0.73859996,\n",
        "#        0.82429999, 0.80669999, 0.76339996, 0.82309997, 0.6469    ,\n",
        "#        0.80930001, 0.83609998, 0.6814    , 0.82489997, 0.7766    ,\n",
        "#        0.76669997, 0.65899998, 0.62379998, 0.87409997, 0.87939996,\n",
        "#        0.89300001, 0.88069999, 0.88429999, 0.86039996, 0.87779999,\n",
        "#        0.88150001, 0.87659997, 0.87369996, 0.83739996, 0.86849999,\n",
        "#        0.90869999, 0.9127    , 0.90999997, 0.90799999, 0.90439999,\n",
        "#        0.9127    , 0.90099996, 0.90559995, 0.91209996, 0.90079999,\n",
        "#        0.9091    , 0.90459996, 0.91889995, 0.92179996, 0.91869998,\n",
        "#        0.92069995, 0.92359996, 0.92189997, 0.92219996, 0.91909999,\n",
        "#        0.92139995, 0.92079997, 0.92309999, 0.92159998, 0.92329997,\n",
        "#        0.91969997, 0.92049998, 0.91939998, 0.92069995, 0.91839999,\n",
        "#        0.91909999, 0.91889995, 0.91959995, 0.92009997, 0.91979998,\n",
        "#        0.91859996, 0.91979998, 0.91909999, 0.9217    , 0.91759998,\n",
        "#        0.92139995, 0.91999996, 0.91869998, 0.91729999, 0.9206    ,\n",
        "#        0.9206    , 0.92109996, 0.91479999, 0.91679996, 0.9174    ,\n",
        "#        0.9174    , 0.9145    , 0.91779995, 0.92029995, 0.9181    ])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjwQcjKaXR3i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36828ba5-7ba5-45e9-d744-f54bc5165820"
      },
      "source": [
        "print(\"Training Loss Set: \", repr(train_loss_ASGD))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_ASGD))\n",
        "print(\"Validation Loss Set: \",repr(valid_loss_ASGD))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_ASGD))\n",
        "print(\"Test Loss Set: \",repr(test_loss_ASGD))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_ASGD))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss Set:  array([1.57502866, 1.08047605, 0.83931744, 0.70710224, 0.62324679,\n",
            "       0.57565814, 0.54190665, 0.51347351, 0.492708  , 0.47430858,\n",
            "       0.46395254, 0.45068899, 0.43693274, 0.42682743, 0.41902897,\n",
            "       0.4133673 , 0.40802956, 0.40162691, 0.39993197, 0.39076176,\n",
            "       0.38590217, 0.3791635 , 0.38191262, 0.37959614, 0.37386909,\n",
            "       0.37054104, 0.36804637, 0.36549455, 0.36893222, 0.36379108,\n",
            "       0.36097518, 0.35650301, 0.35703403, 0.35469306, 0.35357648,\n",
            "       0.3506127 , 0.34584132, 0.35085249, 0.34814525, 0.34627607,\n",
            "       0.34425431, 0.34615329, 0.34411705, 0.33760685, 0.33527744,\n",
            "       0.33758286, 0.33743998, 0.33387458, 0.33227101, 0.331954  ,\n",
            "       0.33520362, 0.33210775, 0.33192831, 0.23566867, 0.19699085,\n",
            "       0.1860801 , 0.17590736, 0.17827462, 0.17650944, 0.18101799,\n",
            "       0.18508273, 0.18198381, 0.18263648, 0.18481809, 0.18527344,\n",
            "       0.12596865, 0.09871442, 0.08699352, 0.08026865, 0.0771133 ,\n",
            "       0.07526734, 0.07090999, 0.07075518, 0.06757583, 0.07207527,\n",
            "       0.07035897, 0.06824516, 0.04695909, 0.03446003, 0.03203276,\n",
            "       0.02821558, 0.02592759, 0.02474765, 0.02391301, 0.02380933,\n",
            "       0.02387779, 0.02405758, 0.02158333, 0.02059282, 0.01946448,\n",
            "       0.01982384, 0.02037095, 0.02027625, 0.01977126, 0.01945576,\n",
            "       0.01755337, 0.01970425, 0.01877963, 0.01764652, 0.01741587,\n",
            "       0.01815439, 0.01762113, 0.02011225, 0.01928076, 0.01920538,\n",
            "       0.01839538, 0.01925996, 0.01649323, 0.01835576, 0.01751933,\n",
            "       0.01763589, 0.01825207, 0.01900795, 0.01718848, 0.02005251,\n",
            "       0.01889904, 0.02090671, 0.02060201, 0.0188938 , 0.01923043])\n",
            "Training Accuracy Set:  array([0.41235   , 0.61167496, 0.70034999, 0.75492495, 0.784325  ,\n",
            "       0.801525  , 0.81329995, 0.82512498, 0.83032501, 0.83812499,\n",
            "       0.83942497, 0.84497499, 0.85187501, 0.85439998, 0.85492498,\n",
            "       0.85747498, 0.85922498, 0.86049998, 0.86157501, 0.86509997,\n",
            "       0.86619997, 0.86902499, 0.8671    , 0.86954999, 0.87217498,\n",
            "       0.87279999, 0.87442499, 0.87509996, 0.87102497, 0.875525  ,\n",
            "       0.87522501, 0.87819999, 0.876975  , 0.87927496, 0.87797499,\n",
            "       0.881675  , 0.88034999, 0.879875  , 0.88207495, 0.880575  ,\n",
            "       0.88104999, 0.88084996, 0.88325   , 0.88314998, 0.884175  ,\n",
            "       0.88412499, 0.88442498, 0.88669997, 0.88619995, 0.88545001,\n",
            "       0.88507497, 0.88717496, 0.88519996, 0.919725  , 0.93282497,\n",
            "       0.93544996, 0.94059998, 0.93867499, 0.93874997, 0.93669999,\n",
            "       0.93614995, 0.93712497, 0.9357    , 0.93689996, 0.93629998,\n",
            "       0.95702499, 0.96709996, 0.970725  , 0.97317499, 0.974675  ,\n",
            "       0.97489995, 0.97607499, 0.97652495, 0.97737497, 0.975725  ,\n",
            "       0.97665   , 0.97697496, 0.98539996, 0.990475  , 0.99072498,\n",
            "       0.99217498, 0.99259996, 0.99264997, 0.99324995, 0.99355   ,\n",
            "       0.99309999, 0.99312496, 0.99417496, 0.99427497, 0.99469995,\n",
            "       0.99472499, 0.99429995, 0.99422497, 0.994775  , 0.99462497,\n",
            "       0.99497497, 0.99449998, 0.99514997, 0.99524999, 0.99539995,\n",
            "       0.99467498, 0.995125  , 0.99355   , 0.99495   , 0.99439996,\n",
            "       0.99479997, 0.99462497, 0.99557495, 0.99487495, 0.99495   ,\n",
            "       0.99514997, 0.99487495, 0.99479997, 0.99549997, 0.99437499,\n",
            "       0.99467498, 0.99392498, 0.99422497, 0.99495   , 0.99439996])\n",
            "Validation Loss Set:  array([1.46583974, 1.23354197, 1.08254826, 0.8185817 , 0.7788468 ,\n",
            "       1.02519   , 0.74150771, 0.74274802, 0.7866537 , 0.8249622 ,\n",
            "       0.63211095, 0.69934589, 0.65626442, 0.76150686, 0.64836478,\n",
            "       0.66487348, 0.93374634, 0.98789614, 0.62551278, 0.75095236,\n",
            "       0.78465217, 0.76135117, 0.52006733, 0.93070203, 0.6332227 ,\n",
            "       0.71169192, 0.58008122, 0.57202351, 0.83296585, 0.71796763,\n",
            "       0.58713728, 0.56171948, 0.68513405, 0.56553513, 0.68973947,\n",
            "       0.61124575, 0.73771405, 0.58883435, 0.52784973, 0.87101662,\n",
            "       0.54460293, 0.59627664, 0.82017761, 0.55169111, 1.28622532,\n",
            "       0.66134191, 0.48960096, 1.1763581 , 0.5582245 , 0.70251054,\n",
            "       0.69469929, 1.08445001, 1.29479539, 0.37902948, 0.38963139,\n",
            "       0.35672399, 0.37396815, 0.37456968, 0.43221587, 0.3992247 ,\n",
            "       0.377087  , 0.40713936, 0.42934626, 0.55326587, 0.41925147,\n",
            "       0.30817056, 0.29904464, 0.30005834, 0.32259378, 0.3487964 ,\n",
            "       0.30299953, 0.33360675, 0.33702037, 0.3300083 , 0.372536  ,\n",
            "       0.33741745, 0.35513806, 0.29806721, 0.30561498, 0.31093735,\n",
            "       0.31528589, 0.31181982, 0.3227196 , 0.30917776, 0.31718075,\n",
            "       0.31704375, 0.31731725, 0.31346449, 0.32573366, 0.32350081,\n",
            "       0.3210949 , 0.34706071, 0.33822322, 0.34547469, 0.33871698,\n",
            "       0.3435199 , 0.33470413, 0.33373737, 0.33954042, 0.34033367,\n",
            "       0.32833239, 0.34188691, 0.3335934 , 0.34107026, 0.34771279,\n",
            "       0.33034062, 0.34211659, 0.35153392, 0.3511821 , 0.34947702,\n",
            "       0.33719692, 0.34113625, 0.36744103, 0.37346277, 0.35476425,\n",
            "       0.35106736, 0.3768543 , 0.37565958, 0.36816606, 0.36399812])\n",
            "Validation Accuracy Set:  array([0.47999999, 0.57589996, 0.6275    , 0.71679997, 0.73710001,\n",
            "       0.66149998, 0.74849999, 0.74869996, 0.73039997, 0.73369998,\n",
            "       0.78139997, 0.76499999, 0.77579999, 0.75389999, 0.78609997,\n",
            "       0.77859998, 0.70969999, 0.66319996, 0.79259998, 0.76599997,\n",
            "       0.72979999, 0.75389999, 0.8272    , 0.72529995, 0.78819996,\n",
            "       0.76699996, 0.80409998, 0.81110001, 0.74629998, 0.76429999,\n",
            "       0.81029999, 0.80789995, 0.7755    , 0.80759996, 0.78319997,\n",
            "       0.80419999, 0.76249999, 0.80909997, 0.82729995, 0.72600001,\n",
            "       0.8179    , 0.80009997, 0.74539995, 0.8186    , 0.65990001,\n",
            "       0.78909999, 0.8362    , 0.67909998, 0.81759995, 0.77509999,\n",
            "       0.77019995, 0.66389996, 0.62      , 0.87379998, 0.87289995,\n",
            "       0.88139999, 0.87779999, 0.8775    , 0.86069995, 0.86859995,\n",
            "       0.87909997, 0.86939996, 0.86559999, 0.82799995, 0.86339998,\n",
            "       0.90789998, 0.90689999, 0.90689999, 0.9023    , 0.89679998,\n",
            "       0.90739995, 0.9016    , 0.90289998, 0.90389997, 0.89479995,\n",
            "       0.90449995, 0.8998    , 0.91589999, 0.91459996, 0.91569996,\n",
            "       0.91609997, 0.91649997, 0.91479999, 0.91619998, 0.91509998,\n",
            "       0.91649997, 0.91789997, 0.9156    , 0.91729999, 0.91469997,\n",
            "       0.91850001, 0.91349995, 0.91579998, 0.91159999, 0.91419995,\n",
            "       0.91499996, 0.91549999, 0.91389996, 0.91359997, 0.91259998,\n",
            "       0.91599995, 0.9149    , 0.91649997, 0.91819996, 0.91029996,\n",
            "       0.91859996, 0.9149    , 0.9102    , 0.9131    , 0.91469997,\n",
            "       0.917     , 0.91389996, 0.90939999, 0.91099995, 0.91339999,\n",
            "       0.91529995, 0.9084    , 0.90999997, 0.91179997, 0.91399997])\n",
            "Test Loss Set:  array([1.46139193, 1.21413159, 1.10805047, 0.74879998, 0.77188402,\n",
            "       0.99513149, 0.70713514, 0.77212697, 0.83759153, 0.85060853,\n",
            "       0.5951727 , 0.69581479, 0.63281524, 0.77506745, 0.65221119,\n",
            "       0.67705917, 0.90669918, 1.0313102 , 0.5849098 , 0.75658691,\n",
            "       0.79608357, 0.77845639, 0.49612346, 1.06944406, 0.6177336 ,\n",
            "       0.73577309, 0.58551687, 0.56342524, 0.85597241, 0.71903759,\n",
            "       0.56711817, 0.58237475, 0.69439429, 0.5326944 , 0.7041024 ,\n",
            "       0.60766888, 0.79497564, 0.59895575, 0.52964807, 0.89498442,\n",
            "       0.55713117, 0.57192558, 0.7926206 , 0.55483192, 1.45654261,\n",
            "       0.62488168, 0.50312793, 1.23297381, 0.55038971, 0.70428151,\n",
            "       0.69985455, 1.15357351, 1.35589421, 0.39515859, 0.38142303,\n",
            "       0.32790166, 0.36983508, 0.36312824, 0.43641996, 0.37237051,\n",
            "       0.37876382, 0.39008579, 0.42174059, 0.56796187, 0.42740116,\n",
            "       0.30250451, 0.29151767, 0.29923809, 0.3196137 , 0.33689249,\n",
            "       0.30202195, 0.34147519, 0.3268688 , 0.31858599, 0.35254028,\n",
            "       0.31463802, 0.34613952, 0.28974992, 0.28964278, 0.29920614,\n",
            "       0.30635992, 0.29643509, 0.30677819, 0.30245423, 0.30754036,\n",
            "       0.30951664, 0.31497204, 0.3140648 , 0.31069142, 0.31887639,\n",
            "       0.32738465, 0.32737654, 0.33885276, 0.3231248 , 0.32783681,\n",
            "       0.33542499, 0.32506323, 0.33135107, 0.33037889, 0.33710599,\n",
            "       0.34647122, 0.34494409, 0.32964563, 0.33943632, 0.34047982,\n",
            "       0.3264451 , 0.33166313, 0.34356833, 0.354011  , 0.34027556,\n",
            "       0.33592778, 0.33934313, 0.36713305, 0.36459428, 0.34930056,\n",
            "       0.33999878, 0.36094955, 0.35093698, 0.33811966, 0.34833971])\n",
            "Test Accuracy Set:  array([0.48819998, 0.59630001, 0.6275    , 0.7493    , 0.74599999,\n",
            "       0.68489999, 0.75779998, 0.7457    , 0.72689998, 0.73859996,\n",
            "       0.79909998, 0.77689999, 0.78799999, 0.7525    , 0.78969997,\n",
            "       0.77899998, 0.72490001, 0.65849996, 0.80320001, 0.76599997,\n",
            "       0.72670001, 0.75329995, 0.83050001, 0.70559996, 0.79289997,\n",
            "       0.76809996, 0.80159998, 0.8179    , 0.74659997, 0.76550001,\n",
            "       0.82389998, 0.81119996, 0.78009999, 0.8215    , 0.78579998,\n",
            "       0.80879998, 0.76440001, 0.80919999, 0.82349998, 0.73859996,\n",
            "       0.82429999, 0.80669999, 0.76339996, 0.82309997, 0.6469    ,\n",
            "       0.80930001, 0.83609998, 0.6814    , 0.82489997, 0.7766    ,\n",
            "       0.76669997, 0.65899998, 0.62379998, 0.87409997, 0.87939996,\n",
            "       0.89300001, 0.88069999, 0.88429999, 0.86039996, 0.87779999,\n",
            "       0.88150001, 0.87659997, 0.87369996, 0.83739996, 0.86849999,\n",
            "       0.90869999, 0.9127    , 0.90999997, 0.90799999, 0.90439999,\n",
            "       0.9127    , 0.90099996, 0.90559995, 0.91209996, 0.90079999,\n",
            "       0.9091    , 0.90459996, 0.91889995, 0.92179996, 0.91869998,\n",
            "       0.92069995, 0.92359996, 0.92189997, 0.92219996, 0.91909999,\n",
            "       0.92139995, 0.92079997, 0.92309999, 0.92159998, 0.92329997,\n",
            "       0.91969997, 0.92049998, 0.91939998, 0.92069995, 0.91839999,\n",
            "       0.91909999, 0.91889995, 0.91959995, 0.92009997, 0.91979998,\n",
            "       0.91859996, 0.91979998, 0.91909999, 0.9217    , 0.91759998,\n",
            "       0.92139995, 0.91999996, 0.91869998, 0.91729999, 0.9206    ,\n",
            "       0.9206    , 0.92109996, 0.91479999, 0.91679996, 0.9174    ,\n",
            "       0.9174    , 0.9145    , 0.91779995, 0.92029995, 0.9181    ])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p8QnVWI0XXhW"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3_D-Y8-mXWPC",
        "colab": {}
      },
      "source": [
        "def fixed_stats_Adam(learning_rate, epochs, change):\n",
        "\n",
        "  #Load PreResnet44\n",
        "  model = Resnet(ResnetBlock, 44, num_classes)\n",
        "  model.eval()\n",
        "\n",
        "  #Initialise all the metrics to be saved\n",
        "  train_loss_Adam = np.zeros(epochs)\n",
        "  train_accuracy_Adam = np.zeros(epochs)\n",
        "  valid_loss_Adam = np.zeros(epochs)\n",
        "  valid_accuracy_Adam = np.zeros(epochs)\n",
        "  test_accuracy_Adam = np.zeros(epochs)\n",
        "  test_loss_Adam = np.zeros(epochs)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  device = \"cuda:0\" \n",
        "\n",
        "  i = 0\n",
        "\n",
        "  loss = 100\n",
        "  prev_loss = 100\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    model = copy.deepcopy(model)\n",
        "    optimiser = AccSGD(model.parameters(),  lr = learning_rate[i], weight_decay=0.0005)\n",
        "\n",
        "    #Train the model on training data\n",
        "    trial = Trial(model, optimiser, loss_function, metrics=['loss', 'accuracy'], verbose=0).to(device)\n",
        "    trial.with_generators(train_loader, valid_loader, test_generator=test_loader)\n",
        "\n",
        "    result = trial.run(epochs=1)\n",
        "\n",
        "    #At change points, update the hyperparameters depending on whether validation loss reduced by more than 1% or not\n",
        "    if epoch%change == 0:\n",
        "      if ((prev_loss - loss)/prev_loss < 0.01 and i < 3 ) and epoch!=0:\n",
        "        i = i + 1\n",
        "    \n",
        "      prev_loss = loss\n",
        "\n",
        "    #Compute the metrics on Test Dataset\n",
        "    test_metric = trial.evaluate(data_key=torchbearer.TEST_DATA)\n",
        "\n",
        "    #Store the metrics at each epoch\n",
        "    train_loss_Adam[epoch] = result[0]['loss']\n",
        "    train_accuracy_Adam[epoch] = result[0]['acc']\n",
        "    valid_loss_Adam[epoch] = result[0]['val_loss']\n",
        "    valid_accuracy_Adam[epoch] = result[0]['val_acc']\n",
        "    test_accuracy_Adam[epoch] = test_metric['test_acc']\n",
        "    test_loss_Adam[epoch] = test_metric['test_loss']\n",
        "    loss = result[0]['loss']\n",
        "\n",
        "    print(epoch, learning_rate[i], result)\n",
        "\n",
        "  return train_loss_Adam, train_accuracy_Adam, valid_loss_Adam, valid_accuracy_Adam, test_accuracy_Adam, test_loss_Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4oCNbRSTXtu6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "b913a712-46f8-4615-daa1-94970bd16cf0"
      },
      "source": [
        "epochs = 120\n",
        "learning_rate = [0.27,0.09,0.03,0.01]\n",
        "change = 4\n",
        "\n",
        "train_loss_Adam, train_accuracy_Adam, valid_loss_Adam, valid_accuracy_Adam, test_accuracy_Adam, test_loss_Adam = fixed_stats_Adam(learning_rate, epochs, change)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.27 [{'running_loss': 1.3452744483947754, 'running_acc': 0.510937511920929, 'loss': 1.6168323755264282, 'acc': 0.39274999499320984, 'val_loss': 1.4993964433670044, 'val_acc': 0.43359997868537903, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.4483208656311035, 'test_acc': 0.44589999318122864}]\n",
            "1 0.27 [{'running_loss': 0.992431104183197, 'running_acc': 0.6479687094688416, 'loss': 1.1320232152938843, 'acc': 0.5915749669075012, 'val_loss': 1.1816471815109253, 'val_acc': 0.5882999897003174, 'train_steps': 313, 'validation_steps': 79, 'test_loss': 1.2078760862350464, 'test_acc': 0.5924999713897705}]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-bed22dc62457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_loss_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accuracy_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy_Adam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_stats_Adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-94944ded08a2>\u001b[0m in \u001b[0;36mfixed_stats_Adam\u001b[0;34m(learning_rate, epochs, change)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackListInjection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprinter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback_list_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, epochs, verbose)\u001b[0m\n\u001b[1;32m    991\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m                 \u001b[0mfinal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_end_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36m_validation_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36m_test_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1058\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCALLBACK_LIST\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_sample_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchbearer/trial.py\u001b[0m in \u001b[0;36mload_batch_standard\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mstate\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[0;32m--> 214\u001b[0;31m     state[torchbearer.X], state[torchbearer.Y_TRUE] = deep_to(next(state[torchbearer.ITERATOR]),\n\u001b[0m\u001b[1;32m    215\u001b[0m                                                               \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorchbearer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                                                               state[torchbearer.DATA_TYPE])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2699\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2701\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2639\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_getdecoder\u001b[0;34m(mode, decoder_name, args, extra)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# get decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_decoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVUsrlOlX-fq"
      },
      "source": [
        "###Load Precomputed Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0g7bcGxhxkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Uncomment the below lines to restore the pre computed metrics for Adam on Batch Size 128 and Fixed Hyperparameter Schedule\n",
        "\n",
        "# train_loss_Adam = np.array([1.63098025, 1.18482816, 0.92434663, 0.74880612, 0.64766753,\n",
        "#        0.5827806 , 0.5433141 , 0.5113861 , 0.49023631, 0.47311229,\n",
        "#        0.45701393, 0.44433579, 0.42935544, 0.41854614, 0.41153285,\n",
        "#        0.40299255, 0.40412036, 0.39132437, 0.38328621, 0.38124898,\n",
        "#        0.38206223, 0.37248716, 0.37170446, 0.36176464, 0.36307919,\n",
        "#        0.35584307, 0.35662925, 0.35808343, 0.35688946, 0.35222122,\n",
        "#        0.35235897, 0.34545928, 0.34290826, 0.34415391, 0.34525529,\n",
        "#        0.34324637, 0.34100187, 0.24085355, 0.20217335, 0.19170846,\n",
        "#        0.18723124, 0.18384106, 0.18559903, 0.18549526, 0.1880773 ,\n",
        "#        0.18568741, 0.19353676, 0.18947835, 0.18815568, 0.12596726,\n",
        "#        0.09945331, 0.08728779, 0.08528612, 0.0803453 , 0.07978326,\n",
        "#        0.07399344, 0.07425223, 0.07360058, 0.06891334, 0.07226951,\n",
        "#        0.07116149, 0.07237988, 0.07525628, 0.07291701, 0.07637273,\n",
        "#        0.04518639, 0.03597797, 0.02885379, 0.02776799, 0.02636736,\n",
        "#        0.0241129 , 0.02403767, 0.02100205, 0.02180966, 0.0211378 ,\n",
        "#        0.02147923, 0.02042247, 0.01871116, 0.01923213, 0.01841849,\n",
        "#        0.01910133, 0.01860956, 0.01646179, 0.01753278, 0.01750859,\n",
        "#        0.01722397, 0.01696129, 0.01845689, 0.01938996, 0.01684781,\n",
        "#        0.01646712, 0.01780239, 0.01903388, 0.01713811, 0.01796572,\n",
        "#        0.01855857, 0.01591437, 0.01919276, 0.01785485, 0.01932137,\n",
        "#        0.02023486, 0.02096085, 0.02076473, 0.02075142, 0.02226698,\n",
        "#        0.01900789, 0.0189402 , 0.01822328, 0.02132328, 0.02087948,\n",
        "#        0.02004796, 0.01892515, 0.01979257, 0.02133206, 0.02405019,\n",
        "#        0.0225936 , 0.02262349, 0.02339199, 0.02365335, 0.02124777])\n",
        "# train_accuracy_Adam = np.array([0.38857499, 0.57124996, 0.67167497, 0.74114996, 0.77442497,\n",
        "#        0.79892498, 0.81364995, 0.82299995, 0.82962495, 0.838525  ,\n",
        "#        0.843225  , 0.84722495, 0.85049999, 0.85492498, 0.85877496,\n",
        "#        0.86052495, 0.86087495, 0.86412495, 0.86857498, 0.86739999,\n",
        "#        0.86750001, 0.87159997, 0.87229997, 0.87579995, 0.87360001,\n",
        "#        0.87554997, 0.87667495, 0.87689996, 0.87649995, 0.878025  ,\n",
        "#        0.87717497, 0.88169998, 0.88134998, 0.88264996, 0.88209999,\n",
        "#        0.88185   , 0.88339996, 0.91747499, 0.93107498, 0.93362498,\n",
        "#        0.93682498, 0.93667495, 0.93605   , 0.93677497, 0.93489999,\n",
        "#        0.93562496, 0.93334997, 0.93454999, 0.93392497, 0.95795   ,\n",
        "#        0.96672499, 0.97094995, 0.97097498, 0.973975  , 0.97317499,\n",
        "#        0.97532499, 0.97474998, 0.97555   , 0.97659999, 0.97557497,\n",
        "#        0.97607499, 0.97584999, 0.97522497, 0.97555   , 0.97442496,\n",
        "#        0.98597497, 0.989375  , 0.99207497, 0.99245   , 0.99204999,\n",
        "#        0.99322498, 0.99299997, 0.99444997, 0.99432498, 0.99409997,\n",
        "#        0.99404997, 0.99422497, 0.99527496, 0.99479997, 0.99469995,\n",
        "#        0.99502498, 0.99502498, 0.99584997, 0.995525  , 0.99549997,\n",
        "#        0.99562496, 0.995875  , 0.99482495, 0.99427497, 0.99577498,\n",
        "#        0.99564999, 0.995125  , 0.99472499, 0.99539995, 0.99524999,\n",
        "#        0.99519998, 0.99589998, 0.99502498, 0.99497497, 0.99454999,\n",
        "#        0.99427497, 0.99427497, 0.99369997, 0.99382496, 0.99307495,\n",
        "#        0.99495   , 0.99474996, 0.99495   , 0.99392498, 0.99369997,\n",
        "#        0.99384999, 0.99474996, 0.99419999, 0.99374998, 0.99282497,\n",
        "#        0.9932    , 0.993325  , 0.99304998, 0.99304998, 0.99384999])\n",
        "# valid_loss_Adam = np.array([1.55783594, 1.52161407, 1.24889517, 0.84182656, 0.89323217,\n",
        "#        0.713413  , 0.76559377, 0.78031182, 0.90696895, 0.85179651,\n",
        "#        0.58523309, 0.70254719, 0.74266708, 0.64539486, 0.56975979,\n",
        "#        0.80132061, 0.70561922, 0.86596519, 0.84709626, 0.64653951,\n",
        "#        0.58040065, 0.67281491, 0.79712498, 0.61198562, 0.56978834,\n",
        "#        0.68041283, 0.69623721, 0.5825749 , 0.60563886, 0.83218521,\n",
        "#        0.54149675, 0.8593418 , 0.66787356, 0.71644032, 1.0116775 ,\n",
        "#        0.59645993, 0.63205767, 0.3847951 , 0.36289248, 0.34446841,\n",
        "#        0.38228068, 0.35512629, 0.38351691, 0.38820621, 0.40156859,\n",
        "#        0.43998909, 0.44266936, 0.42113611, 0.38563371, 0.29343411,\n",
        "#        0.29864109, 0.31735253, 0.31350642, 0.32177123, 0.3160539 ,\n",
        "#        0.32784128, 0.33763063, 0.30781922, 0.34939167, 0.34976146,\n",
        "#        0.33153215, 0.34650236, 0.36840552, 0.35023317, 0.34985146,\n",
        "#        0.32229218, 0.30714357, 0.31356347, 0.31345487, 0.30014107,\n",
        "#        0.31970268, 0.30777767, 0.31078839, 0.32217506, 0.31684774,\n",
        "#        0.33113995, 0.31970516, 0.33407909, 0.31244326, 0.3378734 ,\n",
        "#        0.3417294 , 0.33023912, 0.33285326, 0.3220641 , 0.33516982,\n",
        "#        0.31850144, 0.33705434, 0.33034986, 0.33400241, 0.32248801,\n",
        "#        0.33533341, 0.35160601, 0.36361852, 0.35978967, 0.34152481,\n",
        "#        0.35819933, 0.37667987, 0.3420006 , 0.38998914, 0.36176947,\n",
        "#        0.34866029, 0.3597441 , 0.37947235, 0.34622636, 0.43591633,\n",
        "#        0.36546731, 0.33320844, 0.36168158, 0.35066411, 0.35977909,\n",
        "#        0.37483329, 0.35610616, 0.32663047, 0.3508074 , 0.34853786,\n",
        "#        0.34734043, 0.34473416, 0.35619226, 0.35254195, 0.35536626])\n",
        "# valid_accuracy_Adam = np.array([0.44739997, 0.50139999, 0.57029998, 0.70229995, 0.70199996,\n",
        "#        0.75139999, 0.74360001, 0.74329996, 0.71129996, 0.71989995,\n",
        "#        0.79609996, 0.7701    , 0.75339997, 0.7863    , 0.81239998,\n",
        "#        0.74299997, 0.75650001, 0.72469997, 0.7342    , 0.78549999,\n",
        "#        0.79890001, 0.77859998, 0.74049997, 0.78999996, 0.80219996,\n",
        "#        0.77239996, 0.76889998, 0.80409998, 0.79949999, 0.72789997,\n",
        "#        0.81589997, 0.7306    , 0.78539997, 0.77169997, 0.68329996,\n",
        "#        0.79530001, 0.78889996, 0.8725    , 0.88189995, 0.88739997,\n",
        "#        0.87329996, 0.88229996, 0.87349999, 0.87269998, 0.87339997,\n",
        "#        0.86439997, 0.85549998, 0.86799997, 0.87599999, 0.90329999,\n",
        "#        0.9059    , 0.9034    , 0.90569997, 0.90569997, 0.90359998,\n",
        "#        0.90639997, 0.90019995, 0.9077    , 0.89719999, 0.8976    ,\n",
        "#        0.9023    , 0.89929998, 0.8919    , 0.89789999, 0.89959997,\n",
        "#        0.90679997, 0.91399997, 0.91219997, 0.91399997, 0.91579998,\n",
        "#        0.9145    , 0.91569996, 0.91609997, 0.91419995, 0.9149    ,\n",
        "#        0.91549999, 0.91429996, 0.91670001, 0.9192    , 0.9152    ,\n",
        "#        0.91319996, 0.91429996, 0.91679996, 0.91509998, 0.91349995,\n",
        "#        0.91859996, 0.91429996, 0.9152    , 0.91279995, 0.9163    ,\n",
        "#        0.91459996, 0.91329998, 0.91029996, 0.90979999, 0.9138    ,\n",
        "#        0.90959996, 0.91029996, 0.91479999, 0.90349996, 0.91009998,\n",
        "#        0.9145    , 0.91149998, 0.90359998, 0.90989995, 0.8973    ,\n",
        "#        0.91139996, 0.91289997, 0.91159999, 0.90799999, 0.9138    ,\n",
        "#        0.90849996, 0.9109    , 0.91589999, 0.91289997, 0.91189998,\n",
        "#        0.90809995, 0.9156    , 0.91109997, 0.91099995, 0.91209996])\n",
        "# test_loss_Adam = np.array([1.50516856, 1.51188815, 1.29473245, 0.8595646 , 0.90460736,\n",
        "#        0.70080239, 0.75703222, 0.81455696, 0.93434656, 0.8720606 ,\n",
        "#        0.56318939, 0.71076798, 0.75184232, 0.64574844, 0.55569506,\n",
        "#        0.82789201, 0.70006621, 0.96869075, 0.86290318, 0.67974633,\n",
        "#        0.55451965, 0.67770076, 0.87911534, 0.62351274, 0.58336341,\n",
        "#        0.67355037, 0.73602015, 0.56763124, 0.61525607, 0.86475915,\n",
        "#        0.55456936, 0.90860075, 0.67895633, 0.74933088, 1.03419423,\n",
        "#        0.62102193, 0.66888767, 0.37894154, 0.35685366, 0.35062769,\n",
        "#        0.39061579, 0.3644751 , 0.37589926, 0.3657887 , 0.40115267,\n",
        "#        0.45404416, 0.46673962, 0.42376262, 0.3899346 , 0.31059083,\n",
        "#        0.29400316, 0.32465661, 0.31765386, 0.33160627, 0.3220908 ,\n",
        "#        0.33999759, 0.336647  , 0.31827852, 0.37499851, 0.36277896,\n",
        "#        0.35109949, 0.34594384, 0.3643299 , 0.34833032, 0.36226416,\n",
        "#        0.3153626 , 0.30994228, 0.32804713, 0.32099718, 0.32161883,\n",
        "#        0.32669014, 0.32039464, 0.32662642, 0.32537889, 0.33101076,\n",
        "#        0.32695094, 0.33046752, 0.33126917, 0.32301405, 0.33908272,\n",
        "#        0.33412474, 0.32873991, 0.34328038, 0.34317434, 0.35596618,\n",
        "#        0.33259258, 0.34262055, 0.35036722, 0.34168521, 0.34740785,\n",
        "#        0.35626334, 0.36490032, 0.37679493, 0.38247281, 0.35339424,\n",
        "#        0.36530986, 0.38333189, 0.35091412, 0.41433379, 0.36458987,\n",
        "#        0.35078478, 0.35189176, 0.3810997 , 0.35155326, 0.45320579,\n",
        "#        0.36502641, 0.37128952, 0.3714872 , 0.37667093, 0.35723549,\n",
        "#        0.3733938 , 0.38295144, 0.35331589, 0.37274614, 0.37023613,\n",
        "#        0.38723809, 0.36336458, 0.36656645, 0.36860529, 0.35888502])\n",
        "# test_accuracy_Adam = np.array([0.46829998, 0.523     , 0.57339996, 0.70359999, 0.70809996,\n",
        "#        0.7518    , 0.74579996, 0.74419999, 0.71779996, 0.71810001,\n",
        "#        0.80379999, 0.77279997, 0.75849998, 0.79549998, 0.81889999,\n",
        "#        0.74579996, 0.76859999, 0.7101    , 0.73979998, 0.79009998,\n",
        "#        0.81149995, 0.78669995, 0.7428    , 0.79170001, 0.8132    ,\n",
        "#        0.78959996, 0.76980001, 0.81199998, 0.80719995, 0.72999996,\n",
        "#        0.81939995, 0.72579998, 0.79409999, 0.77669996, 0.69279999,\n",
        "#        0.79149997, 0.7902    , 0.88139999, 0.8818    , 0.88439995,\n",
        "#        0.87409997, 0.88499999, 0.87889999, 0.88369995, 0.87849998,\n",
        "#        0.85869998, 0.85459995, 0.86769998, 0.87829995, 0.90419996,\n",
        "#        0.90819997, 0.90579998, 0.90319997, 0.90469998, 0.90619999,\n",
        "#        0.90270001, 0.90619999, 0.90959996, 0.8944    , 0.89899999,\n",
        "#        0.90209997, 0.9048    , 0.89889997, 0.90399998, 0.89769995,\n",
        "#        0.91259998, 0.91619998, 0.91419995, 0.91729999, 0.91719997,\n",
        "#        0.91529995, 0.9163    , 0.91769999, 0.91789997, 0.91769999,\n",
        "#        0.91859996, 0.91759998, 0.91979998, 0.92079997, 0.91799998,\n",
        "#        0.91869998, 0.91749996, 0.9163    , 0.91679996, 0.91659999,\n",
        "#        0.91670001, 0.91759998, 0.9149    , 0.91689998, 0.9156    ,\n",
        "#        0.91499996, 0.9156    , 0.91149998, 0.9138    , 0.91670001,\n",
        "#        0.9138    , 0.91349995, 0.9181    , 0.90499997, 0.912     ,\n",
        "#        0.91429996, 0.91579998, 0.91139996, 0.9131    , 0.89889997,\n",
        "#        0.91329998, 0.91299999, 0.9145    , 0.91119999, 0.91409999,\n",
        "#        0.91279995, 0.91249996, 0.9145    , 0.91049999, 0.9109    ,\n",
        "#        0.90789998, 0.91499996, 0.91239995, 0.91029996, 0.91219997])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zTgu2lAxYeS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea8a1849-3265-4ff6-c2c5-b16fb84edd8e"
      },
      "source": [
        "print(\"Training Loss Set: \", repr(train_loss_Adam))\n",
        "print(\"Training Accuracy Set: \",repr(train_accuracy_Adam))\n",
        "print(\"Validation Loss Set: \", repr(valid_loss_Adam))\n",
        "print(\"Validation Accuracy Set: \",repr(valid_accuracy_Adam))\n",
        "print(\"Test Loss Set: \", repr(test_loss_Adam))\n",
        "print(\"Test Accuracy Set: \",repr(test_accuracy_Adam))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Loss Set:  array([1.63098025, 1.18482816, 0.92434663, 0.74880612, 0.64766753,\n",
            "       0.5827806 , 0.5433141 , 0.5113861 , 0.49023631, 0.47311229,\n",
            "       0.45701393, 0.44433579, 0.42935544, 0.41854614, 0.41153285,\n",
            "       0.40299255, 0.40412036, 0.39132437, 0.38328621, 0.38124898,\n",
            "       0.38206223, 0.37248716, 0.37170446, 0.36176464, 0.36307919,\n",
            "       0.35584307, 0.35662925, 0.35808343, 0.35688946, 0.35222122,\n",
            "       0.35235897, 0.34545928, 0.34290826, 0.34415391, 0.34525529,\n",
            "       0.34324637, 0.34100187, 0.24085355, 0.20217335, 0.19170846,\n",
            "       0.18723124, 0.18384106, 0.18559903, 0.18549526, 0.1880773 ,\n",
            "       0.18568741, 0.19353676, 0.18947835, 0.18815568, 0.12596726,\n",
            "       0.09945331, 0.08728779, 0.08528612, 0.0803453 , 0.07978326,\n",
            "       0.07399344, 0.07425223, 0.07360058, 0.06891334, 0.07226951,\n",
            "       0.07116149, 0.07237988, 0.07525628, 0.07291701, 0.07637273,\n",
            "       0.04518639, 0.03597797, 0.02885379, 0.02776799, 0.02636736,\n",
            "       0.0241129 , 0.02403767, 0.02100205, 0.02180966, 0.0211378 ,\n",
            "       0.02147923, 0.02042247, 0.01871116, 0.01923213, 0.01841849,\n",
            "       0.01910133, 0.01860956, 0.01646179, 0.01753278, 0.01750859,\n",
            "       0.01722397, 0.01696129, 0.01845689, 0.01938996, 0.01684781,\n",
            "       0.01646712, 0.01780239, 0.01903388, 0.01713811, 0.01796572,\n",
            "       0.01855857, 0.01591437, 0.01919276, 0.01785485, 0.01932137,\n",
            "       0.02023486, 0.02096085, 0.02076473, 0.02075142, 0.02226698,\n",
            "       0.01900789, 0.0189402 , 0.01822328, 0.02132328, 0.02087948,\n",
            "       0.02004796, 0.01892515, 0.01979257, 0.02133206, 0.02405019,\n",
            "       0.0225936 , 0.02262349, 0.02339199, 0.02365335, 0.02124777])\n",
            "Training Accuracy Set:  array([0.38857499, 0.57124996, 0.67167497, 0.74114996, 0.77442497,\n",
            "       0.79892498, 0.81364995, 0.82299995, 0.82962495, 0.838525  ,\n",
            "       0.843225  , 0.84722495, 0.85049999, 0.85492498, 0.85877496,\n",
            "       0.86052495, 0.86087495, 0.86412495, 0.86857498, 0.86739999,\n",
            "       0.86750001, 0.87159997, 0.87229997, 0.87579995, 0.87360001,\n",
            "       0.87554997, 0.87667495, 0.87689996, 0.87649995, 0.878025  ,\n",
            "       0.87717497, 0.88169998, 0.88134998, 0.88264996, 0.88209999,\n",
            "       0.88185   , 0.88339996, 0.91747499, 0.93107498, 0.93362498,\n",
            "       0.93682498, 0.93667495, 0.93605   , 0.93677497, 0.93489999,\n",
            "       0.93562496, 0.93334997, 0.93454999, 0.93392497, 0.95795   ,\n",
            "       0.96672499, 0.97094995, 0.97097498, 0.973975  , 0.97317499,\n",
            "       0.97532499, 0.97474998, 0.97555   , 0.97659999, 0.97557497,\n",
            "       0.97607499, 0.97584999, 0.97522497, 0.97555   , 0.97442496,\n",
            "       0.98597497, 0.989375  , 0.99207497, 0.99245   , 0.99204999,\n",
            "       0.99322498, 0.99299997, 0.99444997, 0.99432498, 0.99409997,\n",
            "       0.99404997, 0.99422497, 0.99527496, 0.99479997, 0.99469995,\n",
            "       0.99502498, 0.99502498, 0.99584997, 0.995525  , 0.99549997,\n",
            "       0.99562496, 0.995875  , 0.99482495, 0.99427497, 0.99577498,\n",
            "       0.99564999, 0.995125  , 0.99472499, 0.99539995, 0.99524999,\n",
            "       0.99519998, 0.99589998, 0.99502498, 0.99497497, 0.99454999,\n",
            "       0.99427497, 0.99427497, 0.99369997, 0.99382496, 0.99307495,\n",
            "       0.99495   , 0.99474996, 0.99495   , 0.99392498, 0.99369997,\n",
            "       0.99384999, 0.99474996, 0.99419999, 0.99374998, 0.99282497,\n",
            "       0.9932    , 0.993325  , 0.99304998, 0.99304998, 0.99384999])\n",
            "Validation Loss Set:  array([1.55783594, 1.52161407, 1.24889517, 0.84182656, 0.89323217,\n",
            "       0.713413  , 0.76559377, 0.78031182, 0.90696895, 0.85179651,\n",
            "       0.58523309, 0.70254719, 0.74266708, 0.64539486, 0.56975979,\n",
            "       0.80132061, 0.70561922, 0.86596519, 0.84709626, 0.64653951,\n",
            "       0.58040065, 0.67281491, 0.79712498, 0.61198562, 0.56978834,\n",
            "       0.68041283, 0.69623721, 0.5825749 , 0.60563886, 0.83218521,\n",
            "       0.54149675, 0.8593418 , 0.66787356, 0.71644032, 1.0116775 ,\n",
            "       0.59645993, 0.63205767, 0.3847951 , 0.36289248, 0.34446841,\n",
            "       0.38228068, 0.35512629, 0.38351691, 0.38820621, 0.40156859,\n",
            "       0.43998909, 0.44266936, 0.42113611, 0.38563371, 0.29343411,\n",
            "       0.29864109, 0.31735253, 0.31350642, 0.32177123, 0.3160539 ,\n",
            "       0.32784128, 0.33763063, 0.30781922, 0.34939167, 0.34976146,\n",
            "       0.33153215, 0.34650236, 0.36840552, 0.35023317, 0.34985146,\n",
            "       0.32229218, 0.30714357, 0.31356347, 0.31345487, 0.30014107,\n",
            "       0.31970268, 0.30777767, 0.31078839, 0.32217506, 0.31684774,\n",
            "       0.33113995, 0.31970516, 0.33407909, 0.31244326, 0.3378734 ,\n",
            "       0.3417294 , 0.33023912, 0.33285326, 0.3220641 , 0.33516982,\n",
            "       0.31850144, 0.33705434, 0.33034986, 0.33400241, 0.32248801,\n",
            "       0.33533341, 0.35160601, 0.36361852, 0.35978967, 0.34152481,\n",
            "       0.35819933, 0.37667987, 0.3420006 , 0.38998914, 0.36176947,\n",
            "       0.34866029, 0.3597441 , 0.37947235, 0.34622636, 0.43591633,\n",
            "       0.36546731, 0.33320844, 0.36168158, 0.35066411, 0.35977909,\n",
            "       0.37483329, 0.35610616, 0.32663047, 0.3508074 , 0.34853786,\n",
            "       0.34734043, 0.34473416, 0.35619226, 0.35254195, 0.35536626])\n",
            "Validation Accuracy Set:  array([0.44739997, 0.50139999, 0.57029998, 0.70229995, 0.70199996,\n",
            "       0.75139999, 0.74360001, 0.74329996, 0.71129996, 0.71989995,\n",
            "       0.79609996, 0.7701    , 0.75339997, 0.7863    , 0.81239998,\n",
            "       0.74299997, 0.75650001, 0.72469997, 0.7342    , 0.78549999,\n",
            "       0.79890001, 0.77859998, 0.74049997, 0.78999996, 0.80219996,\n",
            "       0.77239996, 0.76889998, 0.80409998, 0.79949999, 0.72789997,\n",
            "       0.81589997, 0.7306    , 0.78539997, 0.77169997, 0.68329996,\n",
            "       0.79530001, 0.78889996, 0.8725    , 0.88189995, 0.88739997,\n",
            "       0.87329996, 0.88229996, 0.87349999, 0.87269998, 0.87339997,\n",
            "       0.86439997, 0.85549998, 0.86799997, 0.87599999, 0.90329999,\n",
            "       0.9059    , 0.9034    , 0.90569997, 0.90569997, 0.90359998,\n",
            "       0.90639997, 0.90019995, 0.9077    , 0.89719999, 0.8976    ,\n",
            "       0.9023    , 0.89929998, 0.8919    , 0.89789999, 0.89959997,\n",
            "       0.90679997, 0.91399997, 0.91219997, 0.91399997, 0.91579998,\n",
            "       0.9145    , 0.91569996, 0.91609997, 0.91419995, 0.9149    ,\n",
            "       0.91549999, 0.91429996, 0.91670001, 0.9192    , 0.9152    ,\n",
            "       0.91319996, 0.91429996, 0.91679996, 0.91509998, 0.91349995,\n",
            "       0.91859996, 0.91429996, 0.9152    , 0.91279995, 0.9163    ,\n",
            "       0.91459996, 0.91329998, 0.91029996, 0.90979999, 0.9138    ,\n",
            "       0.90959996, 0.91029996, 0.91479999, 0.90349996, 0.91009998,\n",
            "       0.9145    , 0.91149998, 0.90359998, 0.90989995, 0.8973    ,\n",
            "       0.91139996, 0.91289997, 0.91159999, 0.90799999, 0.9138    ,\n",
            "       0.90849996, 0.9109    , 0.91589999, 0.91289997, 0.91189998,\n",
            "       0.90809995, 0.9156    , 0.91109997, 0.91099995, 0.91209996])\n",
            "Test Loss Set:  array([1.50516856, 1.51188815, 1.29473245, 0.8595646 , 0.90460736,\n",
            "       0.70080239, 0.75703222, 0.81455696, 0.93434656, 0.8720606 ,\n",
            "       0.56318939, 0.71076798, 0.75184232, 0.64574844, 0.55569506,\n",
            "       0.82789201, 0.70006621, 0.96869075, 0.86290318, 0.67974633,\n",
            "       0.55451965, 0.67770076, 0.87911534, 0.62351274, 0.58336341,\n",
            "       0.67355037, 0.73602015, 0.56763124, 0.61525607, 0.86475915,\n",
            "       0.55456936, 0.90860075, 0.67895633, 0.74933088, 1.03419423,\n",
            "       0.62102193, 0.66888767, 0.37894154, 0.35685366, 0.35062769,\n",
            "       0.39061579, 0.3644751 , 0.37589926, 0.3657887 , 0.40115267,\n",
            "       0.45404416, 0.46673962, 0.42376262, 0.3899346 , 0.31059083,\n",
            "       0.29400316, 0.32465661, 0.31765386, 0.33160627, 0.3220908 ,\n",
            "       0.33999759, 0.336647  , 0.31827852, 0.37499851, 0.36277896,\n",
            "       0.35109949, 0.34594384, 0.3643299 , 0.34833032, 0.36226416,\n",
            "       0.3153626 , 0.30994228, 0.32804713, 0.32099718, 0.32161883,\n",
            "       0.32669014, 0.32039464, 0.32662642, 0.32537889, 0.33101076,\n",
            "       0.32695094, 0.33046752, 0.33126917, 0.32301405, 0.33908272,\n",
            "       0.33412474, 0.32873991, 0.34328038, 0.34317434, 0.35596618,\n",
            "       0.33259258, 0.34262055, 0.35036722, 0.34168521, 0.34740785,\n",
            "       0.35626334, 0.36490032, 0.37679493, 0.38247281, 0.35339424,\n",
            "       0.36530986, 0.38333189, 0.35091412, 0.41433379, 0.36458987,\n",
            "       0.35078478, 0.35189176, 0.3810997 , 0.35155326, 0.45320579,\n",
            "       0.36502641, 0.37128952, 0.3714872 , 0.37667093, 0.35723549,\n",
            "       0.3733938 , 0.38295144, 0.35331589, 0.37274614, 0.37023613,\n",
            "       0.38723809, 0.36336458, 0.36656645, 0.36860529, 0.35888502])\n",
            "Test Accuracy Set:  array([0.46829998, 0.523     , 0.57339996, 0.70359999, 0.70809996,\n",
            "       0.7518    , 0.74579996, 0.74419999, 0.71779996, 0.71810001,\n",
            "       0.80379999, 0.77279997, 0.75849998, 0.79549998, 0.81889999,\n",
            "       0.74579996, 0.76859999, 0.7101    , 0.73979998, 0.79009998,\n",
            "       0.81149995, 0.78669995, 0.7428    , 0.79170001, 0.8132    ,\n",
            "       0.78959996, 0.76980001, 0.81199998, 0.80719995, 0.72999996,\n",
            "       0.81939995, 0.72579998, 0.79409999, 0.77669996, 0.69279999,\n",
            "       0.79149997, 0.7902    , 0.88139999, 0.8818    , 0.88439995,\n",
            "       0.87409997, 0.88499999, 0.87889999, 0.88369995, 0.87849998,\n",
            "       0.85869998, 0.85459995, 0.86769998, 0.87829995, 0.90419996,\n",
            "       0.90819997, 0.90579998, 0.90319997, 0.90469998, 0.90619999,\n",
            "       0.90270001, 0.90619999, 0.90959996, 0.8944    , 0.89899999,\n",
            "       0.90209997, 0.9048    , 0.89889997, 0.90399998, 0.89769995,\n",
            "       0.91259998, 0.91619998, 0.91419995, 0.91729999, 0.91719997,\n",
            "       0.91529995, 0.9163    , 0.91769999, 0.91789997, 0.91769999,\n",
            "       0.91859996, 0.91759998, 0.91979998, 0.92079997, 0.91799998,\n",
            "       0.91869998, 0.91749996, 0.9163    , 0.91679996, 0.91659999,\n",
            "       0.91670001, 0.91759998, 0.9149    , 0.91689998, 0.9156    ,\n",
            "       0.91499996, 0.9156    , 0.91149998, 0.9138    , 0.91670001,\n",
            "       0.9138    , 0.91349995, 0.9181    , 0.90499997, 0.912     ,\n",
            "       0.91429996, 0.91579998, 0.91139996, 0.9131    , 0.89889997,\n",
            "       0.91329998, 0.91299999, 0.9145    , 0.91119999, 0.91409999,\n",
            "       0.91279995, 0.91249996, 0.9145    , 0.91049999, 0.9109    ,\n",
            "       0.90789998, 0.91499996, 0.91239995, 0.91029996, 0.91219997])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XS0lPXeaYpxU"
      },
      "source": [
        "##Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dGtwaAXfYmnp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f3c45f1f-4a1f-413b-e725-6fdfab0b0ad2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Create array of epochs for X axis\n",
        "x_axis = np.zeros((120,1))\n",
        "for i in range(120):\n",
        "  x_axis[i] = i\n",
        "\n",
        "#Plot Validation Loss for each algorithm\n",
        "plt.title('Result Analysis')\n",
        "plt.plot(x_axis, valid_loss_SGD, color='green', linewidth = 0.75,label='SGD')\n",
        "plt.plot(x_axis, valid_loss_NAG[:], color='black', label='NAG', linewidth = 0.75,)\n",
        "plt.plot(x_axis, valid_loss_HeavyBall[:], color='blue', label='HB', linewidth = 0.75,)\n",
        "plt.plot(x_axis, valid_loss_ASGD[:], color='red', label='ASGD', linewidth = 0.75,)\n",
        "plt.plot(x_axis, valid_loss_Adam[:], label='Adam', linewidth = 0.75,)\n",
        "\n",
        "plt.legend()\n",
        " \n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "# plt.grid(axis='both')\n",
        "plt.savefig('Resnet128_fixed_validloss.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a8UdSaOTYrxL",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"Resnet128_fixed_validloss.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jtsomITcYytl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "b41a1c43-a173-452b-b273-b9f30077a4d5"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Create array of epochs for X axis\n",
        "x_axis = np.zeros((120,1))\n",
        "for i in range(120):\n",
        "  x_axis[i] = i\n",
        "\n",
        "#Plot Test Error for each algorithm\n",
        "plt.title('Result Analysis')\n",
        "plt.plot(x_axis, 1-train_accuracy_SGD[:], color='green', linewidth = 0.75,label='SGD')\n",
        "plt.plot(x_axis, 1-train_accuracy_NAG[:], color='black', label='NAG', linewidth = 0.75,)\n",
        "plt.plot(x_axis, 1-train_accuracy_HeavyBall[:], color='blue', label='HB', linewidth = 0.75,)\n",
        "plt.plot(x_axis, 1-train_accuracy_ASGD[:], color='red', label='ASGD', linewidth = 0.75,)\n",
        "plt.plot(x_axis, 1-train_accuracy_Adam[:], label='Adam', linewidth = 0.75,)\n",
        "\n",
        "plt.legend()\n",
        " \n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Test Set Error')\n",
        "# plt.grid(axis='both')\n",
        "plt.savefig('Resnet128_fixed_test_error.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Owdv46pCY2oG",
        "colab": {}
      },
      "source": [
        "files.download('Resnet128_fixed_test_error.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}